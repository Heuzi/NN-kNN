{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heuzi/NN-kNN/blob/main/nnknn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to use this file\n",
        "\n",
        "Run everything in \"Module definitions\" section. This will set up all the classes needed.\n",
        "\n",
        "Run the section \"pydml\" to access knn using other distance metric learning algorithms.\n",
        "\n",
        "Run a subsection in \"Data Sets\" section to load the data set you want.\n",
        "\n",
        "If the data set is a regression data set, you may run \"Standardize Regression Data sets\". This may not be needed if your parameters are tuned.\n",
        "\n",
        "Run either \"classification\" or \"Regression\".\n",
        "Notice that at the beginning of these two sections, there are settings you can tune for NN-kNN\n",
        "\n",
        "Run \"Result look up\" section to check results.\n",
        "\n",
        "Good luck, future Ye or whoever reads this.\n",
        "\n",
        "--- Past Ye"
      ],
      "metadata": {
        "id": "OJDLyd_l9gP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Important settings for best performance"
      ],
      "metadata": {
        "id": "ReXmju_QUAEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularizer like L2 REGULARIZATION WILL LOWER THE ACCURACY OF NNKNN!"
      ],
      "metadata": {
        "id": "SSmWpl7f9m7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "default case activation layer's bias needs to be carefully set.\n",
        "\n",
        "```\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/2 )\n",
        "    self.bias = torch.nn.Parameter(torch.ones(num_cases) )\n",
        "```\n",
        "These two choices depends on the number of features you have. If you have too many features and the feature distances are big, they might reduce your case activation to 0 and therefore no learning can occur. In that case, use the first one.\n",
        "\n",
        "Important note: you can tell if your model is trained properly. The model should at the very least, surpass k-nn. If you are having worse accuracy than a vanilla k-nn, then you will need to tune the parameters.\n"
      ],
      "metadata": {
        "id": "UBH03nbnUDQ9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8091XltONjJx"
      },
      "source": [
        "#Module definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2l83dT3OnGc"
      },
      "source": [
        "Boring imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5zwSxqSOkaM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxlGKTJXHTpq"
      },
      "source": [
        "##New Feature Act Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.5"
      ],
      "metadata": {
        "id": "iw7AZFbHmpCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hidden_layers_1(nn.Module):\n",
        "  def __init__(self,num_features, hidden_dim = None, extract_dim = None):\n",
        "    if(hidden_dim == None):\n",
        "      hidden_dim = num_features * 2\n",
        "    if(extract_dim == None):\n",
        "      extract_dim = num_features\n",
        "    self.layer1 = nn.Linear(num_features, hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "    self.layer2 = nn.Linear(hidden_dim,extract_dim)\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "8utMDzteZnQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxfVJTwmHSd6"
      },
      "outputs": [],
      "source": [
        "class FeatureActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the feature distance/activation between query and all cases\n",
        "  '''\n",
        "  def __init__(self, num_features, num_cases, cases, hidden_layers = None):\n",
        "    super().__init__()\n",
        "    #we assume feature weight sharing between segments\n",
        "    self.feature_matrix = cases\n",
        "    self.f1weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "    # self.activation_func = torch.relu\n",
        "\n",
        "    self.hidden_layers = hidden_layers\n",
        "\n",
        "  def forward(self, query, cases):\n",
        "    '''\n",
        "      input: (m+1) cases * n features; where the first case is the query\n",
        "      output: m* n feature activations\n",
        "    '''\n",
        "    if self.hidden_layers != None:\n",
        "      query = self.hidden_layers(query)\n",
        "      cases = self.hidden_layers(cases)\n",
        "\n",
        "    query = query.unsqueeze(1)\n",
        "\n",
        "    #randomly select 10% of the rows of cases\n",
        "    # cases = cases[torch.randperm(cases.shape[0])[:int(cases.shape[0] * 0.1)]]\n",
        "    #lol\n",
        "    # cases = nn.Dropout(p=0.99)(cases);\n",
        "\n",
        "    # print(\"cool\")\n",
        "    # print(query.shape)\n",
        "    # print(self.f1weight.shape)\n",
        "    # print(cases.shape)\n",
        "    # print(query)\n",
        "    # print((query*self.f1weight).shape)\n",
        "    # print((query*self.f1weight))\n",
        "    # print(self.f1weight.shape)\n",
        "    # print(self.f1weight)\n",
        "    # print((cases*self.f1weight).shape)\n",
        "    # print((cases*self.f1weight))\n",
        "    # print((query*self.f1weight - cases*self.f1weight).shape)\n",
        "    # print((query*self.f1weight - cases*self.f1weight))\n",
        "    #the following utilize pytorch tensor broadcasting functionality\n",
        "    #if the below line does not make sense, uncomment lines above to debug and check values\n",
        "    return (query*self.f1weight - cases*self.f1weight)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2gojHNfJacB"
      },
      "source": [
        "###Old Feature Activation Layer\n",
        "Deprecated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPaUn0-Kf0ts"
      },
      "outputs": [],
      "source": [
        "class OldFeatureActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the feature distance/activation between query and all cases\n",
        "  '''\n",
        "  def __init__(self, num_features, num_cases, weight_sharing_within_segment=False, weight_sharing_between_segment= False, hidden_layers = False):\n",
        "    super().__init__()\n",
        "    self.weight_sharing_between_segment = weight_sharing_between_segment\n",
        "    if self.weight_sharing_between_segment:\n",
        "      self.segments = torch.nn.ModuleList([OldFeatureActivationSegment(num_features, weight_sharing_within_segment, hidden_layers)])\n",
        "    else:\n",
        "      self.segments = torch.nn.ModuleList([OldFeatureActivationSegment(num_features, weight_sharing_within_segment, hidden_layers) for _ in range(num_cases)])\n",
        "    # feature activation layer should not have its own activation function,\n",
        "    # instead, it should just reuse feature activation segments' activation function, which is called by OldFeatureActivationSegment\n",
        "    # self.activation_func = torch.nn.Identity()\n",
        "  def set_f_weight_for_all_segments(self, f):\n",
        "    for segment in self.segments:\n",
        "      segment.set_f1(f)\n",
        "      segment.set_f2(f)\n",
        "  def freeze_f_weight_for_all_segments(self):\n",
        "    for segment in self.segments:\n",
        "      segment.freeze_f1()\n",
        "      segment.freeze_f2()\n",
        "  def forward(self, query, cases):\n",
        "    '''\n",
        "      input: (m+1) cases * n features; where the first case is the query\n",
        "      output: m* n feature activations\n",
        "    '''\n",
        "    if self.weight_sharing_between_segment:\n",
        "      #only uses the first segment through out.\n",
        "      return torch.stack([self.segments[0](query, case) for case in cases], dim=1)\n",
        "    return torch.stack([segment(query, case) for segment, case in zip(self.segments, cases)], dim=1) ###OMG, colab is a great programmer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJvuUic5JPoP"
      },
      "source": [
        "###Old Feature Activation Segment\n",
        "\n",
        "Deprecated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAMlrLdaZK5p"
      },
      "source": [
        "assume there are m cases, n features for each case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VJRT4bxOqPr"
      },
      "outputs": [],
      "source": [
        "class OldFeatureActivationSegment(torch.nn.Module):\n",
        "  '''\n",
        "    measures the feature distance/activation between two cases\n",
        "\n",
        "    two features connect to one feature activation node\n",
        "\n",
        "    multiple feature activation layer only connects to one case node\n",
        "\n",
        "    num_features: number of features that f1 or f2 contains. f1 is a vector of features for one case.\n",
        "  '''\n",
        "  def __init__(self, num_features, weight_sharing=False, hidden_layers = False):\n",
        "    super().__init__()\n",
        "    self.weight_sharing = weight_sharing\n",
        "    ## IMPORTANT DESIGN DECISION:: start weights and biases with ones instead of randoms.\n",
        "    self.f1weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "    self.f2weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "    if weight_sharing:\n",
        "      self.f2weight = self.f1weight\n",
        "    #it does not make sense that the bias is non-zero.\n",
        "    # self.bias = torch.nn.Parameter(torch.zeros(num_features))\n",
        "    # self.activation_func = torch.nn.Identity()\n",
        "    self.activation_func = torch.relu\n",
        "\n",
        "    self.hidden_layers = hidden_layers\n",
        "    hidden_dim = num_features * 2\n",
        "    extract_dim = num_features\n",
        "    self.layer1 = nn.Linear(num_features, hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer2 = nn.Linear(hidden_dim, extract_dim)\n",
        "\n",
        "  # maybe not needed\n",
        "  # def set_activation_func(self, activation_function):\n",
        "  #   self.activation_func = activation_function\n",
        "\n",
        "  def set_f1(self, f1):\n",
        "    self.f1weight = f1\n",
        "\n",
        "  def set_f2(self, f2):\n",
        "    self.f2weight = f2\n",
        "\n",
        "  def freeze_f1(self):\n",
        "    self.f1weight.requires_grad = False\n",
        "\n",
        "  def freeze_f2(self):\n",
        "    self.f2weight.requires_grad = False\n",
        "\n",
        "  def forward(self, f1, f2):\n",
        "    '''\n",
        "      input: f1 and f2, are both feature vectors of a case, shape n.\n",
        "      output: feature activation, shape n.\n",
        "    '''\n",
        "    if self.hidden_layers:\n",
        "      f1 = self.layer1(f1)\n",
        "      f1 = self.relu(f1)\n",
        "      f1 = self.layer2(f1)\n",
        "      f1 = self.relu(f1)\n",
        "\n",
        "      f2 = self.layer1(f2)\n",
        "      f2 = self.relu(f2)\n",
        "      f2 = self.layer2(f2)\n",
        "      f2 = self.relu(f2)\n",
        "\n",
        "\n",
        "    # return self.activation_func(self.f1weight * f1 + self.f2weight * f2 + self.bias)\n",
        "    # IMPORTANT DESIGN DECISION:: minus makes more sense here as in standard k-NN.\n",
        "    # feature_distance = self.f1weight * f1 - self.f2weight * f2\n",
        "    # feature_distance = (self.f1weight * f1 - self.f2weight * f2) ** 2\n",
        "    # feature_distance = torch.abs(self.f1weight * f1 - self.f2weight * f2)\n",
        "    feature_distance = (self.f1weight * f1 - self.f2weight * f2) ** 2\n",
        "    # feature_distance = torch.abs(self.f1weight * f1 - self.f2weight * f2)\n",
        "    # return self.activation_func(feature_distance + self.bias)\n",
        "    return self.activation_func(feature_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvaZeqxwJeJ5"
      },
      "source": [
        "##Case Activation Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT.\n",
        "```\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/2 )\n",
        "    self.bias = torch.nn.Parameter(torch.ones(num_cases) )\n",
        "```\n",
        "These two choices depends on the number of features you have. If you have too many features, may use the first one.\n"
      ],
      "metadata": {
        "id": "Cs_eTUGYRNnt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OtiusY8HV8v"
      },
      "outputs": [],
      "source": [
        "class CaseActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a case given some feature activations\n",
        "\n",
        "    input:\n",
        "      m* n feature activations\n",
        "    output:\n",
        "      m case activations\n",
        "  '''\n",
        "  def __init__(self, num_features, num_cases, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.weight_sharing = weight_sharing\n",
        "    self.fa_weight = torch.nn.Parameter(torch.ones((num_cases, num_features)))\n",
        "    if weight_sharing:\n",
        "      self.fa_weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "\n",
        "    self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/2 )\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) )\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/10 )\n",
        "  def forward(self,input):\n",
        "    '''\n",
        "      input: m*n feature activations\n",
        "      output: m case activations\n",
        "    '''\n",
        "    input = - torch.sum(input * F.leaky_relu(self.fa_weight), dim=2) + self.bias\n",
        "    ## we don't want negative values in activations\n",
        "    ## they will mess up our top case selections because if the tops are negatives\n",
        "    ## then the filled 0s are actually bigger.\n",
        "    input = F.sigmoid(input)\n",
        "    # input = torch.relu(input)\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkPmMySVi3jN"
      },
      "source": [
        "##Top Case Selection Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne1tBxp6jEp9"
      },
      "outputs": [],
      "source": [
        "# prompt: define a class named TopCaseLayer inherited from torch.nn.Module. This class is used to select the top k activations of an input tensor (m case activations), and output a tensor of the same shape but only keep the top k activations, and other tensors zeroed out\n",
        "\n",
        "class TopCaseLayer(torch.nn.Module):\n",
        "  def __init__(self, k):\n",
        "    super().__init__()\n",
        "    self.k = k\n",
        "    self.training = True\n",
        "\n",
        "  def forward(self, input):\n",
        "    ##no behavior during training because we want to train for all.\n",
        "    if self.training:\n",
        "      return input\n",
        "    '''\n",
        "      input: m case activations\n",
        "      output: m case activations, the top k activations are kept and others are zeroed out\n",
        "    '''\n",
        "    # print(input)\n",
        "    vals, idx = torch.topk(input, self.k)\n",
        "\n",
        "    #if any of the vals is zero, it will mess things up, reverse them so that they becomes positive.\n",
        "\n",
        "    # print(input.shape)\n",
        "    # print(idx.shape)\n",
        "    # print(vals.shape)\n",
        "    # print(\"huh??\")\n",
        "    output = torch.zeros_like(input).scatter_(1,idx, vals)\n",
        "    #replace output values such that the top k is kept\n",
        "\n",
        "\n",
        "    # output[:, idx] = vals\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESrlZ3sTJhm4"
      },
      "source": [
        "##Class Activation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vg6cXb3rQi1"
      },
      "outputs": [],
      "source": [
        "# class ClassActivationLayer(torch.nn.Module):\n",
        "#   '''\n",
        "#     measures the activation of a class given some case activations\n",
        "\n",
        "#     input:\n",
        "#       m case_activations\n",
        "#     output:\n",
        "#       l class_activations\n",
        "#   '''\n",
        "#   def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "#     super().__init__()\n",
        "#     self.constraints = []\n",
        "#     self.case_labels = case_labels\n",
        "#     self.num_classes = torch.unique(case_labels).shape[0]\n",
        "#     # self.weight_sharing = weight_sharing\n",
        "#     #weight sharing doesn't make sense here, some cases shouldn't contribute to a class while some should\n",
        "#     # they should not share weights.\n",
        "#     self.ca_weight = torch.nn.Parameter(torch.ones((num_cases, self.num_classes))) ## should I use randn here?\n",
        "#     # self.ca_weight = torch.nn.Parameter(torch.rand((num_cases, self.num_classes)))\n",
        "#     # if weight_sharing:\n",
        "#     #   self.ca_weight = torch.nn.Parameter(torch.randn(num_cases))\n",
        "#     # self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "#     self.bias = torch.nn.Parameter(torch.zeros(self.num_classes))\n",
        "\n",
        "#     print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "#   ##NOTE:: the following three constrain uses for loop, may not be the most efficient\n",
        "#   def case_class_constrain_v1(self):\n",
        "#     '''\n",
        "#       ensures that each case only positively activates their correct class label.\n",
        "#     '''\n",
        "#     self.constraints.append(\"case_pos_class\")\n",
        "\n",
        "\n",
        "#   ##NOTE:: if used, this should be called before the constrain_v1()\n",
        "#   def cases_share_weight_on_same_class(self):\n",
        "#     '''\n",
        "#       all cases of the same class share one weight for that class.\n",
        "#     '''\n",
        "#     self.constraints.append(\"case_share_weight_on_same_class\")\n",
        "\n",
        "\n",
        "#   def case_class_constrain_v2(self):\n",
        "#     '''\n",
        "#       ensures that each case does not contribute to incorrect classes.\n",
        "#     '''\n",
        "#     self.constraints.append(\"case_no_contribute_to_wrong_class\")\n",
        "#   def case_class_constrain_v3(self):\n",
        "#     '''\n",
        "#       ensures that each case only negatively contribute to incorrect classes.\n",
        "#     '''\n",
        "#     #TODO, needs revamping\n",
        "#     #The weight manipulation should only happen during forward()\n",
        "#     raise ValueError(\"error, needs revamping\")\n",
        "#     for i in range(self.ca_weight.shape[0]):\n",
        "#       for j in range(self.ca_weight.shape[1]):\n",
        "#         if j != case_labels[i]:\n",
        "#           self.ca_weight[i][j] = 0 - torch.relu(self.ca_weight[i][j])\n",
        "#   def forward(self, ca):\n",
        "#     constrained_weight = self.ca_weight.clone().detach()\n",
        "#     for i in range(constrained_weight.shape[0]):\n",
        "#       for j in range(constrained_weight.shape[1]):\n",
        "#         if(j == self.case_labels[i]):\n",
        "#           if(\"case_pos_class\" in self.constraints):\n",
        "#             constrained_weight[i][j] = torch.relu(self.ca_weight[i][j])\n",
        "#         else: # j != case_labels[i]\n",
        "#           if(\"case_no_contribute_to_wrong_class\" in self.constraints):\n",
        "#             constrained_weight[i][j] = torch.zeros(1)\n",
        "#     return torch.matmul(ca, constrained_weight) + self.bias #matrix multiplication will handle 1 dimension vector property"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Il5qc6sq-3n"
      },
      "source": [
        "The below code is another implementation of ClassActivationLayer which use mask to reduce the amount of computations during the forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So4vm5Z_qwY0"
      },
      "outputs": [],
      "source": [
        "class ClassActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      l class_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.case_labels = case_labels\n",
        "    self.num_classes = torch.unique(case_labels).shape[0]\n",
        "    # self.weight_sharing = weight_sharing\n",
        "    #weight sharing doesn't make sense here, some cases shouldn't contribute to a class while some should\n",
        "    # they should not share weights.\n",
        "    self.ca_weight = torch.nn.Parameter(torch.ones((num_cases, self.num_classes))) ## should I use randn here?\n",
        "    # self.ca_weight = torch.nn.Parameter(torch.rand((num_cases, self.num_classes)))\n",
        "    # if weight_sharing:\n",
        "    #   self.ca_weight = torch.nn.Parameter(torch.randn(num_cases))\n",
        "    # self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    self.bias = torch.nn.Parameter(torch.ones(self.num_classes))\n",
        "\n",
        "    # create a mask\n",
        "    self.mask = torch.zeros_like(self.ca_weight)\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "\n",
        "  def update_mask(self):\n",
        "    for i in range(self.mask.shape[0]):\n",
        "      for j in range(self.mask.shape[1]):\n",
        "        if(j == self.case_labels[i]):\n",
        "          if(\"case_pos_class\" in self.constraints):\n",
        "            self.mask[i][j] = torch.ones(1)\n",
        "        else: # j != case_labels[i]\n",
        "          if(\"case_no_contribute_to_wrong_class\" in self.constraints):\n",
        "            self.mask[i][j] = torch.zeros(1)\n",
        "\n",
        "\n",
        "  ##NOTE:: the following three constrain uses for loop, may not be the most efficient\n",
        "  def case_class_constrain_v1(self):\n",
        "    '''\n",
        "      ensures that each case only positively activates their correct class label.\n",
        "    '''\n",
        "    self.constraints.append(\"case_pos_class\")\n",
        "    self.update_mask()\n",
        "\n",
        "\n",
        "  ##NOTE:: if used, this should be called before the constrain_v1()\n",
        "  def cases_share_weight_on_same_class(self):\n",
        "    '''\n",
        "      all cases of the same class share one weight for that class.\n",
        "    '''\n",
        "    self.constraints.append(\"case_share_weight_on_same_class\")\n",
        "\n",
        "\n",
        "  def case_class_constrain_v2(self):\n",
        "    '''\n",
        "      ensures that each case does not contribute to incorrect classes.\n",
        "    '''\n",
        "    self.constraints.append(\"case_no_contribute_to_wrong_class\")\n",
        "    self.update_mask()\n",
        "\n",
        "  def case_class_constrain_v3(self):\n",
        "    '''\n",
        "      ensures that each case only negatively contribute to incorrect classes.\n",
        "    '''\n",
        "    #TODO, needs revamping\n",
        "    #The weight manipulation should only happen during forward()\n",
        "    raise ValueError(\"error, needs revamping\")\n",
        "    for i in range(self.ca_weight.shape[0]):\n",
        "      for j in range(self.ca_weight.shape[1]):\n",
        "        if j != case_labels[i]:\n",
        "          self.ca_weight[i][j] = 0 - torch.relu(self.ca_weight[i][j])\n",
        "  def get_constrained_weight(self):\n",
        "    constrained_weight = self.mask * torch.relu(self.ca_weight)\n",
        "    return constrained_weight;\n",
        "  def forward(self, ca):\n",
        "    constrained_weight = self.get_constrained_weight();\n",
        "    return torch.matmul(ca, constrained_weight) + self.bias #matrix multiplication will handle 1 dimension vector property"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgd-UBbdbU5N"
      },
      "source": [
        "##Regression Activation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDmkcY7-rfHQ"
      },
      "outputs": [],
      "source": [
        "class CustomSoftmaxLayer(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomSoftmaxLayer, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    total = torch.sum(x, dim=1, keepdim=True)\n",
        "    softmax_output = x / (total + 1e-10)\n",
        "\n",
        "    ##IMPORTANT, cannot use the softmax version below, because of dividing by 0 problem.\n",
        "\n",
        "    # # Apply the exponential function to the input tensor\n",
        "    # exp_x = torch.exp(x)\n",
        "\n",
        "    # # Apply a mask to set the output to 0 where the input is 0\n",
        "    # mask = (x != 0).float()  # Create a binary mask (1 where x is not 0, 0 where x is 0)\n",
        "\n",
        "    # # Apply the mask to the output\n",
        "    # softmax_output = exp_x * mask\n",
        "\n",
        "    # # Normalize the output by dividing by the sum along the same axis\n",
        "    # softmax_output = softmax_output / softmax_output.sum(dim=1, keepdim=True)\n",
        "\n",
        "    return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbNCIgy0aqvW"
      },
      "outputs": [],
      "source": [
        "# prompt: Build a regression activation layer just like the class activation layer above, but for the purpose of regression now\n",
        "## right now this is essentially a linear layer, but somehow it's a lot slower than a linear layer,\n",
        "class RegressionActivation_1_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "    self.ca_weight = torch.nn.Parameter(torch.randn((num_cases, self.num_classes))) ## should I use randn here?\n",
        "    self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "  def forward(self, ca):\n",
        "    ##IMPORTANT, newly added \"* case_labels\" here, so that case labels is now considered in\n",
        "    ##the last layer of NN-k-NN regressor.\n",
        "    # print(\"debugg msaafasdf\")\n",
        "    # print(ca.shape)\n",
        "    # print(self.ca_weight.shape)\n",
        "    # print(self.case_labels.shape)\n",
        "    # print((torch.matmul(ca * self.case_labels, self.ca_weight)).shape)\n",
        "    return torch.matmul(ca, self.ca_weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDbeAaCorwYr"
      },
      "outputs": [],
      "source": [
        "class RegressionActivation_2_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "    self.ca_weight = torch.nn.Parameter(torch.ones((num_cases, self.num_classes))) ## should I use randn here?\n",
        "    self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "  def forward(self, ca):\n",
        "    return torch.matmul(ca* self.case_labels, self.ca_weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkoZwOlQwMlS"
      },
      "outputs": [],
      "source": [
        "class RegressionActivation_3_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "  def forward(self, ca):\n",
        "    return torch.matmul(ca, self.case_labels) #+ self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0LLPPXBsGFj"
      },
      "outputs": [],
      "source": [
        "ca = torch.Tensor([1 , 0.5, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfvItiQqsQdQ",
        "outputId": "6768b33a-bc33-475d-c6c9-4e070845dc5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "ca.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDvdr2TIsSMK"
      },
      "outputs": [],
      "source": [
        "case_labels = torch.Tensor([0,2,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clJvDPlLsfsq",
        "outputId": "02701c6d-b879-443d-ce02-3e443050a198"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "torch.matmul(ca, case_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo1F5P4s8Ed3"
      },
      "outputs": [],
      "source": [
        "class RegressionActivation_4_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "    self.ca_weight = torch.nn.Parameter(torch.tensor(case_labels).reshape(num_cases, self.num_classes)) ## should I use randn here?\n",
        "    self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "  def forward(self, ca):\n",
        "    ##IMPORTANT, newly added \"* case_labels\" here, so that case labels is now considered in\n",
        "    ##the last layer of NN-k-NN regressor.\n",
        "    # print(\"debugg msaafasdf\")\n",
        "    # print(ca.shape)\n",
        "    # print(self.ca_weight.shape)\n",
        "    # print(self.case_labels.shape)\n",
        "    # print((torch.matmul(ca * self.case_labels, self.ca_weight)).shape)\n",
        "    return torch.matmul(ca, self.ca_weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQgug4eSyKDv"
      },
      "outputs": [],
      "source": [
        "a = torch.ones((100, 8))\n",
        "b = torch.ones((1548, 8))\n",
        "c = torch.ones((1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwaQh8NUNDLv"
      },
      "outputs": [],
      "source": [
        "# a-b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3afere_KfLO"
      },
      "outputs": [],
      "source": [
        "# (b*b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5qqmJxXyQsd"
      },
      "outputs": [],
      "source": [
        "# (a*b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bITJLDhAySo8"
      },
      "outputs": [],
      "source": [
        "# torch.matmul(a, b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvvVhcBW241Z"
      },
      "outputs": [],
      "source": [
        "# torch.matmul(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcM3JXmz26Cs"
      },
      "outputs": [],
      "source": [
        "# torch.matmul(a, b) + c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmhxrGP2Jj6T"
      },
      "source": [
        "## NN-k-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9HNVfwjPV8E"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class NN_k_NN(torch.nn.Module):\n",
        "  def __init__(self, cases, case_labels,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing, hidden_layers= None):\n",
        "    super().__init__()\n",
        "    self.cases = cases\n",
        "    num_cases = cases.shape[0]\n",
        "\n",
        "    num_features = cases.shape[1]\n",
        "    if not (hidden_layers is None):\n",
        "      num_features = hidden_layers(cases).shape[1]\n",
        "    # num_classes = torch.unique(case_labels).shape[0] # len(set(case_labels))\n",
        "    ##IMPORTANT:: CHECK THE WEIGHT_SHARING PARAMETERS!!\n",
        "    # self.fa_layer = FeatureActivationLayer(num_features, num_cases,\n",
        "    #                                        weight_sharing_within_segment= fa_weight_sharing_within_segment,\n",
        "    #                                        weight_sharing_between_segment= fa_weight_sharing_between_segment,\n",
        "    #                                        hidden_layers = hidden_layers)\n",
        "    self.fa_layer = FeatureActivationLayer(num_features, num_cases, self.cases, hidden_layers = hidden_layers)\n",
        "\n",
        "    self.ca_layer = CaseActivationLayer(num_features, num_cases,\n",
        "                                        weight_sharing= ca_weight_sharing)\n",
        "    self.top_case_enabled = top_case_enabled\n",
        "    self.selection_layer = TopCaseLayer(top_k)\n",
        "    self.class_layer = ClassActivationLayer(num_cases, case_labels,\n",
        "                                            weight_sharing=class_weight_sharing)\n",
        "    ## more preprocessing here needed to tie each case to their correct class\n",
        "    ## only one is turned on right now for testing purpose.\n",
        "    self.class_layer.case_class_constrain_v1()\n",
        "    self.class_layer.case_class_constrain_v2()\n",
        "\n",
        "    self.ca_dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, query):\n",
        "    #repeat query so that it has the same shape as self.cases\n",
        "    # query = query.repeat(self.cases.shape[0], 1)\n",
        "    #no need to repeat the query, fa_layer's forward will handle it.\n",
        "    feature_activations = self.fa_layer(query, self.cases)\n",
        "    # print(\"feature_activations.shape\", feature_activations.shape)\n",
        "    ##tie the fa_segments with corresponding case in ca layer\n",
        "    #each segment should only tie to one case in ca_layer\n",
        "    case_activations = self.ca_layer(feature_activations)\n",
        "    # print(\"case_activations\" , case_activations.shape)\n",
        "    #uncomment to enable topk\n",
        "    if self.top_case_enabled:\n",
        "      case_activations = self.selection_layer(case_activations)\n",
        "\n",
        "    if self.training:\n",
        "      ##  case_activations = self.ca_dropout(case_activations)\n",
        "      ## print(\"dropout\")\n",
        "      ##==========#\n",
        "      ## expect the case_activations has the shape of (query_number, case_numer)\n",
        "      ## expect the query has the shape of (batch_size, feature_size)\n",
        "      ## expect the cases has the shape of (case_number, feature_size)\n",
        "      dquery = query.unsqueeze(1)\n",
        "      dcases = self.cases.unsqueeze(0)\n",
        "      difference = torch.sum(dquery - dcases, dim=-1)\n",
        "      masks = torch.where(difference == 0, 0., 1.)\n",
        "      case_activations = case_activations * masks\n",
        "\n",
        "\n",
        "    # print(\"case_activations\" , case_activations.shape)\n",
        "    class_ativations = self.class_layer(case_activations)\n",
        "    # print(\"class_ativations.shape\", class_ativations.shape)\n",
        "\n",
        "    # Softmax activation for classification\n",
        "    # output = F.softmax(class_ativations, dim=1)\n",
        "    output = class_ativations\n",
        "\n",
        "    # Get the class predictions (class with the highest probability)\n",
        "    ## TODO:: need debug here, am I using the right dim?\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "    return feature_activations, case_activations, output, predicted_class\n",
        "    # return self.class_layer(ca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukjmB70eHr7a"
      },
      "source": [
        "##NN-k-NN regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD1WpUjVHV58"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "##IMPORTANT, cannot use top case selection layer here like we do in for classification.\n",
        "## it has to be either enabled or disabled the whole time.\n",
        "class NN_k_NN_regression(torch.nn.Module):\n",
        "  def __init__(self, cases, case_labels,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing, hidden_layers= None):\n",
        "    super().__init__()\n",
        "    self.cases = cases\n",
        "    num_cases = cases.shape[0]\n",
        "    num_features = cases.shape[1]\n",
        "    # num_classes = torch.unique(case_labels).shape[0] # len(set(case_labels))\n",
        "    ##IMPORTANT:: CHECK THE WEIGHT_SHARING PARAMETERS!!\n",
        "    # self.fa_layer = FeatureActivationLayer(num_features, num_cases,\n",
        "    #                                        weight_sharing_within_segment= fa_weight_sharing_within_segment,\n",
        "    #                                        weight_sharing_between_segment= fa_weight_sharing_between_segment,\n",
        "    #                                        hidden_layers = hidden_layers)\n",
        "    self.fa_layer = FeatureActivationLayer(num_features, num_cases, self.cases, hidden_layers = hidden_layers)\n",
        "    self.ca_layer = CaseActivationLayer(num_features, num_cases,\n",
        "                                        weight_sharing= ca_weight_sharing)\n",
        "    self.top_case_enabled = top_case_enabled\n",
        "    self.selection_layer = TopCaseLayer(top_k)\n",
        "\n",
        "    self.ca_dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    self.softmax = CustomSoftmaxLayer()\n",
        "    # self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    self.class_layer = RegressionActivation_3_Layer(num_cases, case_labels,weight_sharing=class_weight_sharing)\n",
        "    # self.class_layer = torch.nn.Linear(num_cases, 1)\n",
        "\n",
        "\n",
        "  def forward(self, query):\n",
        "    feature_activations = self.fa_layer(query, self.cases)\n",
        "    # print(\"feature_activations.shape\", feature_activations.shape)\n",
        "    ##tie the fa_segments with corresponding case in ca layer\n",
        "    #each segment should only tie to one case in ca_layer\n",
        "    case_activations = self.ca_layer(feature_activations)\n",
        "    # print(\"case_activations\" , case_activations.shape)\n",
        "    #uncomment to enable topk\n",
        "    if self.top_case_enabled:\n",
        "      case_activations = self.selection_layer(case_activations)\n",
        "    # print(\"case_activations1\" , torch.sum(case_activations, dim= 1))\n",
        "\n",
        "    if self.training:\n",
        "      # case_activations = self.ca_dropout(case_activations)\n",
        "      # print(\"dropout\")\n",
        "      # ==========#\n",
        "      # expect the case_activations has the shape of (query_number, case_numer)\n",
        "      # expect the query has the shape of (batch_size, feature_size)\n",
        "      # expect the cases has the shape of (case_number, feature_size)\n",
        "      dquery = query.unsqueeze(1)\n",
        "      dcases = self.cases.unsqueeze(0)\n",
        "      difference = torch.sum(dquery - dcases, dim=-1)\n",
        "      masks = torch.where(difference == 0, 0., 1.)\n",
        "      case_activations = case_activations * masks\n",
        "\n",
        "    case_activations = self.softmax(case_activations)\n",
        "    #get top k of case_activations\n",
        "    values, indices = torch.topk(case_activations, k=self.selection_layer.k, dim=1)\n",
        "\n",
        "    output = torch.mean(self.class_layer.case_labels[indices], dim=1)\n",
        "    # print(\"case_activations\" , torch.sum(case_activations, dim= 1))\n",
        "    predicted_number = self.class_layer(case_activations)\n",
        "\n",
        "    # # Softmax activation for classification\n",
        "    # output = F.softmax(class_ativations, dim=1)\n",
        "    # # output = class_ativations\n",
        "    # _, predicted_class = torch.max(output, 1)\n",
        "    # print(\"predicted_class\", predicted_class)\n",
        "    # predicted_class = case_labels[predicted_class]\n",
        "    # print(\"predicted_class after label lookup\", predicted_class)\n",
        "    ##added feature activations here in output because they might be used for adaptation\n",
        "    ####VERY VERY IMPORTANT, the output format is different!!\n",
        "    return feature_activations, case_activations, output,  predicted_number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXaL3PmlBwg6"
      },
      "outputs": [],
      "source": [
        "# # prompt: a standard neural network with fully connected layers that has the same architecture as NN_k_NN_regression\n",
        "\n",
        "# class nn_regression(torch.nn.Module):\n",
        "#   def __init__(self, cases, case_labels,\n",
        "#                fa_weight_sharing_within_segment,\n",
        "#                fa_weight_sharing_between_segment,\n",
        "#                ca_weight_sharing,\n",
        "#                top_case_enabled, top_k,\n",
        "#                class_weight_sharing):\n",
        "#     super().__init__()\n",
        "#     ##WIP\n",
        "#     self.cases = torch.nn.parameters(torch.randn_like(cases))\n",
        "#     num_cases = cases.shape[0]\n",
        "#     num_features = cases.shape[1]\n",
        "#     # num_classes = torch.unique(case_labels).shape[0] # len(set(case_labels))\n",
        "#     ##IMPORTANT:: CHECK THE WEIGHT_SHARING PARAMETERS!!\n",
        "#     self.fa_layer = FeatureActivationLayer(num_features, num_cases, weight_sharing_within_segment= fa_weight_sharing_within_segment,\n",
        "#                                            weight_sharing_between_segment= fa_weight_sharing_between_segment)\n",
        "#     self.ca_layer = CaseActivationLayer(num_features, num_cases,\n",
        "#                                         weight_sharing= ca_weight_sharing)\n",
        "#     self.top_case_enabled = top_case_enabled\n",
        "#     self.selection_layer = TopCaseLayer(top_k)\n",
        "#     self.class_layer = RegressionActivationLayer(num_cases,weight_sharing=class_weight_sharing)\n",
        "\n",
        "\n",
        "#   def forward(self, query):\n",
        "#     feature_activations = self.fa_layer(query, self.cases)\n",
        "#     ##tie the fa_segments with corresponding case in ca layer\n",
        "#     #each segment should only tie to one case in ca_layer\n",
        "#     case_activations = self.ca_layer(feature_activations)\n",
        "\n",
        "#     #uncomment to enable topk\n",
        "#     if self.top_case_enabled:\n",
        "#       case_activations = self.selection_layer(case_activations)\n",
        "\n",
        "#     predicted_number = self.class_layer(case_activations)\n",
        "\n",
        "#     # # Softmax activation for classification\n",
        "#     # output = F.softmax(class_ativations, dim=1)\n",
        "#     # # output = class_ativations\n",
        "#     # _, predicted_class = torch.max(output, 1)\n",
        "#     # print(\"predicted_class\", predicted_class)\n",
        "#     # predicted_class = case_labels[predicted_class]\n",
        "#     # print(\"predicted_class after label lookup\", predicted_class)\n",
        "#     ##added feature activations here in output because they might be used for adaptation\n",
        "#     return feature_activations, case_activations, predicted_number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdFOP30kurhF"
      },
      "outputs": [],
      "source": [
        "def print_model_features(input_model):\n",
        "  for n, p in model.named_parameters():\n",
        "    print(n)\n",
        "    print(p.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJz8UivW3nYt"
      },
      "outputs": [],
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "input = torch.randn(2, 3)\n",
        "output = m(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pydml"
      ],
      "metadata": {
        "id": "aIg-EoJydTrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install metric-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzEt9iuYZvIy",
        "outputId": "cefe90ed-3b62-43a8-b8e2-baf1b1e75956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metric-learn\n",
            "  Downloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (3.4.0)\n",
            "Installing collected packages: metric-learn\n",
            "Successfully installed metric-learn-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import metric_learn"
      ],
      "metadata": {
        "id": "UpdWR_eh3c1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_0mocIXjT0"
      },
      "source": [
        "#Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification, depression\n",
        "\n",
        "From Zach Wilkerson, ICCBR challenge."
      ],
      "metadata": {
        "id": "j9FXMTYmttOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQS5emSOU4Ga",
        "outputId": "c2e4929c-55dd-4676-cd10-bbc6603d79af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2023 research/NN-kNN/Dataset_MO_ENG.csv\")"
      ],
      "metadata": {
        "id": "betH4sOCu1i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## eliminating physical-related questions\n",
        "df = df.drop(df.columns[102:-1], axis=1)\n",
        "\n",
        "\n",
        "## Creating classes 0-> Low risk, 1->Medium Risk, 2->High risk\n",
        "dic = { 1: 0 , 2: 0, 3:1, 4:2, 5:2}\n",
        "df['Target'] = df['Target'].map(dic)\n",
        "\n",
        "train_cols = df.columns[0:-1]\n",
        "label = df.columns[-1]\n",
        "X = df[train_cols]\n",
        "y = df[label]\n",
        "target_names=[\"Low\",\"Medium\",\"High\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "Ok5PiJQXu-qp",
        "outputId": "511642de-0c19-4275-e44e-e633b1bfcdc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     1. Most of the time I have difficulty concentrating on simple tasks  \\\n",
              "0                                                    1                     \n",
              "1                                                    1                     \n",
              "2                                                    0                     \n",
              "3                                                    0                     \n",
              "4                                                    1                     \n",
              "..                                                 ...                     \n",
              "99                                                   1                     \n",
              "100                                                  1                     \n",
              "101                                                  0                     \n",
              "102                                                  1                     \n",
              "103                                                  1                     \n",
              "\n",
              "     2. I don't feel like doing my daily duties  \\\n",
              "0                                             1   \n",
              "1                                             0   \n",
              "2                                             0   \n",
              "3                                             0   \n",
              "4                                             1   \n",
              "..                                          ...   \n",
              "99                                            1   \n",
              "100                                           1   \n",
              "101                                           0   \n",
              "102                                           0   \n",
              "103                                           1   \n",
              "\n",
              "     3. My friends or family have told me that I look different  \\\n",
              "0                                                    0            \n",
              "1                                                    1            \n",
              "2                                                    0            \n",
              "3                                                    0            \n",
              "4                                                    0            \n",
              "..                                                 ...            \n",
              "99                                                   1            \n",
              "100                                                  0            \n",
              "101                                                  1            \n",
              "102                                                  0            \n",
              "103                                                  0            \n",
              "\n",
              "     4. When I think about the future it is difficult for me to imagine it clearly  \\\n",
              "0                                                    1                               \n",
              "1                                                    1                               \n",
              "2                                                    0                               \n",
              "3                                                    1                               \n",
              "4                                                    1                               \n",
              "..                                                 ...                               \n",
              "99                                                   0                               \n",
              "100                                                  0                               \n",
              "101                                                  1                               \n",
              "102                                                  0                               \n",
              "103                                                  0                               \n",
              "\n",
              "     5. People around me often ask me how I feel  \\\n",
              "0                                              0   \n",
              "1                                              0   \n",
              "2                                              1   \n",
              "3                                              0   \n",
              "4                                              0   \n",
              "..                                           ...   \n",
              "99                                             0   \n",
              "100                                            1   \n",
              "101                                            1   \n",
              "102                                            1   \n",
              "103                                            0   \n",
              "\n",
              "     6. I consider that my life is full of good things  \\\n",
              "0                                                    1   \n",
              "1                                                    1   \n",
              "2                                                    1   \n",
              "3                                                    1   \n",
              "4                                                    1   \n",
              "..                                                 ...   \n",
              "99                                                   1   \n",
              "100                                                  1   \n",
              "101                                                  1   \n",
              "102                                                  1   \n",
              "103                                                  1   \n",
              "\n",
              "     7. My hobbies are still important to me  \\\n",
              "0                                          1   \n",
              "1                                          1   \n",
              "2                                          1   \n",
              "3                                          1   \n",
              "4                                          0   \n",
              "..                                       ...   \n",
              "99                                         1   \n",
              "100                                        1   \n",
              "101                                        1   \n",
              "102                                        1   \n",
              "103                                        1   \n",
              "\n",
              "     8. I'm still as punctual as I have always been  \\\n",
              "0                                                 1   \n",
              "1                                                 1   \n",
              "2                                                 0   \n",
              "3                                                 1   \n",
              "4                                                 1   \n",
              "..                                              ...   \n",
              "99                                                1   \n",
              "100                                               0   \n",
              "101                                               0   \n",
              "102                                               1   \n",
              "103                                               0   \n",
              "\n",
              "     9. If I had the chance, I would spend all day in my bed  \\\n",
              "0                                                    0         \n",
              "1                                                    0         \n",
              "2                                                    0         \n",
              "3                                                    1         \n",
              "4                                                    1         \n",
              "..                                                 ...         \n",
              "99                                                   0         \n",
              "100                                                  0         \n",
              "101                                                  1         \n",
              "102                                                  1         \n",
              "103                                                  0         \n",
              "\n",
              "     10. I have found that I can spend a lot of time scrolling the screen \\nof my cell phone without searching or stopping at anything in particular  \\\n",
              "0                                                    0                                                                                                 \n",
              "1                                                    0                                                                                                 \n",
              "2                                                    1                                                                                                 \n",
              "3                                                    0                                                                                                 \n",
              "4                                                    1                                                                                                 \n",
              "..                                                 ...                                                                                                 \n",
              "99                                                   1                                                                                                 \n",
              "100                                                  0                                                                                                 \n",
              "101                                                  1                                                                                                 \n",
              "102                                                  1                                                                                                 \n",
              "103                                                  1                                                                                                 \n",
              "\n",
              "     ...  94. I lose control easily  \\\n",
              "0    ...                          1   \n",
              "1    ...                          0   \n",
              "2    ...                          0   \n",
              "3    ...                          0   \n",
              "4    ...                          0   \n",
              "..   ...                        ...   \n",
              "99   ...                          0   \n",
              "100  ...                          0   \n",
              "101  ...                          0   \n",
              "102  ...                          0   \n",
              "103  ...                          0   \n",
              "\n",
              "     95. Neighbors must put up with each other's noises without complaining  \\\n",
              "0                                                    1                        \n",
              "1                                                    1                        \n",
              "2                                                    1                        \n",
              "3                                                    1                        \n",
              "4                                                    1                        \n",
              "..                                                 ...                        \n",
              "99                                                   1                        \n",
              "100                                                  1                        \n",
              "101                                                  1                        \n",
              "102                                                  1                        \n",
              "103                                                  1                        \n",
              "\n",
              "     96. Littering on public roads is wrong  \\\n",
              "0                                         0   \n",
              "1                                         0   \n",
              "2                                         0   \n",
              "3                                         0   \n",
              "4                                         0   \n",
              "..                                      ...   \n",
              "99                                        1   \n",
              "100                                       0   \n",
              "101                                       0   \n",
              "102                                       1   \n",
              "103                                       0   \n",
              "\n",
              "     97. People who commit crimes have their reasons for doing it  \\\n",
              "0                                                    0              \n",
              "1                                                    0              \n",
              "2                                                    0              \n",
              "3                                                    1              \n",
              "4                                                    0              \n",
              "..                                                 ...              \n",
              "99                                                   1              \n",
              "100                                                  1              \n",
              "101                                                  0              \n",
              "102                                                  0              \n",
              "103                                                  0              \n",
              "\n",
              "     98. It is normal to change jobs several times a year  \\\n",
              "0                                                    1      \n",
              "1                                                    1      \n",
              "2                                                    1      \n",
              "3                                                    1      \n",
              "4                                                    1      \n",
              "..                                                 ...      \n",
              "99                                                   1      \n",
              "100                                                  1      \n",
              "101                                                  1      \n",
              "102                                                  1      \n",
              "103                                                  1      \n",
              "\n",
              "     99. It is important to respect turns  \\\n",
              "0                                       0   \n",
              "1                                       0   \n",
              "2                                       0   \n",
              "3                                       0   \n",
              "4                                       0   \n",
              "..                                    ...   \n",
              "99                                      1   \n",
              "100                                     0   \n",
              "101                                     1   \n",
              "102                                     0   \n",
              "103                                     0   \n",
              "\n",
              "     100. I could pretend to be someone else to achieve what I want  \\\n",
              "0                                                    1                \n",
              "1                                                    1                \n",
              "2                                                    1                \n",
              "3                                                    1                \n",
              "4                                                    1                \n",
              "..                                                 ...                \n",
              "99                                                   1                \n",
              "100                                                  0                \n",
              "101                                                  1                \n",
              "102                                                  1                \n",
              "103                                                  1                \n",
              "\n",
              "     101. I consider it important that all people have the same rights  \\\n",
              "0                                                    0                   \n",
              "1                                                    1                   \n",
              "2                                                    0                   \n",
              "3                                                    0                   \n",
              "4                                                    0                   \n",
              "..                                                 ...                   \n",
              "99                                                   0                   \n",
              "100                                                  0                   \n",
              "101                                                  1                   \n",
              "102                                                  1                   \n",
              "103                                                  0                   \n",
              "\n",
              "     102. I have a hard time taking  \"no\" for an answer  Target  \n",
              "0                                                    1        0  \n",
              "1                                                    0        0  \n",
              "2                                                    1        1  \n",
              "3                                                    1        2  \n",
              "4                                                    0        0  \n",
              "..                                                 ...      ...  \n",
              "99                                                   1        2  \n",
              "100                                                  0        1  \n",
              "101                                                  1        0  \n",
              "102                                                  0        2  \n",
              "103                                                  1        2  \n",
              "\n",
              "[104 rows x 103 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eea65e68-3c4a-4053-986d-48fbcd38641b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1. Most of the time I have difficulty concentrating on simple tasks</th>\n",
              "      <th>2. I don't feel like doing my daily duties</th>\n",
              "      <th>3. My friends or family have told me that I look different</th>\n",
              "      <th>4. When I think about the future it is difficult for me to imagine it clearly</th>\n",
              "      <th>5. People around me often ask me how I feel</th>\n",
              "      <th>6. I consider that my life is full of good things</th>\n",
              "      <th>7. My hobbies are still important to me</th>\n",
              "      <th>8. I'm still as punctual as I have always been</th>\n",
              "      <th>9. If I had the chance, I would spend all day in my bed</th>\n",
              "      <th>10. I have found that I can spend a lot of time scrolling the screen \\nof my cell phone without searching or stopping at anything in particular</th>\n",
              "      <th>...</th>\n",
              "      <th>94. I lose control easily</th>\n",
              "      <th>95. Neighbors must put up with each other's noises without complaining</th>\n",
              "      <th>96. Littering on public roads is wrong</th>\n",
              "      <th>97. People who commit crimes have their reasons for doing it</th>\n",
              "      <th>98. It is normal to change jobs several times a year</th>\n",
              "      <th>99. It is important to respect turns</th>\n",
              "      <th>100. I could pretend to be someone else to achieve what I want</th>\n",
              "      <th>101. I consider it important that all people have the same rights</th>\n",
              "      <th>102. I have a hard time taking  \"no\" for an answer</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>104 rows × 103 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eea65e68-3c4a-4053-986d-48fbcd38641b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eea65e68-3c4a-4053-986d-48fbcd38641b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eea65e68-3c4a-4053-986d-48fbcd38641b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-12b43f7f-1e23-4ee1-ba37-c292624db7dc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12b43f7f-1e23-4ee1-ba37-c292624db7dc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-12b43f7f-1e23-4ee1-ba37-c292624db7dc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_306affc4-1439-458e-8f60-64fe9549efec\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_306affc4-1439-458e-8f60-64fe9549efec button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_OxX3zJvI7t",
        "outputId": "5862b7c8-1415-4e27-c711-bfbe071bf391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Target\n",
              "0    39\n",
              "1    35\n",
              "2    30\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 13\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(random_state=random_state, k_neighbors=3)\n",
        "X, y = oversample.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "_u_ljx8mvChF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxo6BfGFvGbk",
        "outputId": "da243f3b-5b84-46e2-8cd2-5b5ab55876ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Target\n",
              "0    39\n",
              "1    39\n",
              "2    39\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert X and y into tensors\n",
        "\n",
        "Xs = torch.tensor(X.values).float()\n",
        "ys = torch.tensor(y.values).long()\n"
      ],
      "metadata": {
        "id": "kbF8eKpEVZqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression, depression\n",
        "\n",
        "From Zach Wilkerson, ICCBR challenge."
      ],
      "metadata": {
        "id": "Hvx2FEVOTufl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301bb4a5-a965-495c-dff9-4fc2ade6e701",
        "id": "hEhreZZMTufr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2023 research/NN-kNN/Dataset_MO_ENG.csv\")"
      ],
      "metadata": {
        "id": "zYC9NbrxTufs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## eliminating physical-related questions\n",
        "df = df.drop(df.columns[102:-1], axis=1)\n",
        "\n",
        "\n",
        "## Creating classes 0-> Low risk, 1->Medium Risk, 2->High risk\n",
        "# dic = { 1: 0 , 2: 0, 3:1, 4:2, 5:2}\n",
        "# df['Target'] = df['Target'].map(dic)\n",
        "\n",
        "train_cols = df.columns[0:-1]\n",
        "label = df.columns[-1]\n",
        "X = df[train_cols]\n",
        "y = df[label]\n",
        "target_names=[\"Low\",\"Medium\",\"High\"]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d45266-acfe-48ce-88cf-91bc614f3349",
        "id": "OL_0h2MYTufs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     1. Most of the time I have difficulty concentrating on simple tasks  \\\n",
              "0                                                    1                     \n",
              "1                                                    1                     \n",
              "2                                                    0                     \n",
              "3                                                    0                     \n",
              "4                                                    1                     \n",
              "..                                                 ...                     \n",
              "99                                                   1                     \n",
              "100                                                  1                     \n",
              "101                                                  0                     \n",
              "102                                                  1                     \n",
              "103                                                  1                     \n",
              "\n",
              "     2. I don't feel like doing my daily duties  \\\n",
              "0                                             1   \n",
              "1                                             0   \n",
              "2                                             0   \n",
              "3                                             0   \n",
              "4                                             1   \n",
              "..                                          ...   \n",
              "99                                            1   \n",
              "100                                           1   \n",
              "101                                           0   \n",
              "102                                           0   \n",
              "103                                           1   \n",
              "\n",
              "     3. My friends or family have told me that I look different  \\\n",
              "0                                                    0            \n",
              "1                                                    1            \n",
              "2                                                    0            \n",
              "3                                                    0            \n",
              "4                                                    0            \n",
              "..                                                 ...            \n",
              "99                                                   1            \n",
              "100                                                  0            \n",
              "101                                                  1            \n",
              "102                                                  0            \n",
              "103                                                  0            \n",
              "\n",
              "     4. When I think about the future it is difficult for me to imagine it clearly  \\\n",
              "0                                                    1                               \n",
              "1                                                    1                               \n",
              "2                                                    0                               \n",
              "3                                                    1                               \n",
              "4                                                    1                               \n",
              "..                                                 ...                               \n",
              "99                                                   0                               \n",
              "100                                                  0                               \n",
              "101                                                  1                               \n",
              "102                                                  0                               \n",
              "103                                                  0                               \n",
              "\n",
              "     5. People around me often ask me how I feel  \\\n",
              "0                                              0   \n",
              "1                                              0   \n",
              "2                                              1   \n",
              "3                                              0   \n",
              "4                                              0   \n",
              "..                                           ...   \n",
              "99                                             0   \n",
              "100                                            1   \n",
              "101                                            1   \n",
              "102                                            1   \n",
              "103                                            0   \n",
              "\n",
              "     6. I consider that my life is full of good things  \\\n",
              "0                                                    1   \n",
              "1                                                    1   \n",
              "2                                                    1   \n",
              "3                                                    1   \n",
              "4                                                    1   \n",
              "..                                                 ...   \n",
              "99                                                   1   \n",
              "100                                                  1   \n",
              "101                                                  1   \n",
              "102                                                  1   \n",
              "103                                                  1   \n",
              "\n",
              "     7. My hobbies are still important to me  \\\n",
              "0                                          1   \n",
              "1                                          1   \n",
              "2                                          1   \n",
              "3                                          1   \n",
              "4                                          0   \n",
              "..                                       ...   \n",
              "99                                         1   \n",
              "100                                        1   \n",
              "101                                        1   \n",
              "102                                        1   \n",
              "103                                        1   \n",
              "\n",
              "     8. I'm still as punctual as I have always been  \\\n",
              "0                                                 1   \n",
              "1                                                 1   \n",
              "2                                                 0   \n",
              "3                                                 1   \n",
              "4                                                 1   \n",
              "..                                              ...   \n",
              "99                                                1   \n",
              "100                                               0   \n",
              "101                                               0   \n",
              "102                                               1   \n",
              "103                                               0   \n",
              "\n",
              "     9. If I had the chance, I would spend all day in my bed  \\\n",
              "0                                                    0         \n",
              "1                                                    0         \n",
              "2                                                    0         \n",
              "3                                                    1         \n",
              "4                                                    1         \n",
              "..                                                 ...         \n",
              "99                                                   0         \n",
              "100                                                  0         \n",
              "101                                                  1         \n",
              "102                                                  1         \n",
              "103                                                  0         \n",
              "\n",
              "     10. I have found that I can spend a lot of time scrolling the screen \\nof my cell phone without searching or stopping at anything in particular  \\\n",
              "0                                                    0                                                                                                 \n",
              "1                                                    0                                                                                                 \n",
              "2                                                    1                                                                                                 \n",
              "3                                                    0                                                                                                 \n",
              "4                                                    1                                                                                                 \n",
              "..                                                 ...                                                                                                 \n",
              "99                                                   1                                                                                                 \n",
              "100                                                  0                                                                                                 \n",
              "101                                                  1                                                                                                 \n",
              "102                                                  1                                                                                                 \n",
              "103                                                  1                                                                                                 \n",
              "\n",
              "     ...  94. I lose control easily  \\\n",
              "0    ...                          1   \n",
              "1    ...                          0   \n",
              "2    ...                          0   \n",
              "3    ...                          0   \n",
              "4    ...                          0   \n",
              "..   ...                        ...   \n",
              "99   ...                          0   \n",
              "100  ...                          0   \n",
              "101  ...                          0   \n",
              "102  ...                          0   \n",
              "103  ...                          0   \n",
              "\n",
              "     95. Neighbors must put up with each other's noises without complaining  \\\n",
              "0                                                    1                        \n",
              "1                                                    1                        \n",
              "2                                                    1                        \n",
              "3                                                    1                        \n",
              "4                                                    1                        \n",
              "..                                                 ...                        \n",
              "99                                                   1                        \n",
              "100                                                  1                        \n",
              "101                                                  1                        \n",
              "102                                                  1                        \n",
              "103                                                  1                        \n",
              "\n",
              "     96. Littering on public roads is wrong  \\\n",
              "0                                         0   \n",
              "1                                         0   \n",
              "2                                         0   \n",
              "3                                         0   \n",
              "4                                         0   \n",
              "..                                      ...   \n",
              "99                                        1   \n",
              "100                                       0   \n",
              "101                                       0   \n",
              "102                                       1   \n",
              "103                                       0   \n",
              "\n",
              "     97. People who commit crimes have their reasons for doing it  \\\n",
              "0                                                    0              \n",
              "1                                                    0              \n",
              "2                                                    0              \n",
              "3                                                    1              \n",
              "4                                                    0              \n",
              "..                                                 ...              \n",
              "99                                                   1              \n",
              "100                                                  1              \n",
              "101                                                  0              \n",
              "102                                                  0              \n",
              "103                                                  0              \n",
              "\n",
              "     98. It is normal to change jobs several times a year  \\\n",
              "0                                                    1      \n",
              "1                                                    1      \n",
              "2                                                    1      \n",
              "3                                                    1      \n",
              "4                                                    1      \n",
              "..                                                 ...      \n",
              "99                                                   1      \n",
              "100                                                  1      \n",
              "101                                                  1      \n",
              "102                                                  1      \n",
              "103                                                  1      \n",
              "\n",
              "     99. It is important to respect turns  \\\n",
              "0                                       0   \n",
              "1                                       0   \n",
              "2                                       0   \n",
              "3                                       0   \n",
              "4                                       0   \n",
              "..                                    ...   \n",
              "99                                      1   \n",
              "100                                     0   \n",
              "101                                     1   \n",
              "102                                     0   \n",
              "103                                     0   \n",
              "\n",
              "     100. I could pretend to be someone else to achieve what I want  \\\n",
              "0                                                    1                \n",
              "1                                                    1                \n",
              "2                                                    1                \n",
              "3                                                    1                \n",
              "4                                                    1                \n",
              "..                                                 ...                \n",
              "99                                                   1                \n",
              "100                                                  0                \n",
              "101                                                  1                \n",
              "102                                                  1                \n",
              "103                                                  1                \n",
              "\n",
              "     101. I consider it important that all people have the same rights  \\\n",
              "0                                                    0                   \n",
              "1                                                    1                   \n",
              "2                                                    0                   \n",
              "3                                                    0                   \n",
              "4                                                    0                   \n",
              "..                                                 ...                   \n",
              "99                                                   0                   \n",
              "100                                                  0                   \n",
              "101                                                  1                   \n",
              "102                                                  1                   \n",
              "103                                                  0                   \n",
              "\n",
              "     102. I have a hard time taking  \"no\" for an answer  Target  \n",
              "0                                                    1        2  \n",
              "1                                                    0        2  \n",
              "2                                                    1        3  \n",
              "3                                                    1        5  \n",
              "4                                                    0        2  \n",
              "..                                                 ...      ...  \n",
              "99                                                   1        5  \n",
              "100                                                  0        3  \n",
              "101                                                  1        2  \n",
              "102                                                  0        4  \n",
              "103                                                  1        4  \n",
              "\n",
              "[104 rows x 103 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05dcec5f-0c67-446b-ae67-adccf87aff58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1. Most of the time I have difficulty concentrating on simple tasks</th>\n",
              "      <th>2. I don't feel like doing my daily duties</th>\n",
              "      <th>3. My friends or family have told me that I look different</th>\n",
              "      <th>4. When I think about the future it is difficult for me to imagine it clearly</th>\n",
              "      <th>5. People around me often ask me how I feel</th>\n",
              "      <th>6. I consider that my life is full of good things</th>\n",
              "      <th>7. My hobbies are still important to me</th>\n",
              "      <th>8. I'm still as punctual as I have always been</th>\n",
              "      <th>9. If I had the chance, I would spend all day in my bed</th>\n",
              "      <th>10. I have found that I can spend a lot of time scrolling the screen \\nof my cell phone without searching or stopping at anything in particular</th>\n",
              "      <th>...</th>\n",
              "      <th>94. I lose control easily</th>\n",
              "      <th>95. Neighbors must put up with each other's noises without complaining</th>\n",
              "      <th>96. Littering on public roads is wrong</th>\n",
              "      <th>97. People who commit crimes have their reasons for doing it</th>\n",
              "      <th>98. It is normal to change jobs several times a year</th>\n",
              "      <th>99. It is important to respect turns</th>\n",
              "      <th>100. I could pretend to be someone else to achieve what I want</th>\n",
              "      <th>101. I consider it important that all people have the same rights</th>\n",
              "      <th>102. I have a hard time taking  \"no\" for an answer</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>104 rows × 103 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05dcec5f-0c67-446b-ae67-adccf87aff58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-05dcec5f-0c67-446b-ae67-adccf87aff58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-05dcec5f-0c67-446b-ae67-adccf87aff58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7987b99-7f68-445b-9d77-bb88b866acea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7987b99-7f68-445b-9d77-bb88b866acea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7987b99-7f68-445b-9d77-bb88b866acea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9aeedc75-b8f7-41f3-ab08-e6f491da872c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9aeedc75-b8f7-41f3-ab08-e6f491da872c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d808e0-893f-4f39-b8c9-22b8b4ea5cf9",
        "id": "nU0koDd1Tufs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Target\n",
              "2    35\n",
              "3    35\n",
              "4    22\n",
              "5     8\n",
              "1     4\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 13\n",
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(random_state=random_state, k_neighbors=3)\n",
        "X, y = oversample.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "MNsZhRFzTufs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e9b917-d8db-47aa-9b7c-a21fa2e9e6cc",
        "id": "NWsRUfQJTufs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Target\n",
              "2    35\n",
              "3    35\n",
              "5    35\n",
              "1    35\n",
              "4    35\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert X and y into tensors\n",
        "\n",
        "Xs = torch.tensor(X.values).float()\n",
        "ys = torch.tensor(y.values).long()\n"
      ],
      "metadata": {
        "id": "C5oG0i9xTuft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## only use this if you want to treat it as a regression problem.\n",
        "ys = ys.float()"
      ],
      "metadata": {
        "id": "Bw9t9I-4Tuft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification, CIFAR-10\n",
        "\n"
      ],
      "metadata": {
        "id": "Dk-QmZsZYYBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE:: Tried using the CIFAR-10. Too big to run."
      ],
      "metadata": {
        "id": "3hlhvTLGIoVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prep data and conv extractor"
      ],
      "metadata": {
        "id": "WYNusXh2LpN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description of this data set can be found on https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "According to this page:\n",
        "\n",
        "\"Baseline results\n",
        "\n",
        "You can find some baseline replicable results on this dataset on the project page for cuda-convnet. These results were obtained with a convolutional neural network. Briefly, they are 18% test error without data augmentation and 11% with. Additionally, Jasper Snoek has a new paper in which he used Bayesian hyperparameter optimization to find nice settings of the weight decay and other hyperparameters, which allowed him to obtain a test error rate of 15% (without data augmentation) using the architecture of the net that got 18%.\""
      ],
      "metadata": {
        "id": "mQokwlyrZHZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code in this section is based on https://colab.research.google.com/drive/1js_1rFZist_-rfNwmVZbXj0m7hKyX7xn#scrollTo=IR1Do_wt14ZE\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "it is a pytorch tutorial named cifar10_tutorial.ipynb"
      ],
      "metadata": {
        "id": "moBnRJmYYj2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "uQoM3PJKYXqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm9QR10214ZD"
      },
      "source": [
        "The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "We transform them to Tensors of normalized range [-1, 1].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR1Do_wt14ZE"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get trainset into Xs and ys as tensors\n",
        "\n",
        "X_train, y_train = [], []\n",
        "for x, y in trainloader:\n",
        "  X_train.append(x)\n",
        "  y_train.append(y)\n",
        "X_train = torch.cat(X_train, dim=0)\n",
        "y_train = torch.cat(y_train, dim=0)\n",
        "\n",
        "X_test, y_test = [], []\n",
        "for x, y in testloader:\n",
        "  X_test.append(x)\n",
        "  y_test.append(y)\n",
        "X_test = torch.cat(X_test, dim=0)\n",
        "y_test = torch.cat(y_test, dim=0)"
      ],
      "metadata": {
        "id": "PUa_UYLuY6Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "SVy94FiFLwYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "ESJRsxiKMdcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "XLPOsa8nMeqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQb0q_6l14ZE"
      },
      "source": [
        "Let us show some of the training images, for fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTAbYR-b14ZE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv_cifar10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_cifar10, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        #QUESTION:: should we have fc layers here?\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "XmbIGlo4bTNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_cifar10(model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  return correct, total"
      ],
      "metadata": {
        "id": "9QNRTwySd1br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ConvNN baseline"
      ],
      "metadata": {
        "id": "8bCQcmRq4_r6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3giYTxdX14ZE"
      },
      "source": [
        "2. Define a Convolution Neural Network\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "Copy the neural network from the Neural Networks section before and modify it to\n",
        "take 3-channel images (instead of 1-channel images as it was defined).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-EaNXuy14ZF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx0hm3BU14ZF"
      },
      "source": [
        "3. Define a Loss function and optimizer\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nde2FNH14ZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmOAkyw-14ZG"
      },
      "source": [
        "4. Train the network\n",
        "^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BenR4VWn14ZG"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0  # best test accuracy\n",
        "patience_counter = 0\n",
        "for epoch in range(1000):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "    # evaluate on test set\n",
        "    correct, total = eval_cifar10(net)\n",
        "    best_accuracy = max(best_accuracy, correct / total)\n",
        "    if best_accuracy == correct / total:\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter > 40:\n",
        "        break\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpNNk4U814ZG"
      },
      "source": [
        "5. Test the network on the test data\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "We have trained the network for 2 passes over the training dataset.\n",
        "But we need to check if the network has learnt anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network\n",
        "outputs, and checking it against the ground-truth. If the prediction is\n",
        "correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbG-_1fn14ZG"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRLQb7dw14ZG"
      },
      "source": [
        "Okay, now let us see what the neural network thinks these examples above are:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7BwKQcX14ZG"
      },
      "outputs": [],
      "source": [
        "outputs = net(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioYdIzMf14ZG"
      },
      "source": [
        "The outputs are energies for the 10 classes.\n",
        "Higher the energy for a class, the more the network\n",
        "thinks that the image is of the particular class.\n",
        "So, let's get the index of the highest energy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMhPxaaI14ZG"
      },
      "outputs": [],
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yRpa8Bj14ZH"
      },
      "source": [
        "The results seem pretty good.\n",
        "\n",
        "Let us look at how the network performs on the whole dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V01f9Nq14ZH"
      },
      "outputs": [],
      "source": [
        "\n",
        "correct, total = eval_cifar10(net)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CGILJan14ZH"
      },
      "source": [
        "That looks waaay better than chance, which is 10% accuracy (randomly picking\n",
        "a class out of 10 classes).\n",
        "Seems like the network learnt something.\n",
        "\n",
        "Hmmm, what are the classes that performed well, and the classes that did\n",
        "not perform well:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-KgaElM14ZH"
      },
      "outputs": [],
      "source": [
        "def eval_by_class(model_to_test):\n",
        "  class_correct = list(0. for i in range(10))\n",
        "  class_total = list(0. for i in range(10))\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data\n",
        "          outputs = model_to_test(images)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          c = (predicted == labels).squeeze()\n",
        "          for i in range(4):\n",
        "              label = labels[i]\n",
        "              class_correct[label] += c[i].item()\n",
        "              class_total[label] += 1\n",
        "  return class_correct, class_total\n",
        "\n",
        "class_correct, class_total = eval_by_class(net)\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIDEqsvbfUMf"
      },
      "source": [
        "##Classification, Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2dEUwwWbL2M"
      },
      "outputs": [],
      "source": [
        "# prompt: load fashion mnist data set into Xs and ys\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load training set and test set\n",
        "trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW2UFYznfndb"
      },
      "outputs": [],
      "source": [
        "trainset[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnZSBxEWf_LH"
      },
      "outputs": [],
      "source": [
        "trainset[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJiR0Ti-v2gG"
      },
      "source": [
        "##Classification, data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8NANW0CwW5X"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "# sklearn.datasets.fetch_olivetti_faces\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "iris = load_breast_cancer()\n",
        "Xs = iris['data']\n",
        "ys = iris['target']\n",
        "# names = iris['target_names']\n",
        "# feature_names = iris['feature_names']\n",
        "\n",
        "## fit Xs in a scaler transform, transform Xs into 0-1\n",
        "# scaler = MinMaxScaler()\n",
        "# Xs = scaler.fit_transform(Xs)\n",
        "if Xs.shape[1] == 64: #for breast cancer\n",
        "  scaler = MinMaxScaler()\n",
        "  Xs = scaler.fit_transform(Xs)\n",
        "\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rLv9czNqfnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc46119-286d-425c-b1dc-7d433cf3900a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([569, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydE6eHnvb4Z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac8072e-d270-4d94-abb7-0b94b54da1b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7990e+01, 1.0380e+01, 1.2280e+02, 1.0010e+03, 1.1840e-01, 2.7760e-01,\n",
              "        3.0010e-01, 1.4710e-01, 2.4190e-01, 7.8710e-02, 1.0950e+00, 9.0530e-01,\n",
              "        8.5890e+00, 1.5340e+02, 6.3990e-03, 4.9040e-02, 5.3730e-02, 1.5870e-02,\n",
              "        3.0030e-02, 6.1930e-03, 2.5380e+01, 1.7330e+01, 1.8460e+02, 2.0190e+03,\n",
              "        1.6220e-01, 6.6560e-01, 7.1190e-01, 2.6540e-01, 4.6010e-01, 1.1890e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUGLcsqvC-TP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d2fd04-ca9f-4760-ff33-2ad6c35160a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "ys[0].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UFFbmbxFoYa"
      },
      "source": [
        "## Sanity check: A normal neural network classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0kqnMz5Ehs3"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.1, random_state=42, stratify=ys)\n",
        "\n",
        "# # Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Convert data to PyTorch tensors\n",
        "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# # Define the neural network architecture for classification\n",
        "# class NeuralNet(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_classes):\n",
        "#         super(NeuralNet, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# # Hyperparameters\n",
        "# input_size = X_train.shape[1]\n",
        "# hidden_size = 64\n",
        "# num_classes = torch.unique(ys).shape[0]\n",
        "# learning_rate = 0.001\n",
        "# batch_size = 16\n",
        "# epochs = 50\n",
        "\n",
        "# # Create DataLoader for training\n",
        "# train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Initialize the model, loss function, and optimizer\n",
        "# model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(epochs):\n",
        "#     for i, (inputs, labels) in enumerate(train_loader):\n",
        "#         # Forward pass\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backward and optimize\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if (i + 1) % 5 == 0:\n",
        "#             print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "# # Testing the model\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(X_test_tensor)\n",
        "#     _, predicted = torch.max(outputs, 1)\n",
        "#     accuracy = torch.sum(predicted == y_test_tensor).item() / len(y_test_tensor)\n",
        "#     print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "pK6bzvrPNSt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.1, random_state=42, stratify=ys)\n",
        "\n",
        "# Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = X_train\n",
        "y_train_tensor = y_train\n",
        "X_test_tensor = X_test\n",
        "y_test_tensor = y_test\n",
        "\n",
        "# Define the neural network architecture for classification\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size // 4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size // 4, hidden_size // 2),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 2),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "            )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 1024\n",
        "num_classes = torch.unique(ys).shape[0]\n",
        "learning_rate = 1e-5\n",
        "batch_size = 16\n",
        "epochs = 2000\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    accuracy = 0.0\n",
        "    total_num = 0\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        batch_size = len(labels)\n",
        "        total_num += batch_size\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy += torch.sum(predicted == labels).item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # if (i + 1) % 5 == 0\n",
        "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {total_loss/(i+1):.2f} Acc: {accuracy/total_num :.2f}\")\n",
        "    # Testing the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = torch.sum(predicted == y_test_tensor).item() / len(y_test_tensor)\n",
        "        print(f'Accuracy on the test set: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "-Anm2zhINjLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef536f0e-44dc-4a18-dd19-ac4474f0bbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.10 Acc: 0.31\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Epoch: 100, Loss: 0.12 Acc: 1.00\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 41.67%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 16.67%\n",
            "Accuracy on the test set: 16.67%\n",
            "Accuracy on the test set: 16.67%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 200, Loss: 0.01 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 300, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 400, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 500, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 600, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 700, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 800, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 900, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1000, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1100, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1200, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1300, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1400, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1500, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1600, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Epoch: 1700, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 25.00%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Epoch: 1800, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Epoch: 1900, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Accuracy on the test set: 33.33%\n",
            "Epoch: 2000, Loss: 0.00 Acc: 1.00\n",
            "Accuracy on the test set: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN_3_0zd1WD3"
      },
      "source": [
        "##Classification, Zebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-w9R0eu1Ys-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "e95fa0a7-0e33-4561-9076-e9d0dd3c7ecd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of points per class\n",
        "num_points = 110\n",
        "\n",
        "# Generate alternating classes along the X-axis\n",
        "x = np.linspace(0, 100, (int) (num_points/10))\n",
        "#repeat X 10 times\n",
        "x = np.repeat(x, 10)\n",
        "y = np.random.rand(num_points)*100\n",
        "labels = np.zeros(num_points)\n",
        "\n",
        "# Assign alternating classes\n",
        "labels[x%20 == 0] = 0\n",
        "labels[x%20 != 0] = 1\n",
        "\n",
        "# Plot the generated data\n",
        "plt.scatter(x, y, c=labels, cmap='viridis', marker='.')\n",
        "plt.title('Synthetic Zebra-Style Classification Dataset')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y (not relevant)')\n",
        "# plt.colorbar(ticks=[0, 1], label='Class')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJhXEd7TAoGT"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(np.column_stack((x, y)), dtype=torch.float32)\n",
        "ys = torch.tensor(labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0haNFtPWBILt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bfce41-1d96-4055-8454-250f25d51f46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([110, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 467
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl3ZyEY0CZY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bea12cc-492f-46af-b9bb-7de46a01e068"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 468
        }
      ],
      "source": [
        "ys[13]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lDS9m8xpXXt"
      },
      "source": [
        "##Classification, Special Zebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icfMjXivpkLI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "fc79af08-34dc-4460-9832-db35d63f5d75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of points per class\n",
        "num_points = 110\n",
        "\n",
        "# Generate alternating classes along the X-axis\n",
        "x1 = np.linspace(0, 100, (int) (num_points/10))\n",
        "#repeat X 10 times\n",
        "x1 = np.repeat(x1, 10)\n",
        "y1 = np.random.rand(num_points)*100\n",
        "labels1 = np.zeros(num_points)\n",
        "\n",
        "# Assign alternating classes\n",
        "labels1[x1%20 == 0] = 0\n",
        "labels1[x1%20 != 0] = 1\n",
        "\n",
        "# Generate alternating classes along the X-axis\n",
        "y2 = np.linspace(0, 100, (int) (num_points/10))\n",
        "#repeat X 10 times\n",
        "y2 = np.repeat(y2, 10)\n",
        "x2 = 100 + np.random.rand(num_points)*100\n",
        "labels2 = np.zeros(num_points)\n",
        "\n",
        "# Assign alternating classes\n",
        "labels2[y2%20 == 0] = 0\n",
        "labels2[y2%20 != 0] = 1\n",
        "\n",
        "x = np.concatenate((x1,x2))\n",
        "y = np.concatenate((y1,y2))\n",
        "labels = np.concatenate((labels1,labels2))\n",
        "\n",
        "\n",
        "# Plot the generated data\n",
        "plt.scatter(x, y, c=labels, cmap='viridis', marker='.')\n",
        "plt.title('Synthetic Zebra-Style Classification Dataset')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "# plt.colorbar(ticks=[0, 1], label='Class')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNjOT7AqpnTT"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(np.column_stack((x, y)), dtype=torch.float32)\n",
        "ys = torch.tensor(labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cZECSDXY7V"
      },
      "source": [
        "##Classification, Breast Cancer Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwIvazPxXYr0"
      },
      "outputs": [],
      "source": [
        "# prompt: load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "Xs = cancer.data\n",
        "ys = cancer.target\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powiAg8RL5L7"
      },
      "source": [
        "##Classification, BAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmIR8VjJL8ck"
      },
      "source": [
        "Ziwei: Download the file below from https://archive.ics.uci.edu/dataset/12/balance+scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hgdQkbqL7Wt"
      },
      "outputs": [],
      "source": [
        "with open(\"balance-scale.data\",\"r\") as filef:\n",
        "    bal_file = filef.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ39n1GOMElm"
      },
      "outputs": [],
      "source": [
        "Xs = []\n",
        "ys = []\n",
        "for line in bal_file:\n",
        "    Xs.append([int(line[2]),int(line[4]),int(line[6]),int(line[8])])\n",
        "    if line[0] == 'L':\n",
        "        ys.append(0)\n",
        "    elif line[0] == 'B':\n",
        "        ys.append(1)\n",
        "    elif line[0] == 'R':\n",
        "        ys.append(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_1K8YFPMGWt"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(Xs).float()\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6BE0VHTogJo"
      },
      "source": [
        "##Regression, California Housing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMOxxo_MKAci"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california_housing = fetch_california_housing()\n",
        "Xs = california_housing.data\n",
        "ys = california_housing.target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iix84gG6KLYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9133ff5b-2a40-422d-877a-cef3998b1bf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   8.3252    ,   41.        ,    6.98412698,    1.02380952,\n",
              "        322.        ,    2.55555556,   37.88      , -122.23      ])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN2dWEpzKM33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0413c768-2c8b-4558-9af4-61cc5a7e2004"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.526"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzGjqqSSV9Fu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffb2b95-be82-418b-c8aa-1509fcc3d877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.526  , 3.585  , 3.521  , 3.413  , 3.422  , 2.697  , 2.992  ,\n",
              "       2.414  , 2.267  , 2.611  , 2.815  , 2.418  , 2.135  , 1.913  ,\n",
              "       1.592  , 1.4    , 1.525  , 1.555  , 1.587  , 1.629  , 1.475  ,\n",
              "       1.598  , 1.139  , 0.997  , 1.326  , 1.075  , 0.938  , 1.055  ,\n",
              "       1.089  , 1.32   , 1.223  , 1.152  , 1.104  , 1.049  , 1.097  ,\n",
              "       0.972  , 1.045  , 1.039  , 1.914  , 1.76   , 1.554  , 1.5    ,\n",
              "       1.188  , 1.888  , 1.844  , 1.823  , 1.425  , 1.375  , 1.875  ,\n",
              "       1.125  , 1.719  , 0.938  , 0.975  , 1.042  , 0.875  , 0.831  ,\n",
              "       0.875  , 0.853  , 0.803  , 0.6    , 0.757  , 0.75   , 0.861  ,\n",
              "       0.761  , 0.735  , 0.784  , 0.844  , 0.813  , 0.85   , 1.292  ,\n",
              "       0.825  , 0.952  , 0.75   , 0.675  , 1.375  , 1.775  , 1.021  ,\n",
              "       1.083  , 1.125  , 1.313  , 1.625  , 1.125  , 1.125  , 1.375  ,\n",
              "       1.188  , 0.982  , 1.188  , 1.625  , 1.375  , 5.00001, 1.625  ,\n",
              "       1.375  , 1.625  , 1.875  , 1.792  , 1.3    , 1.838  , 1.25   ,\n",
              "       1.7    , 1.931  ])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "ys[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFuta_wnUjCE"
      },
      "outputs": [],
      "source": [
        "# prompt: Convert Xs and ys to tensors for pytorch\n",
        "\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJWpLsWXVZZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89e2487-27cf-47c9-e9b1-745d9a09a8bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20640, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poEqTR7bZuRf"
      },
      "outputs": [],
      "source": [
        "# prompt: downsize the data set to 10%\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "_, Xs, _, ys = train_test_split(Xs, ys, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB_cOi7sZ3rB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707bb410-9012-47ff-85a6-ff71cb486b09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2064, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdMkoZr_KgZX"
      },
      "source": [
        "##Regression, Abalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTi9rxq1Kfl5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from urllib import request\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "\n",
        "# Download the Abalone dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
        "response = request.urlopen(url)\n",
        "abalone_data = response.read().decode(\"utf-8\").splitlines()\n",
        "\n",
        "# Process and convert the data to PyTorch tensors\n",
        "data = [line.strip().split(',') for line in abalone_data]\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# categories = ['M', 'F', 'I']\n",
        "# label_encoder = OneHotEncoder(categories=[categories])\n",
        "# label_encoder.fit(data)\n",
        "\n",
        "def encode_sex(sex):\n",
        "    if sex == 'M':\n",
        "        return [1, 0, 0]\n",
        "    elif sex == 'F':\n",
        "        return [0, 1, 0]\n",
        "    elif sex == 'I':\n",
        "        return [0, 0, 1]\n",
        "\n",
        "for row in data:\n",
        "    # One-hot encode the 'Sex' feature\n",
        "    # sex_encoded = label_encoder.transform([[row[0]]])[0]\n",
        "    sex_encoded = encode_sex(row[0])\n",
        "\n",
        "    # Convert the row to float and extract the target variable ('Rings')\n",
        "    X.append(sex_encoded + list(map(float, row[1:-1])))\n",
        "    y.append(float(row[-1]))\n",
        "\n",
        "    # # Encode the categorical 'Sex' feature\n",
        "    # row[0] = label_encoder.transform([row[0]])[0]\n",
        "    # # Convert the row to float and extract the target variable ('Rings')\n",
        "    # X.append(list(map(float, row[:-1])))\n",
        "    # y.append(float(row[-1]))\n",
        "\n",
        "Xs = torch.tensor(X, dtype=torch.float32)\n",
        "ys = torch.tensor(y, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vxYpkSzeWw_"
      },
      "source": [
        "##Regression, Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP127Lw6gNFk"
      },
      "outputs": [],
      "source": [
        "# prompt: load the diabetes dataset from sklearn\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "Xs = diabetes.data\n",
        "ys = diabetes.target\n",
        "# prompt: convert Xs and ys to float tensor\n",
        "\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZdQOsd0gdG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fd2262-97ab-44ac-dd2d-d40cbfec556e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n",
              "         0.0199, -0.0176])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk8Iw3eL6SF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e3bfb2-c14b-490c-ecde-28a53254b7f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([0.1107, 0.0507, 0.1706, 0.1320, 0.1539, 0.1988, 0.1812, 0.1852, 0.1336,\n",
              "        0.1356]),\n",
              "indices=tensor([204,   0, 367, 340, 230, 123,  58, 123,  23,  23]))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "torch.max(Xs,dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTuguLpxgeVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bb0c81-030c-4e65-9c8c-a07b02f4f545"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(151.)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fprm-3uOgfbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c57effd-8001-40c7-9aef-5bbdd7937ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([442, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyOuSplUiUY"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ta_zinTqY48"
      },
      "source": [
        "##Regression, Patchy classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_4xNhFTfQP_"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZshHTL0qcnZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C29ELkyUUzEf"
      },
      "source": [
        "##Regression, Body Fat\n",
        "\n",
        "Tried this data set on multiple settings. Our NN-kNN consistently perform poorly on this when compared to other methods.\n",
        "\n",
        "https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset?resource=download\n",
        "\n",
        "https://www.kaggle.com/code/casper6290/bodyfat-prediction#1-|-Importing-Libraries-and-Loading-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmlUIHa3cFLy",
        "outputId": "6225c3e5-4412-4482-af11-9970424d47b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umDa4rqucbgk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "#\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2023 research/NN-kNN/bodyfat.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFrJNY2d4uL"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_j5YekSd_3A"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['BodyFat','Density'],axis=1)\n",
        "y = df['Density']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7BzcLAveA9Q"
      },
      "outputs": [],
      "source": [
        "X['Bmi']=703*X['Weight']/(X['Height']*X['Height'])\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tizcri8eDvY"
      },
      "outputs": [],
      "source": [
        "X['ACratio'] = X['Abdomen']/X['Chest']\n",
        "X['HTratio'] = X['Hip']/X['Thigh']\n",
        "X.drop(['Weight','Height','Abdomen','Chest','Hip','Thigh'],axis=1,inplace=True)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJUKzpoHeSax"
      },
      "outputs": [],
      "source": [
        "z = np.abs(stats.zscore(X))\n",
        "\n",
        "#only keep rows in dataframe with all z-scores less than absolute value of 3\n",
        "X_clean = X[(z<3).all(axis=1)]\n",
        "y_clean = y[(z<3).all(axis=1)]\n",
        "#find how many rows are left in the dataframe\n",
        "X_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4haAdPoVeXfc"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor( X_clean.to_numpy(), dtype=torch.float32)\n",
        "ys = torch.tensor( y_clean.to_numpy(), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efTT8qVCeipZ"
      },
      "outputs": [],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hTs_aikeg_p"
      },
      "outputs": [],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swy1EhJD31r6"
      },
      "source": [
        "##Regression, Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GLpgoy_36B9"
      },
      "outputs": [],
      "source": [
        "Xs = np.load(\"part_features.npy\")\n",
        "ys = np.load(\"part_targets.npy\")\n",
        "#These two files are in the nn-Knn folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z6Psjbh4CGD"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXPNaMjp4DpP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "_, Xs, _, ys = train_test_split(Xs, ys, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORxZVyTS4FWG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo4BdbmCFC0d"
      },
      "source": [
        "##Sanity Check: A standard NN for regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM_WhRJqD11s"
      },
      "outputs": [],
      "source": [
        "# prompt: a standard neural network with 3 fully connected layers for regression\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class RegressionNet(nn.Module):\n",
        "#   def __init__(self, input_size):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(input_size, 32)\n",
        "#     self.fc2 = nn.Linear(32, 16)\n",
        "#     self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = F.relu(self.fc1(x))\n",
        "#     x = F.relu(self.fc2(x))\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a standard neural network with 3 fully connected layers for regression\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class RegressionNet(nn.Module):\n",
        "#   def __init__(self, input_size):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(input_size, 32)\n",
        "#     self.fc2 = nn.Linear(32, 16)\n",
        "#     self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = F.relu(self.fc1(x))\n",
        "#     x = F.relu(self.fc2(x))\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n",
        "\n",
        "class RegressionNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionNet, self).__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(input_size, 32),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(32, 10),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(10, 1)\n",
        "\n",
        "            # nn.Linear(32, 64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # nn.Linear(64, 128),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # nn.Linear(128, 1)\n",
        "            )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x).squeeze()"
      ],
      "metadata": {
        "id": "k1g3svUFNrsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cme3KleQEdQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3b5549-35a9-476c-8492-bb21fd2eccfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/3000], Test Loss: 0.6423723101615906\n",
            "Epoch [200/3000], Test Loss: 0.45588478446006775\n",
            "Epoch [300/3000], Test Loss: 0.3575444519519806\n",
            "Epoch [400/3000], Test Loss: 0.29476022720336914\n",
            "Epoch [500/3000], Test Loss: 0.25905001163482666\n",
            "Epoch [600/3000], Test Loss: 0.23944370448589325\n",
            "Epoch [700/3000], Test Loss: 0.23291081190109253\n",
            "Epoch [100/3000], Test Loss: 0.3844171166419983\n",
            "Epoch [100/3000], Test Loss: 0.6953907608985901\n",
            "Epoch [200/3000], Test Loss: 0.5814568400382996\n",
            "Epoch [300/3000], Test Loss: 0.5507189631462097\n",
            "Epoch [100/3000], Test Loss: 0.9911725521087646\n",
            "Epoch [200/3000], Test Loss: 0.8221722841262817\n",
            "Epoch [300/3000], Test Loss: 0.6419295072555542\n",
            "Epoch [400/3000], Test Loss: 0.5331980586051941\n",
            "Epoch [500/3000], Test Loss: 0.4931984543800354\n",
            "Epoch [600/3000], Test Loss: 0.4732033312320709\n",
            "Epoch [700/3000], Test Loss: 0.4568718373775482\n",
            "Epoch [800/3000], Test Loss: 0.4418834447860718\n",
            "Epoch [900/3000], Test Loss: 0.430118203163147\n",
            "Epoch [1000/3000], Test Loss: 0.41960418224334717\n",
            "Epoch [1100/3000], Test Loss: 0.41405007243156433\n",
            "Epoch [100/3000], Test Loss: 0.7991136312484741\n",
            "Epoch [200/3000], Test Loss: 0.5428569912910461\n",
            "Epoch [300/3000], Test Loss: 0.44872331619262695\n",
            "Epoch [400/3000], Test Loss: 0.40909451246261597\n",
            "Epoch [500/3000], Test Loss: 0.3921073079109192\n",
            "Epoch [600/3000], Test Loss: 0.38249051570892334\n",
            "Epoch [700/3000], Test Loss: 0.3723190724849701\n",
            "Epoch [800/3000], Test Loss: 0.35298606753349304\n",
            "Epoch [900/3000], Test Loss: 0.3318910002708435\n",
            "Epoch [1000/3000], Test Loss: 0.3138778805732727\n",
            "Epoch [1100/3000], Test Loss: 0.2979242503643036\n",
            "Epoch [1200/3000], Test Loss: 0.28772932291030884\n",
            "Epoch [1300/3000], Test Loss: 0.28110599517822266\n",
            "Epoch [1400/3000], Test Loss: 0.27916809916496277\n",
            "Epoch [100/3000], Test Loss: 0.46088099479675293\n",
            "Epoch [200/3000], Test Loss: 0.37476813793182373\n",
            "Epoch [300/3000], Test Loss: 0.3029530942440033\n",
            "Epoch [400/3000], Test Loss: 0.24975891411304474\n",
            "Epoch [500/3000], Test Loss: 0.21890972554683685\n",
            "Epoch [600/3000], Test Loss: 0.1945917010307312\n",
            "Epoch [700/3000], Test Loss: 0.17385299503803253\n",
            "Epoch [800/3000], Test Loss: 0.16119062900543213\n",
            "Epoch [900/3000], Test Loss: 0.1527552306652069\n",
            "Epoch [100/3000], Test Loss: 1.0866233110427856\n",
            "Epoch [200/3000], Test Loss: 1.0182160139083862\n",
            "Epoch [300/3000], Test Loss: 0.9863619804382324\n",
            "Epoch [400/3000], Test Loss: 0.9378039240837097\n",
            "Epoch [500/3000], Test Loss: 0.8909115195274353\n",
            "Epoch [600/3000], Test Loss: 0.8731088042259216\n",
            "Epoch [100/3000], Test Loss: 1.5739227533340454\n",
            "Epoch [200/3000], Test Loss: 1.1440744400024414\n",
            "Epoch [300/3000], Test Loss: 0.8155997395515442\n",
            "Epoch [400/3000], Test Loss: 0.5653856992721558\n",
            "Epoch [500/3000], Test Loss: 0.44329530000686646\n",
            "Epoch [600/3000], Test Loss: 0.35733193159103394\n",
            "Epoch [700/3000], Test Loss: 0.3125115633010864\n",
            "Epoch [800/3000], Test Loss: 0.3065263330936432\n",
            "Epoch [100/3000], Test Loss: 1.086895227432251\n",
            "Epoch [200/3000], Test Loss: 1.0114021301269531\n",
            "Epoch [300/3000], Test Loss: 0.9426521062850952\n",
            "Epoch [400/3000], Test Loss: 0.8483902215957642\n",
            "Epoch [500/3000], Test Loss: 0.7744505405426025\n",
            "Epoch [600/3000], Test Loss: 0.7136306762695312\n",
            "Epoch [700/3000], Test Loss: 0.6429731845855713\n",
            "Epoch [800/3000], Test Loss: 0.5845013856887817\n",
            "Epoch [900/3000], Test Loss: 0.5342788100242615\n",
            "Epoch [1000/3000], Test Loss: 0.4953165054321289\n",
            "Epoch [1100/3000], Test Loss: 0.4669903814792633\n",
            "Epoch [1200/3000], Test Loss: 0.45510879158973694\n",
            "Epoch [1300/3000], Test Loss: 0.45048511028289795\n",
            "Epoch [1400/3000], Test Loss: 0.44549012184143066\n",
            "Epoch [1500/3000], Test Loss: 0.4370318055152893\n",
            "Epoch [1600/3000], Test Loss: 0.4350496828556061\n",
            "Epoch [100/3000], Test Loss: 0.8045233488082886\n",
            "Epoch [200/3000], Test Loss: 0.6656548976898193\n",
            "Epoch [300/3000], Test Loss: 0.6401614546775818\n",
            "Epoch [400/3000], Test Loss: 0.5964481234550476\n",
            "Epoch [500/3000], Test Loss: 0.5613476037979126\n",
            "Epoch [600/3000], Test Loss: 0.5203315615653992\n",
            "Epoch [700/3000], Test Loss: 0.49132874608039856\n",
            "Epoch [800/3000], Test Loss: 0.46781712770462036\n",
            "Epoch [900/3000], Test Loss: 0.44250229001045227\n",
            "Epoch [1000/3000], Test Loss: 0.41942423582077026\n",
            "Epoch [1100/3000], Test Loss: 0.40086761116981506\n",
            "Epoch [1200/3000], Test Loss: 0.3867904245853424\n",
            "Epoch [1300/3000], Test Loss: 0.3742176592350006\n",
            "Epoch [1400/3000], Test Loss: 0.3620333969593048\n",
            "Epoch [1500/3000], Test Loss: 0.35600754618644714\n",
            "Epoch [1600/3000], Test Loss: 0.3523792028427124\n",
            "Epoch [1700/3000], Test Loss: 0.34903737902641296\n",
            "Average loss on the test set: 0.3961945280432701\n"
          ]
        }
      ],
      "source": [
        "training_epochs = 3000\n",
        "learning_rate = 1e-5\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 10\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "best_model = None\n",
        "patience = 40\n",
        "\n",
        "# prompt: Train and test the RegressionNet on Xs and ys\n",
        "\n",
        "k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in k_fold.split(Xs):\n",
        "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "  y_train, y_test = ys[train_index], ys[test_index]\n",
        "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "  patience_counter = 0\n",
        "  best_accuracy = None\n",
        "  model = RegressionNet(Xs.shape[1])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  for epoch in range(training_epochs):\n",
        "    epoch_msg = True\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "      model.train()\n",
        "      # Forward pass\n",
        "      outputs = model(X_train_batch)\n",
        "      loss = criterion(outputs, y_train_batch)\n",
        "\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # if epoch_msg and (epoch + 1) % 100 == 0:\n",
        "      #   epoch_msg = False\n",
        "      #   print(f'Epoch [{epoch + 1}/{training_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "    # Testing the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      outputs = model(X_test)\n",
        "      loss = criterion(outputs, y_test)\n",
        "      if best_accuracy is None or loss.item() < best_accuracy:\n",
        "        best_accuracy = loss.item()\n",
        "        best_model = model\n",
        "        patience_counter = 0\n",
        "      else:\n",
        "        patience_counter += 1\n",
        "      if epoch_msg and (epoch + 1) % 100 == 0:\n",
        "        epoch_msg = False\n",
        "        print(f'Epoch [{epoch + 1}/{training_epochs}], Test Loss: {loss.item()}')\n",
        "        # print(f'Loss on the test set: {loss.item()}')\n",
        "    if patience_counter >= patience:\n",
        "      break\n",
        "  accuracies.append(best_accuracy)\n",
        "print(f'Average loss on the test set: {sum(accuracies)/len(accuracies)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC3uDRqv8T0V"
      },
      "source": [
        "For the vanilla neural network\n",
        "\n",
        "```\n",
        "Average loss on the test set: 1.3484851598739624\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOGHMEAqaoaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e52ffe9-435a-40c0-db93-2e69928914f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23242825269699097,\n",
              " 0.3771035969257355,\n",
              " 0.5493008494377136,\n",
              " 0.4118078947067261,\n",
              " 0.2786405086517334,\n",
              " 0.15259669721126556,\n",
              " 0.8715450763702393,\n",
              " 0.30623748898506165,\n",
              " 0.4340432286262512,\n",
              " 0.3482416868209839]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOcyoBruG_1-",
        "outputId": "11cedc62-969c-45e6-99f6-88067c80b06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3961945280432701"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9_2oHTKXHIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3ee809-ae95-4924-b030-022f2b2f7a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3637\n"
          ]
        }
      ],
      "source": [
        "# prompt: print the number of parameters in model\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lcYtG21v45O"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPlAPD92JC_j"
      },
      "source": [
        "##Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "newly added here."
      ],
      "metadata": {
        "id": "SqrFf1LvSw7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: center and scale to normalize my Xs and ys\n",
        "\n",
        "# def standardize_tensor(input_tensor):\n",
        "#     mean = input_tensor.mean()\n",
        "#     std = input_tensor.std()\n",
        "#     standardized_tensor = (input_tensor - mean) / std\n",
        "#     return standardized_tensor\n",
        "\n",
        "# Xs = standardize_tensor(Xs)"
      ],
      "metadata": {
        "id": "vlnbKhrYSpNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28HWRvIWsz65"
      },
      "outputs": [],
      "source": [
        "training_epochs = 1000\n",
        "learning_rate = 0.01 #0.0001\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100 #4 #100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "hidden_layers = None #Conv_cifar10() #None\n",
        "\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModels(X_train,y_train):\n",
        "  # https://contrib.scikit-learn.org/metric-learn/supervised.html#lmnn\n",
        "  # lmnn = LMNN(n_neighbors=5, learn_rate=1e-6)\n",
        "  # ##TODO, change here if you need to use a different one\n",
        "  # # lmnn = metric_learn.MLKR()\n",
        "  # # lmnn = metric_learn.NCA(max_iter=1000)\n",
        "  # lmnn.fit(X_train,y_train)\n",
        "  # knn = KNeighborsClassifier(n_neighbors=5,metric=lmnn.get_metric())\n",
        "  # knn.fit(X_train,y_train)\n",
        "  # # klmnn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n",
        "  # klmnn_acc = accuracy_score(knn.predict(X_test), y_test)\n",
        "  klmnn_acc = 0\n",
        "  # print(\"lmnn experiment done\")\n",
        "  # continue\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Train model\n",
        "  model = NN_k_NN(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing,hidden_layers)\n",
        "  # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #, weight_decay=1e-5)\n",
        "  # print_model_features(model)\n",
        "  #Training loop\n",
        "  # temp = []\n",
        "  patience_counter = 0\n",
        "  for epoch in range(training_epochs):\n",
        "    epoch_msg = True\n",
        "    # model.eval()\n",
        "    # break\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "      model.train()\n",
        "      feature_activations, case_activations, output, predicted_class = model(X_train_batch)\n",
        "      loss = criterion(output, y_train_batch)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if epoch_msg and (epoch + 1) % 2 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{training_epochs}], Loss: {loss.item()}')\n",
        "        #inspecting the case activations\n",
        "        # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "        # print(top_case_indices)\n",
        "        epoch_msg = False\n",
        "      # print(\"evaluating\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      feature_activations, case_activations, output, predicted_class = model(X_test)\n",
        "      # inspecting the case activations\n",
        "      # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "      # print(top_case_indices)\n",
        "\n",
        "      # Calculate accuracy\n",
        "      accuracy_temp = accuracy_score(y_test, predicted_class)\n",
        "    if epoch == 0:\n",
        "      best_accuracy = accuracy_temp\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "    elif accuracy_temp > best_accuracy:\n",
        "      #memorize best model\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "      best_accuracy = accuracy_temp\n",
        "      patience_counter = 0\n",
        "    elif patience_counter > patience:\n",
        "      model = NN_k_NN(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing)\n",
        "      model.load_state_dict(torch.load(PATH))\n",
        "      model.eval()\n",
        "      print(\"patience exceeded, loading best model\")\n",
        "      break\n",
        "    else:\n",
        "      patience_counter += 1\n",
        "\n",
        "  # best_accuracies.append(best_accuracy)\n",
        "\n",
        "  # print_model_features(model)\n",
        "  # Test model\n",
        "  with torch.no_grad():\n",
        "    feature_activations, case_activations, output, predicted_class = model(X_test)\n",
        "\n",
        "    #inspecting the case activations\n",
        "    top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "    # print(top_case_indices)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, predicted_class)\n",
        "    # Add accuracy to list\n",
        "    # accuracies.append(accuracy)\n",
        "\n",
        "  ##compare with a normal k-nn\n",
        "  knn =  KNeighborsClassifier(n_neighbors=top_k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  knn_acc  = accuracy_score(knn.predict(X_test), y_test)\n",
        "  # knn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n",
        "  return klmnn_acc, best_accuracy, knn_acc, model"
      ],
      "metadata": {
        "id": "DFX8bBPAn8HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Z8bG1lwYT9"
      },
      "outputs": [],
      "source": [
        "# prompt: train and test my model on the Xs and ys in a 10 fold cross validation\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from metric_learn import LMNN,NCA\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Split data into 10 folds\n",
        "k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n",
        "# Scale data, not enabled right now.\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(Xs)\n",
        "# Xs = scaler.transform(Xs)\n",
        "\n",
        "# Train and test model on each fold\n",
        "best_model = None\n",
        "model = None\n",
        "PATH = 'best_classifier_model.h5'\n",
        "best_accuracy = None\n",
        "best_accuracies = []\n",
        "accuracies = []\n",
        "knn_accuracies = []\n",
        "\n",
        "nca_accuracies = []\n",
        "klmnn_accuracies = []\n",
        "\n",
        "count = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "# y_train, y_test = ys[train_index], ys[test_index]\n",
        "\n",
        "# klmnn_acc, best_accuracy, knn_acc, model = trainModels(X_train,y_train)\n",
        "# accuracies.append(best_accuracy)\n",
        "# knn_accuracies.append(knn_acc)\n",
        "# klmnn_accuracies.append(klmnn_acc)\n"
      ],
      "metadata": {
        "id": "LO8_C0UOzkD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in k_fold.split(Xs):\n",
        "  # Get training and testing data\n",
        "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "  y_train, y_test = ys[train_index], ys[test_index]\n",
        "  klmnn_acc, best_accuracy, knn_acc, model = trainModels(X_train,y_train)\n",
        "  accuracies.append(best_accuracy)\n",
        "  knn_accuracies.append(knn_acc)\n",
        "  klmnn_accuracies.append(klmnn_acc)\n",
        "\n",
        "  #added for case maintenance experiment. comment it out if needed\n",
        "  # break;\n",
        "\n",
        "  # count += 1\n",
        "  # if count == 2:\n",
        "  #   break\n",
        "\n"
      ],
      "metadata": {
        "id": "hGHd8zsqzc8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0252e76a-1f7c-477b-e573-a7ccff6adc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6835060119628906\n",
            "Epoch [4/1000], Loss: 0.6549917459487915\n",
            "Epoch [6/1000], Loss: 0.6732542514801025\n",
            "Epoch [8/1000], Loss: 0.6484996676445007\n",
            "Epoch [10/1000], Loss: 0.6589815020561218\n",
            "Epoch [12/1000], Loss: 0.6907955408096313\n",
            "Epoch [14/1000], Loss: 0.650188148021698\n",
            "Epoch [16/1000], Loss: 0.5293143391609192\n",
            "Epoch [18/1000], Loss: 0.37050861120224\n",
            "Epoch [20/1000], Loss: 0.3491923213005066\n",
            "Epoch [22/1000], Loss: 0.21838915348052979\n",
            "Epoch [24/1000], Loss: 0.17247681319713593\n",
            "Epoch [26/1000], Loss: 0.13314658403396606\n",
            "Epoch [28/1000], Loss: 0.18764671683311462\n",
            "Epoch [30/1000], Loss: 0.133039191365242\n",
            "Epoch [32/1000], Loss: 0.15448327362537384\n",
            "Epoch [34/1000], Loss: 0.06965664029121399\n",
            "Epoch [36/1000], Loss: 0.06796222925186157\n",
            "Epoch [38/1000], Loss: 0.09050679206848145\n",
            "Epoch [40/1000], Loss: 0.05250363051891327\n",
            "Epoch [42/1000], Loss: 0.05987957492470741\n",
            "Epoch [44/1000], Loss: 0.044495973736047745\n",
            "Epoch [46/1000], Loss: 0.023151008412241936\n",
            "Epoch [48/1000], Loss: 0.014931376092135906\n",
            "Epoch [50/1000], Loss: 0.05250678211450577\n",
            "Epoch [52/1000], Loss: 0.0049751149490475655\n",
            "Epoch [54/1000], Loss: 0.04273310303688049\n",
            "Epoch [56/1000], Loss: 0.01083778403699398\n",
            "Epoch [58/1000], Loss: 0.011862053535878658\n",
            "Epoch [60/1000], Loss: 0.019890449941158295\n",
            "Epoch [62/1000], Loss: 0.007228564005345106\n",
            "Epoch [64/1000], Loss: 0.01350947842001915\n",
            "Epoch [66/1000], Loss: 0.007417204789817333\n",
            "Epoch [68/1000], Loss: 0.011707199737429619\n",
            "Epoch [70/1000], Loss: 0.0064719743095338345\n",
            "Epoch [72/1000], Loss: 0.005071080289781094\n",
            "Epoch [74/1000], Loss: 0.006503005512058735\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6823551058769226\n",
            "Epoch [4/1000], Loss: 0.6653344631195068\n",
            "Epoch [6/1000], Loss: 0.6280864477157593\n",
            "Epoch [8/1000], Loss: 0.6200582385063171\n",
            "Epoch [10/1000], Loss: 0.6480870246887207\n",
            "Epoch [12/1000], Loss: 0.646181046962738\n",
            "Epoch [14/1000], Loss: 0.6511254906654358\n",
            "Epoch [16/1000], Loss: 0.6367346048355103\n",
            "Epoch [18/1000], Loss: 0.37790027260780334\n",
            "Epoch [20/1000], Loss: 0.27872082591056824\n",
            "Epoch [22/1000], Loss: 0.2373763620853424\n",
            "Epoch [24/1000], Loss: 0.1648276299238205\n",
            "Epoch [26/1000], Loss: 0.1334993690252304\n",
            "Epoch [28/1000], Loss: 0.14679904282093048\n",
            "Epoch [30/1000], Loss: 0.07523426413536072\n",
            "Epoch [32/1000], Loss: 0.04174254089593887\n",
            "Epoch [34/1000], Loss: 0.04606169834733009\n",
            "Epoch [36/1000], Loss: 0.04693238437175751\n",
            "Epoch [38/1000], Loss: 0.020653914660215378\n",
            "Epoch [40/1000], Loss: 0.010902222245931625\n",
            "Epoch [42/1000], Loss: 0.026755204424262047\n",
            "Epoch [44/1000], Loss: 0.03795259818434715\n",
            "Epoch [46/1000], Loss: 0.024773143231868744\n",
            "Epoch [48/1000], Loss: 0.011261975392699242\n",
            "Epoch [50/1000], Loss: 0.015476319007575512\n",
            "Epoch [52/1000], Loss: 0.0029977739322930574\n",
            "Epoch [54/1000], Loss: 0.0025501041673123837\n",
            "Epoch [56/1000], Loss: 0.013157609850168228\n",
            "Epoch [58/1000], Loss: 0.004810483194887638\n",
            "Epoch [60/1000], Loss: 0.0015091827372089028\n",
            "Epoch [62/1000], Loss: 0.0038764746859669685\n",
            "Epoch [64/1000], Loss: 0.0020643179304897785\n",
            "Epoch [66/1000], Loss: 0.0015231702709570527\n",
            "Epoch [68/1000], Loss: 0.011630740948021412\n",
            "Epoch [70/1000], Loss: 0.003362405812367797\n",
            "Epoch [72/1000], Loss: 0.003428906900808215\n",
            "Epoch [74/1000], Loss: 0.011299384757876396\n",
            "Epoch [76/1000], Loss: 0.0085062887519598\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6812422275543213\n",
            "Epoch [4/1000], Loss: 0.6865759491920471\n",
            "Epoch [6/1000], Loss: 0.6306061744689941\n",
            "Epoch [8/1000], Loss: 0.653441309928894\n",
            "Epoch [10/1000], Loss: 0.6458491683006287\n",
            "Epoch [12/1000], Loss: 0.6474654674530029\n",
            "Epoch [14/1000], Loss: 0.7227113246917725\n",
            "Epoch [16/1000], Loss: 0.4751306176185608\n",
            "Epoch [18/1000], Loss: 0.36765193939208984\n",
            "Epoch [20/1000], Loss: 0.25943970680236816\n",
            "Epoch [22/1000], Loss: 0.2004939615726471\n",
            "Epoch [24/1000], Loss: 0.11444666236639023\n",
            "Epoch [26/1000], Loss: 0.14690375328063965\n",
            "Epoch [28/1000], Loss: 0.08299562335014343\n",
            "Epoch [30/1000], Loss: 0.11797456443309784\n",
            "Epoch [32/1000], Loss: 0.10264935344457626\n",
            "Epoch [34/1000], Loss: 0.04165785759687424\n",
            "Epoch [36/1000], Loss: 0.017939135432243347\n",
            "Epoch [38/1000], Loss: 0.066476970911026\n",
            "Epoch [40/1000], Loss: 0.020632896572351456\n",
            "Epoch [42/1000], Loss: 0.05384362116456032\n",
            "Epoch [44/1000], Loss: 0.024739813059568405\n",
            "Epoch [46/1000], Loss: 0.01525474525988102\n",
            "Epoch [48/1000], Loss: 0.01071962807327509\n",
            "Epoch [50/1000], Loss: 0.011441915296018124\n",
            "Epoch [52/1000], Loss: 0.012775502167642117\n",
            "Epoch [54/1000], Loss: 0.012406863272190094\n",
            "Epoch [56/1000], Loss: 0.021395385265350342\n",
            "Epoch [58/1000], Loss: 0.010969262570142746\n",
            "Epoch [60/1000], Loss: 0.010262615978717804\n",
            "Epoch [62/1000], Loss: 0.007946028374135494\n",
            "Epoch [64/1000], Loss: 0.003325260942801833\n",
            "Epoch [66/1000], Loss: 0.0017992156790569425\n",
            "Epoch [68/1000], Loss: 0.0017530014738440514\n",
            "Epoch [70/1000], Loss: 0.01040883082896471\n",
            "Epoch [72/1000], Loss: 0.0027140367310494184\n",
            "Epoch [74/1000], Loss: 0.0018454577075317502\n",
            "Epoch [76/1000], Loss: 0.008029058575630188\n",
            "Epoch [78/1000], Loss: 0.0023185894824564457\n",
            "Epoch [80/1000], Loss: 0.004049778450280428\n",
            "Epoch [82/1000], Loss: 0.001741640968248248\n",
            "Epoch [84/1000], Loss: 0.0015478837303817272\n",
            "Epoch [86/1000], Loss: 0.001404437469318509\n",
            "Epoch [88/1000], Loss: 0.004270232282578945\n",
            "Epoch [90/1000], Loss: 0.001220621750690043\n",
            "Epoch [92/1000], Loss: 0.002830317709594965\n",
            "Epoch [94/1000], Loss: 0.003550425171852112\n",
            "Epoch [96/1000], Loss: 0.0022659962996840477\n",
            "Epoch [98/1000], Loss: 0.0020319409668445587\n",
            "Epoch [100/1000], Loss: 0.0055667078122496605\n",
            "Epoch [102/1000], Loss: 0.001919194357469678\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6869160532951355\n",
            "Epoch [4/1000], Loss: 0.6572407484054565\n",
            "Epoch [6/1000], Loss: 0.6510093212127686\n",
            "Epoch [8/1000], Loss: 0.6794734001159668\n",
            "Epoch [10/1000], Loss: 0.6692457795143127\n",
            "Epoch [12/1000], Loss: 0.6781860589981079\n",
            "Epoch [14/1000], Loss: 0.6521331071853638\n",
            "Epoch [16/1000], Loss: 0.6711773872375488\n",
            "Epoch [18/1000], Loss: 0.6116443872451782\n",
            "Epoch [20/1000], Loss: 0.4544999599456787\n",
            "Epoch [22/1000], Loss: 0.42286479473114014\n",
            "Epoch [24/1000], Loss: 0.24550798535346985\n",
            "Epoch [26/1000], Loss: 0.16821998357772827\n",
            "Epoch [28/1000], Loss: 0.15929538011550903\n",
            "Epoch [30/1000], Loss: 0.12790459394454956\n",
            "Epoch [32/1000], Loss: 0.06320878863334656\n",
            "Epoch [34/1000], Loss: 0.16421109437942505\n",
            "Epoch [36/1000], Loss: 0.06859768182039261\n",
            "Epoch [38/1000], Loss: 0.1743219494819641\n",
            "Epoch [40/1000], Loss: 0.029230501502752304\n",
            "Epoch [42/1000], Loss: 0.023699158802628517\n",
            "Epoch [44/1000], Loss: 0.01787535473704338\n",
            "Epoch [46/1000], Loss: 0.02045833133161068\n",
            "Epoch [48/1000], Loss: 0.03835540637373924\n",
            "Epoch [50/1000], Loss: 0.011519077233970165\n",
            "Epoch [52/1000], Loss: 0.04222767427563667\n",
            "Epoch [54/1000], Loss: 0.026678360998630524\n",
            "Epoch [56/1000], Loss: 0.022310424596071243\n",
            "Epoch [58/1000], Loss: 0.005096171051263809\n",
            "Epoch [60/1000], Loss: 0.0029648784548044205\n",
            "Epoch [62/1000], Loss: 0.004117087926715612\n",
            "Epoch [64/1000], Loss: 0.01275873463600874\n",
            "Epoch [66/1000], Loss: 0.006533297710120678\n",
            "Epoch [68/1000], Loss: 0.005737724713981152\n",
            "Epoch [70/1000], Loss: 0.017576251178979874\n",
            "Epoch [72/1000], Loss: 0.00894155539572239\n",
            "Epoch [74/1000], Loss: 0.002670746063813567\n",
            "Epoch [76/1000], Loss: 0.0034596635960042477\n",
            "Epoch [78/1000], Loss: 0.0061262850649654865\n",
            "Epoch [80/1000], Loss: 0.004045483656227589\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6809950470924377\n",
            "Epoch [4/1000], Loss: 0.6802921295166016\n",
            "Epoch [6/1000], Loss: 0.6593512892723083\n",
            "Epoch [8/1000], Loss: 0.6219298243522644\n",
            "Epoch [10/1000], Loss: 0.6780057549476624\n",
            "Epoch [12/1000], Loss: 0.6691957116127014\n",
            "Epoch [14/1000], Loss: 0.6754741072654724\n",
            "Epoch [16/1000], Loss: 0.6497807502746582\n",
            "Epoch [18/1000], Loss: 0.3824489116668701\n",
            "Epoch [20/1000], Loss: 0.32933634519577026\n",
            "Epoch [22/1000], Loss: 0.295448362827301\n",
            "Epoch [24/1000], Loss: 0.22610262036323547\n",
            "Epoch [26/1000], Loss: 0.19828584790229797\n",
            "Epoch [28/1000], Loss: 0.15891426801681519\n",
            "Epoch [30/1000], Loss: 0.12123927474021912\n",
            "Epoch [32/1000], Loss: 0.08007324486970901\n",
            "Epoch [34/1000], Loss: 0.05117334425449371\n",
            "Epoch [36/1000], Loss: 0.05058373510837555\n",
            "Epoch [38/1000], Loss: 0.0415654182434082\n",
            "Epoch [40/1000], Loss: 0.06213102862238884\n",
            "Epoch [42/1000], Loss: 0.06305047869682312\n",
            "Epoch [44/1000], Loss: 0.022645525634288788\n",
            "Epoch [46/1000], Loss: 0.027032768353819847\n",
            "Epoch [48/1000], Loss: 0.05097315460443497\n",
            "Epoch [50/1000], Loss: 0.027951152995228767\n",
            "Epoch [52/1000], Loss: 0.0155370207503438\n",
            "Epoch [54/1000], Loss: 0.012082695960998535\n",
            "Epoch [56/1000], Loss: 0.01566918194293976\n",
            "Epoch [58/1000], Loss: 0.014408178627490997\n",
            "Epoch [60/1000], Loss: 0.010905729606747627\n",
            "Epoch [62/1000], Loss: 0.01592755876481533\n",
            "Epoch [64/1000], Loss: 0.005487642716616392\n",
            "Epoch [66/1000], Loss: 0.009900708682835102\n",
            "Epoch [68/1000], Loss: 0.02910146676003933\n",
            "Epoch [70/1000], Loss: 0.010472416877746582\n",
            "Epoch [72/1000], Loss: 0.0036426258739084005\n",
            "Epoch [74/1000], Loss: 0.007742824498564005\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6789494156837463\n",
            "Epoch [4/1000], Loss: 0.6454164981842041\n",
            "Epoch [6/1000], Loss: 0.6265646815299988\n",
            "Epoch [8/1000], Loss: 0.6802966594696045\n",
            "Epoch [10/1000], Loss: 0.6412153840065002\n",
            "Epoch [12/1000], Loss: 0.6357349157333374\n",
            "Epoch [14/1000], Loss: 0.6293192505836487\n",
            "Epoch [16/1000], Loss: 0.3519379794597626\n",
            "Epoch [18/1000], Loss: 0.4004928469657898\n",
            "Epoch [20/1000], Loss: 0.32498157024383545\n",
            "Epoch [22/1000], Loss: 0.3233890235424042\n",
            "Epoch [24/1000], Loss: 0.24204681813716888\n",
            "Epoch [26/1000], Loss: 0.22095060348510742\n",
            "Epoch [28/1000], Loss: 0.16311725974082947\n",
            "Epoch [30/1000], Loss: 0.1664971113204956\n",
            "Epoch [32/1000], Loss: 0.16098564863204956\n",
            "Epoch [34/1000], Loss: 0.0955512747168541\n",
            "Epoch [36/1000], Loss: 0.08001347631216049\n",
            "Epoch [38/1000], Loss: 0.04118312895298004\n",
            "Epoch [40/1000], Loss: 0.06643449515104294\n",
            "Epoch [42/1000], Loss: 0.047870878130197525\n",
            "Epoch [44/1000], Loss: 0.027647536247968674\n",
            "Epoch [46/1000], Loss: 0.04574557766318321\n",
            "Epoch [48/1000], Loss: 0.010668311268091202\n",
            "Epoch [50/1000], Loss: 0.01708204112946987\n",
            "Epoch [52/1000], Loss: 0.028875578194856644\n",
            "Epoch [54/1000], Loss: 0.010642788372933865\n",
            "Epoch [56/1000], Loss: 0.03406894579529762\n",
            "Epoch [58/1000], Loss: 0.01879221946001053\n",
            "Epoch [60/1000], Loss: 0.007199262268841267\n",
            "Epoch [62/1000], Loss: 0.002627995330840349\n",
            "Epoch [64/1000], Loss: 0.008093362674117088\n",
            "Epoch [66/1000], Loss: 0.025151168927550316\n",
            "Epoch [68/1000], Loss: 0.014020071364939213\n",
            "Epoch [70/1000], Loss: 0.015033005736768246\n",
            "Epoch [72/1000], Loss: 0.0110093392431736\n",
            "Epoch [74/1000], Loss: 0.0042825681157410145\n",
            "Epoch [76/1000], Loss: 0.01192089170217514\n",
            "Epoch [78/1000], Loss: 0.005070814397186041\n",
            "Epoch [80/1000], Loss: 0.011473247781395912\n",
            "Epoch [82/1000], Loss: 0.009877224452793598\n",
            "Epoch [84/1000], Loss: 0.004121363628655672\n",
            "Epoch [86/1000], Loss: 0.01124823372811079\n",
            "Epoch [88/1000], Loss: 0.009246855974197388\n",
            "Epoch [90/1000], Loss: 0.010383878834545612\n",
            "Epoch [92/1000], Loss: 0.017134997993707657\n",
            "Epoch [94/1000], Loss: 0.01727662980556488\n",
            "Epoch [96/1000], Loss: 0.003388799959793687\n",
            "Epoch [98/1000], Loss: 0.0012805811129510403\n",
            "Epoch [100/1000], Loss: 0.01617000624537468\n",
            "Epoch [102/1000], Loss: 0.0028832689858973026\n",
            "Epoch [104/1000], Loss: 0.002117843134328723\n",
            "Epoch [106/1000], Loss: 0.0029495633207261562\n",
            "Epoch [108/1000], Loss: 0.008551275357604027\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6880049705505371\n",
            "Epoch [4/1000], Loss: 0.6436541080474854\n",
            "Epoch [6/1000], Loss: 0.6506041884422302\n",
            "Epoch [8/1000], Loss: 0.6535589098930359\n",
            "Epoch [10/1000], Loss: 0.6756433248519897\n",
            "Epoch [12/1000], Loss: 0.6824808716773987\n",
            "Epoch [14/1000], Loss: 0.6543309092521667\n",
            "Epoch [16/1000], Loss: 0.6062053442001343\n",
            "Epoch [18/1000], Loss: 0.3578030467033386\n",
            "Epoch [20/1000], Loss: 0.31598755717277527\n",
            "Epoch [22/1000], Loss: 0.36116766929626465\n",
            "Epoch [24/1000], Loss: 0.2923862338066101\n",
            "Epoch [26/1000], Loss: 0.20845694839954376\n",
            "Epoch [28/1000], Loss: 0.1279832124710083\n",
            "Epoch [30/1000], Loss: 0.15622621774673462\n",
            "Epoch [32/1000], Loss: 0.05790843069553375\n",
            "Epoch [34/1000], Loss: 0.16076061129570007\n",
            "Epoch [36/1000], Loss: 0.05692298337817192\n",
            "Epoch [38/1000], Loss: 0.03576868772506714\n",
            "Epoch [40/1000], Loss: 0.05931314453482628\n",
            "Epoch [42/1000], Loss: 0.02006768062710762\n",
            "Epoch [44/1000], Loss: 0.030002783983945847\n",
            "Epoch [46/1000], Loss: 0.032205723226070404\n",
            "Epoch [48/1000], Loss: 0.040669649839401245\n",
            "Epoch [50/1000], Loss: 0.01958010159432888\n",
            "Epoch [52/1000], Loss: 0.03842899948358536\n",
            "Epoch [54/1000], Loss: 0.00822298415005207\n",
            "Epoch [56/1000], Loss: 0.005918564740568399\n",
            "Epoch [58/1000], Loss: 0.0018667050171643496\n",
            "Epoch [60/1000], Loss: 0.013778755441308022\n",
            "Epoch [62/1000], Loss: 0.007728376891463995\n",
            "Epoch [64/1000], Loss: 0.022034043446183205\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6801590919494629\n",
            "Epoch [4/1000], Loss: 0.6523246169090271\n",
            "Epoch [6/1000], Loss: 0.6546604037284851\n",
            "Epoch [8/1000], Loss: 0.685783326625824\n",
            "Epoch [10/1000], Loss: 0.6805101037025452\n",
            "Epoch [12/1000], Loss: 0.661620557308197\n",
            "Epoch [14/1000], Loss: 0.6631743907928467\n",
            "Epoch [16/1000], Loss: 0.434128999710083\n",
            "Epoch [18/1000], Loss: 0.3514782190322876\n",
            "Epoch [20/1000], Loss: 0.46192681789398193\n",
            "Epoch [22/1000], Loss: 0.2222512811422348\n",
            "Epoch [24/1000], Loss: 0.2098928838968277\n",
            "Epoch [26/1000], Loss: 0.14293743669986725\n",
            "Epoch [28/1000], Loss: 0.13100583851337433\n",
            "Epoch [30/1000], Loss: 0.09890246391296387\n",
            "Epoch [32/1000], Loss: 0.06047092005610466\n",
            "Epoch [34/1000], Loss: 0.06281587481498718\n",
            "Epoch [36/1000], Loss: 0.0811435803771019\n",
            "Epoch [38/1000], Loss: 0.06707096844911575\n",
            "Epoch [40/1000], Loss: 0.029862163588404655\n",
            "Epoch [42/1000], Loss: 0.03602620214223862\n",
            "Epoch [44/1000], Loss: 0.012839975766837597\n",
            "Epoch [46/1000], Loss: 0.03093581274151802\n",
            "Epoch [48/1000], Loss: 0.02854657918214798\n",
            "Epoch [50/1000], Loss: 0.02464882656931877\n",
            "Epoch [52/1000], Loss: 0.012080412358045578\n",
            "Epoch [54/1000], Loss: 0.012111143209040165\n",
            "Epoch [56/1000], Loss: 0.011196417734026909\n",
            "Epoch [58/1000], Loss: 0.023886971175670624\n",
            "Epoch [60/1000], Loss: 0.011360282078385353\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "Epoch [2/1000], Loss: 0.6868895888328552\n",
            "Epoch [4/1000], Loss: 0.674397349357605\n",
            "Epoch [6/1000], Loss: 0.6813315749168396\n",
            "Epoch [8/1000], Loss: 0.6301496028900146\n",
            "Epoch [10/1000], Loss: 0.7060859203338623\n",
            "Epoch [12/1000], Loss: 0.7110854983329773\n",
            "Epoch [14/1000], Loss: 0.6704690456390381\n",
            "Epoch [16/1000], Loss: 0.4341047406196594\n",
            "Epoch [18/1000], Loss: 0.2752976417541504\n",
            "Epoch [20/1000], Loss: 0.3831712007522583\n",
            "Epoch [22/1000], Loss: 0.18475158512592316\n",
            "Epoch [24/1000], Loss: 0.23842798173427582\n",
            "Epoch [26/1000], Loss: 0.13785327970981598\n",
            "Epoch [28/1000], Loss: 0.13549241423606873\n",
            "Epoch [30/1000], Loss: 0.15023699402809143\n",
            "Epoch [32/1000], Loss: 0.15790723264217377\n",
            "Epoch [34/1000], Loss: 0.11423222720623016\n",
            "Epoch [36/1000], Loss: 0.11639585345983505\n",
            "Epoch [38/1000], Loss: 0.08917760848999023\n",
            "Epoch [40/1000], Loss: 0.06837914139032364\n",
            "Epoch [42/1000], Loss: 0.06124033406376839\n",
            "Epoch [44/1000], Loss: 0.04588651284575462\n",
            "Epoch [46/1000], Loss: 0.08081293851137161\n",
            "Epoch [48/1000], Loss: 0.018287600949406624\n",
            "Epoch [50/1000], Loss: 0.03557179495692253\n",
            "Epoch [52/1000], Loss: 0.036926522850990295\n",
            "Epoch [54/1000], Loss: 0.03769148886203766\n",
            "Epoch [56/1000], Loss: 0.03798216953873634\n",
            "Epoch [58/1000], Loss: 0.007296428084373474\n",
            "Epoch [60/1000], Loss: 0.013778767548501492\n",
            "Epoch [62/1000], Loss: 0.013280868530273438\n",
            "Epoch [64/1000], Loss: 0.019122298806905746\n",
            "Epoch [66/1000], Loss: 0.014089711010456085\n",
            "Epoch [68/1000], Loss: 0.004392439965158701\n",
            "Epoch [70/1000], Loss: 0.008206571452319622\n",
            "Epoch [72/1000], Loss: 0.004042485263198614\n",
            "Epoch [74/1000], Loss: 0.014835646376013756\n",
            "Epoch [76/1000], Loss: 0.00929518323391676\n",
            "Epoch [78/1000], Loss: 0.0074185561388731\n",
            "Epoch [80/1000], Loss: 0.010838501155376434\n",
            "Epoch [82/1000], Loss: 0.014253396540880203\n",
            "Epoch [84/1000], Loss: 0.00926840677857399\n",
            "Epoch [86/1000], Loss: 0.009507044218480587\n",
            "Epoch [88/1000], Loss: 0.010009841993451118\n",
            "Epoch [90/1000], Loss: 0.0039874836802482605\n",
            "self.ca_weight.shape torch.Size([512, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([513, 2])\n",
            "Epoch [2/1000], Loss: 0.6800981760025024\n",
            "Epoch [4/1000], Loss: 0.665924072265625\n",
            "Epoch [6/1000], Loss: 0.680854082107544\n",
            "Epoch [8/1000], Loss: 0.6407053470611572\n",
            "Epoch [10/1000], Loss: 0.6478846073150635\n",
            "Epoch [12/1000], Loss: 0.6442580223083496\n",
            "Epoch [14/1000], Loss: 0.6809344291687012\n",
            "Epoch [16/1000], Loss: 0.4223383069038391\n",
            "Epoch [18/1000], Loss: 0.2444111853837967\n",
            "Epoch [20/1000], Loss: 0.1766781061887741\n",
            "Epoch [22/1000], Loss: 0.301447331905365\n",
            "Epoch [24/1000], Loss: 0.1127239242196083\n",
            "Epoch [26/1000], Loss: 0.21562112867832184\n",
            "Epoch [28/1000], Loss: 0.10341433435678482\n",
            "Epoch [30/1000], Loss: 0.09136608242988586\n",
            "Epoch [32/1000], Loss: 0.04892832413315773\n",
            "Epoch [34/1000], Loss: 0.0883079320192337\n",
            "Epoch [36/1000], Loss: 0.07978129386901855\n",
            "Epoch [38/1000], Loss: 0.06658592075109482\n",
            "Epoch [40/1000], Loss: 0.03579598292708397\n",
            "Epoch [42/1000], Loss: 0.0335620678961277\n",
            "Epoch [44/1000], Loss: 0.02168058231472969\n",
            "Epoch [46/1000], Loss: 0.02278999797999859\n",
            "Epoch [48/1000], Loss: 0.023279523476958275\n",
            "Epoch [50/1000], Loss: 0.013193151913583279\n",
            "Epoch [52/1000], Loss: 0.00675005791708827\n",
            "Epoch [54/1000], Loss: 0.008959447965025902\n",
            "Epoch [56/1000], Loss: 0.017154302448034286\n",
            "Epoch [58/1000], Loss: 0.004925982095301151\n",
            "Epoch [60/1000], Loss: 0.012732353061437607\n",
            "Epoch [62/1000], Loss: 0.012760731391608715\n",
            "self.ca_weight.shape torch.Size([513, 2])\n",
            "patience exceeded, loading best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print average accuracy\n",
        "print(\"Average accuracy:\", np.mean(accuracies))\n",
        "print(np.mean(klmnn_accuracies))\n",
        "print(np.mean(knn_accuracies))"
      ],
      "metadata": {
        "id": "hp0VlYenhs7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29bedb9-42b7-4f79-bef9-734df9b61a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy: 0.9683270676691729\n",
            "0.0\n",
            "0.4128446115288221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMgwt-v5c49R"
      },
      "source": [
        "## Classification with train test loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJBxiGuThMsp"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AA83z3Oc8gb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2NnlWpVWqn6"
      },
      "source": [
        "##Standardize Regression Data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6818C09SWst6"
      },
      "outputs": [],
      "source": [
        "# prompt: center and scale to normalize my Xs and ys\n",
        "\n",
        "def standardize_tensor(input_tensor):\n",
        "    mean = input_tensor.mean()\n",
        "    std = input_tensor.std()\n",
        "    standardized_tensor = (input_tensor - mean) / std\n",
        "    return standardized_tensor\n",
        "\n",
        "Xs = standardize_tensor(Xs)\n",
        "ys = standardize_tensor(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_WVmie3JH9c"
      },
      "source": [
        "##Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPpxOXA2JJJe"
      },
      "outputs": [],
      "source": [
        "training_epochs = 1500\n",
        "learning_rate = 0.01 #0.0001 #0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "hidden_layers = None\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r8m3pd6NMrl"
      },
      "source": [
        "This code below will attempt to use TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LWnNYJn2APf"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "# !pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
        "# # imports pytorch\n",
        "# import torch\n",
        "\n",
        "# # imports the torch_xla package\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "\n",
        "# dev = xm.xla_device()\n",
        "# Xs = Xs.to(dev)\n",
        "# ys = ys.to(dev)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5PloUp6JL3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6452a7f2-209b-47a9-fea5-8790808ef4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/1500], Loss: 0.5380368828773499\n",
            "Epoch [4/1500], Loss: 0.6111276149749756\n",
            "Epoch [6/1500], Loss: 0.46001067757606506\n",
            "Epoch [8/1500], Loss: 0.41264796257019043\n",
            "Epoch [10/1500], Loss: 0.45030689239501953\n",
            "Epoch [12/1500], Loss: 0.5843074321746826\n",
            "Epoch [14/1500], Loss: 0.4940412938594818\n",
            "Epoch [16/1500], Loss: 0.2602710425853729\n",
            "Epoch [18/1500], Loss: 0.4859312176704407\n",
            "Epoch [20/1500], Loss: 0.4066120982170105\n",
            "Epoch [22/1500], Loss: 0.6090803742408752\n",
            "Epoch [24/1500], Loss: 0.40855926275253296\n",
            "Epoch [26/1500], Loss: 0.2712518572807312\n",
            "Epoch [28/1500], Loss: 0.3194822371006012\n",
            "Epoch [30/1500], Loss: 0.38951370120048523\n",
            "Epoch [32/1500], Loss: 0.393422931432724\n",
            "Epoch [34/1500], Loss: 0.423280268907547\n",
            "Epoch [36/1500], Loss: 0.3997667729854584\n",
            "Epoch [38/1500], Loss: 0.48717761039733887\n",
            "Epoch [40/1500], Loss: 0.4000498056411743\n",
            "Epoch [42/1500], Loss: 0.4061751961708069\n",
            "Epoch [44/1500], Loss: 0.3863859176635742\n",
            "Epoch [46/1500], Loss: 0.36606475710868835\n",
            "Epoch [48/1500], Loss: 0.3012378513813019\n",
            "Epoch [50/1500], Loss: 0.4389488995075226\n",
            "Epoch [52/1500], Loss: 0.36305907368659973\n",
            "Epoch [54/1500], Loss: 0.4125908613204956\n",
            "Epoch [56/1500], Loss: 0.3640715479850769\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.7018711566925049\n",
            "Epoch [4/1500], Loss: 0.6181589365005493\n",
            "Epoch [6/1500], Loss: 0.5821380019187927\n",
            "Epoch [8/1500], Loss: 0.38861969113349915\n",
            "Epoch [10/1500], Loss: 0.4768017828464508\n",
            "Epoch [12/1500], Loss: 0.45705345273017883\n",
            "Epoch [14/1500], Loss: 0.36609840393066406\n",
            "Epoch [16/1500], Loss: 0.48000824451446533\n",
            "Epoch [18/1500], Loss: 0.26289376616477966\n",
            "Epoch [20/1500], Loss: 0.49992820620536804\n",
            "Epoch [22/1500], Loss: 0.508272111415863\n",
            "Epoch [24/1500], Loss: 0.4231107234954834\n",
            "Epoch [26/1500], Loss: 0.43410903215408325\n",
            "Epoch [28/1500], Loss: 0.5603561997413635\n",
            "Epoch [30/1500], Loss: 0.6393336057662964\n",
            "Epoch [32/1500], Loss: 0.6629387140274048\n",
            "Epoch [34/1500], Loss: 0.620602548122406\n",
            "Epoch [36/1500], Loss: 0.5371745228767395\n",
            "Epoch [38/1500], Loss: 0.32212138175964355\n",
            "Epoch [40/1500], Loss: 0.4581158757209778\n",
            "Epoch [42/1500], Loss: 0.2807615101337433\n",
            "Epoch [44/1500], Loss: 0.4170908033847809\n",
            "Epoch [46/1500], Loss: 0.4100022614002228\n",
            "Epoch [48/1500], Loss: 0.40301093459129333\n",
            "Epoch [50/1500], Loss: 0.3516119718551636\n",
            "Epoch [52/1500], Loss: 0.3263484835624695\n",
            "Epoch [54/1500], Loss: 0.31605228781700134\n",
            "Epoch [56/1500], Loss: 0.30302682518959045\n",
            "Epoch [58/1500], Loss: 0.46406352519989014\n",
            "Epoch [60/1500], Loss: 0.3366944193840027\n",
            "Epoch [62/1500], Loss: 0.40994372963905334\n",
            "Epoch [64/1500], Loss: 0.33136433362960815\n",
            "Epoch [66/1500], Loss: 0.3379020392894745\n",
            "Epoch [68/1500], Loss: 0.4443715810775757\n",
            "Epoch [70/1500], Loss: 0.4725830554962158\n",
            "Epoch [72/1500], Loss: 0.2701190710067749\n",
            "Epoch [74/1500], Loss: 0.3727666139602661\n",
            "Epoch [76/1500], Loss: 0.4879944324493408\n",
            "Epoch [78/1500], Loss: 0.41730251908302307\n",
            "Epoch [80/1500], Loss: 0.3707214593887329\n",
            "Epoch [82/1500], Loss: 0.4227184057235718\n",
            "Epoch [84/1500], Loss: 0.45366746187210083\n",
            "Epoch [86/1500], Loss: 0.32761695981025696\n",
            "Epoch [88/1500], Loss: 0.45691773295402527\n",
            "Epoch [90/1500], Loss: 0.33739373087882996\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.6873051524162292\n",
            "Epoch [4/1500], Loss: 0.6634993553161621\n",
            "Epoch [6/1500], Loss: 0.5102749466896057\n",
            "Epoch [8/1500], Loss: 0.49817216396331787\n",
            "Epoch [10/1500], Loss: 0.5473398566246033\n",
            "Epoch [12/1500], Loss: 0.2983763515949249\n",
            "Epoch [14/1500], Loss: 0.3997093439102173\n",
            "Epoch [16/1500], Loss: 0.32536080479621887\n",
            "Epoch [18/1500], Loss: 0.520746111869812\n",
            "Epoch [20/1500], Loss: 0.414199560880661\n",
            "Epoch [22/1500], Loss: 0.3153516948223114\n",
            "Epoch [24/1500], Loss: 0.45555737614631653\n",
            "Epoch [26/1500], Loss: 0.43628811836242676\n",
            "Epoch [28/1500], Loss: 0.37398087978363037\n",
            "Epoch [30/1500], Loss: 0.3891248404979706\n",
            "Epoch [32/1500], Loss: 0.4817500412464142\n",
            "Epoch [34/1500], Loss: 0.4164465069770813\n",
            "Epoch [36/1500], Loss: 0.23884254693984985\n",
            "Epoch [38/1500], Loss: 0.4187862277030945\n",
            "Epoch [40/1500], Loss: 0.29901522397994995\n",
            "Epoch [42/1500], Loss: 0.38570523262023926\n",
            "Epoch [44/1500], Loss: 0.7403723001480103\n",
            "Epoch [46/1500], Loss: 0.39061206579208374\n",
            "Epoch [48/1500], Loss: 0.36667782068252563\n",
            "Epoch [50/1500], Loss: 0.36437520384788513\n",
            "Epoch [52/1500], Loss: 0.5683388710021973\n",
            "Epoch [54/1500], Loss: 0.2621767222881317\n",
            "Epoch [56/1500], Loss: 0.4483153820037842\n",
            "Epoch [58/1500], Loss: 0.22804084420204163\n",
            "Epoch [60/1500], Loss: 0.30001431703567505\n",
            "Epoch [62/1500], Loss: 0.566839873790741\n",
            "Epoch [64/1500], Loss: 0.32392844557762146\n",
            "Epoch [66/1500], Loss: 0.2310427874326706\n",
            "Epoch [68/1500], Loss: 0.3526977598667145\n",
            "Epoch [70/1500], Loss: 0.35561156272888184\n",
            "Epoch [72/1500], Loss: 0.3968815505504608\n",
            "Epoch [74/1500], Loss: 0.5905104875564575\n",
            "Epoch [76/1500], Loss: 0.3934544324874878\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.6217323541641235\n",
            "Epoch [4/1500], Loss: 0.7671574950218201\n",
            "Epoch [6/1500], Loss: 0.2733999192714691\n",
            "Epoch [8/1500], Loss: 0.4656637907028198\n",
            "Epoch [10/1500], Loss: 0.40019211173057556\n",
            "Epoch [12/1500], Loss: 0.3852357566356659\n",
            "Epoch [14/1500], Loss: 0.6141858100891113\n",
            "Epoch [16/1500], Loss: 0.28957024216651917\n",
            "Epoch [18/1500], Loss: 0.5803235769271851\n",
            "Epoch [20/1500], Loss: 0.2987404465675354\n",
            "Epoch [22/1500], Loss: 0.3808356821537018\n",
            "Epoch [24/1500], Loss: 0.4598974585533142\n",
            "Epoch [26/1500], Loss: 0.4489101469516754\n",
            "Epoch [28/1500], Loss: 0.4102562665939331\n",
            "Epoch [30/1500], Loss: 0.3385292887687683\n",
            "Epoch [32/1500], Loss: 0.37837547063827515\n",
            "Epoch [34/1500], Loss: 0.4771769642829895\n",
            "Epoch [36/1500], Loss: 0.43092238903045654\n",
            "Epoch [38/1500], Loss: 0.2785981297492981\n",
            "Epoch [40/1500], Loss: 0.3970893919467926\n",
            "Epoch [42/1500], Loss: 0.3880411386489868\n",
            "Epoch [44/1500], Loss: 0.334473192691803\n",
            "Epoch [46/1500], Loss: 0.2663355767726898\n",
            "Epoch [48/1500], Loss: 0.3430115580558777\n",
            "Epoch [50/1500], Loss: 0.5976723432540894\n",
            "Epoch [52/1500], Loss: 0.5059666037559509\n",
            "Epoch [54/1500], Loss: 0.41487666964530945\n",
            "Epoch [56/1500], Loss: 0.41975876688957214\n",
            "Epoch [58/1500], Loss: 0.32373929023742676\n",
            "Epoch [60/1500], Loss: 0.37324610352516174\n",
            "Epoch [62/1500], Loss: 0.24295303225517273\n",
            "Epoch [64/1500], Loss: 0.4508835971355438\n",
            "Epoch [66/1500], Loss: 0.45247364044189453\n",
            "Epoch [68/1500], Loss: 0.3404788076877594\n",
            "Epoch [70/1500], Loss: 0.3116147816181183\n",
            "Epoch [72/1500], Loss: 0.25878798961639404\n",
            "Epoch [74/1500], Loss: 0.3485972285270691\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.8456861972808838\n",
            "Epoch [4/1500], Loss: 0.5031528472900391\n",
            "Epoch [6/1500], Loss: 0.5998574495315552\n",
            "Epoch [8/1500], Loss: 0.2886323034763336\n",
            "Epoch [10/1500], Loss: 0.3852097988128662\n",
            "Epoch [12/1500], Loss: 0.39469966292381287\n",
            "Epoch [14/1500], Loss: 0.3490975499153137\n",
            "Epoch [16/1500], Loss: 0.4357064366340637\n",
            "Epoch [18/1500], Loss: 0.6421581506729126\n",
            "Epoch [20/1500], Loss: 0.5254753828048706\n",
            "Epoch [22/1500], Loss: 0.2726876735687256\n",
            "Epoch [24/1500], Loss: 0.3745061755180359\n",
            "Epoch [26/1500], Loss: 0.39224737882614136\n",
            "Epoch [28/1500], Loss: 0.5710369944572449\n",
            "Epoch [30/1500], Loss: 0.36955687403678894\n",
            "Epoch [32/1500], Loss: 0.238025963306427\n",
            "Epoch [34/1500], Loss: 0.3498167395591736\n",
            "Epoch [36/1500], Loss: 0.3274477422237396\n",
            "Epoch [38/1500], Loss: 0.3816349506378174\n",
            "Epoch [40/1500], Loss: 0.35338154435157776\n",
            "Epoch [42/1500], Loss: 0.38703736662864685\n",
            "Epoch [44/1500], Loss: 0.3682880699634552\n",
            "Epoch [46/1500], Loss: 0.43696802854537964\n",
            "Epoch [48/1500], Loss: 0.39432787895202637\n",
            "Epoch [50/1500], Loss: 0.40066471695899963\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.4837423264980316\n",
            "Epoch [4/1500], Loss: 0.503804624080658\n",
            "Epoch [6/1500], Loss: 0.625866174697876\n",
            "Epoch [8/1500], Loss: 0.3483692407608032\n",
            "Epoch [10/1500], Loss: 0.2739996016025543\n",
            "Epoch [12/1500], Loss: 0.5364883542060852\n",
            "Epoch [14/1500], Loss: 0.37278881669044495\n",
            "Epoch [16/1500], Loss: 0.40707075595855713\n",
            "Epoch [18/1500], Loss: 0.39456066489219666\n",
            "Epoch [20/1500], Loss: 0.4321250021457672\n",
            "Epoch [22/1500], Loss: 0.42120254039764404\n",
            "Epoch [24/1500], Loss: 0.47330915927886963\n",
            "Epoch [26/1500], Loss: 0.4761306047439575\n",
            "Epoch [28/1500], Loss: 0.4023410677909851\n",
            "Epoch [30/1500], Loss: 0.5022789239883423\n",
            "Epoch [32/1500], Loss: 0.861762285232544\n",
            "Epoch [34/1500], Loss: 0.4485802948474884\n",
            "Epoch [36/1500], Loss: 0.35056933760643005\n",
            "Epoch [38/1500], Loss: 0.5094842314720154\n",
            "Epoch [40/1500], Loss: 0.4034298062324524\n",
            "Epoch [42/1500], Loss: 0.4108445644378662\n",
            "Epoch [44/1500], Loss: 0.4010361135005951\n",
            "Epoch [46/1500], Loss: 0.4320339262485504\n",
            "Epoch [48/1500], Loss: 0.4500293731689453\n",
            "Epoch [50/1500], Loss: 0.4412985146045685\n",
            "Epoch [52/1500], Loss: 0.42688074707984924\n",
            "Epoch [54/1500], Loss: 0.39920684695243835\n",
            "Epoch [56/1500], Loss: 0.3718772530555725\n",
            "Epoch [58/1500], Loss: 0.4017102122306824\n",
            "Epoch [60/1500], Loss: 0.5096790790557861\n",
            "Epoch [62/1500], Loss: 0.34887245297431946\n",
            "Epoch [64/1500], Loss: 0.4730467200279236\n",
            "Epoch [66/1500], Loss: 0.34908145666122437\n",
            "Epoch [68/1500], Loss: 0.44336384534835815\n",
            "Epoch [70/1500], Loss: 0.38668957352638245\n",
            "Epoch [72/1500], Loss: 0.5547276139259338\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.6080946922302246\n",
            "Epoch [4/1500], Loss: 0.7490417957305908\n",
            "Epoch [6/1500], Loss: 0.5259779095649719\n",
            "Epoch [8/1500], Loss: 0.32008418440818787\n",
            "Epoch [10/1500], Loss: 0.4348268210887909\n",
            "Epoch [12/1500], Loss: 0.467776894569397\n",
            "Epoch [14/1500], Loss: 0.35835835337638855\n",
            "Epoch [16/1500], Loss: 0.4778694808483124\n",
            "Epoch [18/1500], Loss: 0.439046174287796\n",
            "Epoch [20/1500], Loss: 0.4440544843673706\n",
            "Epoch [22/1500], Loss: 0.5165026187896729\n",
            "Epoch [24/1500], Loss: 0.336214154958725\n",
            "Epoch [26/1500], Loss: 0.6358001828193665\n",
            "Epoch [28/1500], Loss: 0.5176128149032593\n",
            "Epoch [30/1500], Loss: 0.5566521286964417\n",
            "Epoch [32/1500], Loss: 0.3708057105541229\n",
            "Epoch [34/1500], Loss: 0.5251696705818176\n",
            "Epoch [36/1500], Loss: 0.32161200046539307\n",
            "Epoch [38/1500], Loss: 0.35610252618789673\n",
            "Epoch [40/1500], Loss: 0.43906906247138977\n",
            "Epoch [42/1500], Loss: 0.2679547071456909\n",
            "Epoch [44/1500], Loss: 0.27328065037727356\n",
            "Epoch [46/1500], Loss: 0.48027634620666504\n",
            "Epoch [48/1500], Loss: 0.39703574776649475\n",
            "Epoch [50/1500], Loss: 0.3288300633430481\n",
            "Epoch [52/1500], Loss: 0.2506290674209595\n",
            "Epoch [54/1500], Loss: 0.567859947681427\n",
            "Epoch [56/1500], Loss: 0.4671834707260132\n",
            "Epoch [58/1500], Loss: 0.4513002038002014\n",
            "Epoch [60/1500], Loss: 0.3848079442977905\n",
            "Epoch [62/1500], Loss: 0.23157773911952972\n",
            "Epoch [64/1500], Loss: 0.5000068545341492\n",
            "Epoch [66/1500], Loss: 0.372731477022171\n",
            "Epoch [68/1500], Loss: 0.39459362626075745\n",
            "Epoch [70/1500], Loss: 0.2929563820362091\n",
            "Epoch [72/1500], Loss: 0.33933037519454956\n",
            "Epoch [74/1500], Loss: 0.26195916533470154\n",
            "Epoch [76/1500], Loss: 0.3986867666244507\n",
            "Epoch [78/1500], Loss: 0.4185522794723511\n",
            "Epoch [80/1500], Loss: 0.3666488230228424\n",
            "Epoch [82/1500], Loss: 0.4126461148262024\n",
            "Epoch [84/1500], Loss: 0.3176532983779907\n",
            "Epoch [86/1500], Loss: 0.3937164545059204\n",
            "Epoch [88/1500], Loss: 0.23962315917015076\n",
            "Epoch [90/1500], Loss: 0.32312819361686707\n",
            "Epoch [92/1500], Loss: 0.36162105202674866\n",
            "Epoch [94/1500], Loss: 0.29736462235450745\n",
            "Epoch [96/1500], Loss: 0.26710471510887146\n",
            "Epoch [98/1500], Loss: 0.31797048449516296\n",
            "Epoch [100/1500], Loss: 0.5063528418540955\n",
            "Epoch [102/1500], Loss: 0.5240944623947144\n",
            "Epoch [104/1500], Loss: 0.414208322763443\n",
            "Epoch [106/1500], Loss: 0.2168216109275818\n",
            "Epoch [108/1500], Loss: 0.27677837014198303\n",
            "Epoch [110/1500], Loss: 0.3282291889190674\n",
            "Epoch [112/1500], Loss: 0.23194539546966553\n",
            "Epoch [114/1500], Loss: 0.3356662690639496\n",
            "Epoch [116/1500], Loss: 0.22265352308750153\n",
            "Epoch [118/1500], Loss: 0.28352850675582886\n",
            "Epoch [120/1500], Loss: 0.3874632716178894\n",
            "Epoch [122/1500], Loss: 0.37572768330574036\n",
            "Epoch [124/1500], Loss: 0.3597434163093567\n",
            "Epoch [126/1500], Loss: 0.36932599544525146\n",
            "Epoch [128/1500], Loss: 0.3416857123374939\n",
            "Epoch [130/1500], Loss: 0.29261794686317444\n",
            "Epoch [132/1500], Loss: 0.4244701862335205\n",
            "Epoch [134/1500], Loss: 0.3506906032562256\n",
            "Epoch [136/1500], Loss: 0.2848148047924042\n",
            "Epoch [138/1500], Loss: 0.3694651126861572\n",
            "Epoch [140/1500], Loss: 0.38094428181648254\n",
            "Epoch [142/1500], Loss: 0.29713156819343567\n",
            "Epoch [144/1500], Loss: 0.35560256242752075\n",
            "Epoch [146/1500], Loss: 0.2204858660697937\n",
            "Epoch [148/1500], Loss: 0.4399767220020294\n",
            "Epoch [150/1500], Loss: 0.3695317506790161\n",
            "Epoch [152/1500], Loss: 0.39296409487724304\n",
            "Epoch [154/1500], Loss: 0.2726203203201294\n",
            "Epoch [156/1500], Loss: 0.6026484966278076\n",
            "Epoch [158/1500], Loss: 0.5389425158500671\n",
            "Epoch [160/1500], Loss: 0.4211116135120392\n",
            "Epoch [162/1500], Loss: 0.3282417356967926\n",
            "Epoch [164/1500], Loss: 0.2233956754207611\n",
            "Epoch [166/1500], Loss: 0.3312373757362366\n",
            "Epoch [168/1500], Loss: 0.3319539725780487\n",
            "Epoch [170/1500], Loss: 0.6147711873054504\n",
            "Epoch [172/1500], Loss: 0.35800740122795105\n",
            "Epoch [174/1500], Loss: 0.29828354716300964\n",
            "Epoch [176/1500], Loss: 0.4155305027961731\n",
            "Epoch [178/1500], Loss: 0.28041398525238037\n",
            "Epoch [180/1500], Loss: 0.36058375239372253\n",
            "Epoch [182/1500], Loss: 0.3152806758880615\n",
            "Epoch [184/1500], Loss: 0.43006500601768494\n",
            "Epoch [186/1500], Loss: 0.2780895233154297\n",
            "Epoch [188/1500], Loss: 0.339821994304657\n",
            "Epoch [190/1500], Loss: 0.2820630967617035\n",
            "Epoch [192/1500], Loss: 0.25904542207717896\n",
            "Epoch [194/1500], Loss: 0.31460192799568176\n",
            "Epoch [196/1500], Loss: 0.5572875738143921\n",
            "Epoch [198/1500], Loss: 0.28046169877052307\n",
            "Epoch [200/1500], Loss: 0.30845314264297485\n",
            "Epoch [202/1500], Loss: 0.28292763233184814\n",
            "Epoch [204/1500], Loss: 0.23070506751537323\n",
            "Epoch [206/1500], Loss: 0.4390316903591156\n",
            "Epoch [208/1500], Loss: 0.4045945405960083\n",
            "Epoch [210/1500], Loss: 0.4728456735610962\n",
            "Epoch [212/1500], Loss: 0.447174996137619\n",
            "Epoch [214/1500], Loss: 0.2645476460456848\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.4695253372192383\n",
            "Epoch [4/1500], Loss: 0.6411500573158264\n",
            "Epoch [6/1500], Loss: 0.4427439868450165\n",
            "Epoch [8/1500], Loss: 0.56630539894104\n",
            "Epoch [10/1500], Loss: 0.4572514593601227\n",
            "Epoch [12/1500], Loss: 0.35992196202278137\n",
            "Epoch [14/1500], Loss: 0.33813294768333435\n",
            "Epoch [16/1500], Loss: 0.39554184675216675\n",
            "Epoch [18/1500], Loss: 0.5486133098602295\n",
            "Epoch [20/1500], Loss: 0.33050310611724854\n",
            "Epoch [22/1500], Loss: 0.41133788228034973\n",
            "Epoch [24/1500], Loss: 0.3423631191253662\n",
            "Epoch [26/1500], Loss: 0.43246951699256897\n",
            "Epoch [28/1500], Loss: 0.4893202483654022\n",
            "Epoch [30/1500], Loss: 0.5157284736633301\n",
            "Epoch [32/1500], Loss: 0.37252482771873474\n",
            "Epoch [34/1500], Loss: 0.43884509801864624\n",
            "Epoch [36/1500], Loss: 0.43409737944602966\n",
            "Epoch [38/1500], Loss: 0.5422880053520203\n",
            "Epoch [40/1500], Loss: 0.4415327310562134\n",
            "Epoch [42/1500], Loss: 0.48730286955833435\n",
            "Epoch [44/1500], Loss: 0.31535670161247253\n",
            "Epoch [46/1500], Loss: 0.30725303292274475\n",
            "Epoch [48/1500], Loss: 0.44902193546295166\n",
            "Epoch [50/1500], Loss: 0.3891737461090088\n",
            "Epoch [52/1500], Loss: 0.4702151417732239\n",
            "Epoch [54/1500], Loss: 0.2584986984729767\n",
            "Epoch [56/1500], Loss: 0.3730449974536896\n",
            "Epoch [58/1500], Loss: 0.3757908344268799\n",
            "Epoch [60/1500], Loss: 0.3264924883842468\n",
            "Epoch [62/1500], Loss: 0.45245957374572754\n",
            "Epoch [64/1500], Loss: 0.4422670304775238\n",
            "Epoch [66/1500], Loss: 0.3295988142490387\n",
            "Epoch [68/1500], Loss: 0.36071574687957764\n",
            "Epoch [70/1500], Loss: 0.39471501111984253\n",
            "Epoch [72/1500], Loss: 0.5005013942718506\n",
            "Epoch [74/1500], Loss: 0.35103103518486023\n",
            "Epoch [76/1500], Loss: 0.5491222739219666\n",
            "Epoch [78/1500], Loss: 0.44730743765830994\n",
            "Epoch [80/1500], Loss: 0.38324347138404846\n",
            "Epoch [82/1500], Loss: 0.5170480608940125\n",
            "Epoch [84/1500], Loss: 0.302341490983963\n",
            "Epoch [86/1500], Loss: 0.4715246856212616\n",
            "Epoch [88/1500], Loss: 0.26223546266555786\n",
            "Epoch [90/1500], Loss: 0.3715764582157135\n",
            "Epoch [92/1500], Loss: 0.30178549885749817\n",
            "Epoch [94/1500], Loss: 0.28583863377571106\n",
            "Epoch [96/1500], Loss: 0.3233654797077179\n",
            "Epoch [98/1500], Loss: 0.32428935170173645\n",
            "Epoch [100/1500], Loss: 0.34161457419395447\n",
            "Epoch [102/1500], Loss: 0.35594412684440613\n",
            "Epoch [104/1500], Loss: 0.45919087529182434\n",
            "Epoch [106/1500], Loss: 0.4121064841747284\n",
            "Epoch [108/1500], Loss: 0.3536776304244995\n",
            "Epoch [110/1500], Loss: 0.32004401087760925\n",
            "Epoch [112/1500], Loss: 0.3403865396976471\n",
            "Epoch [114/1500], Loss: 0.34885334968566895\n",
            "Epoch [116/1500], Loss: 0.31690797209739685\n",
            "Epoch [118/1500], Loss: 0.45106929540634155\n",
            "Epoch [120/1500], Loss: 0.40331095457077026\n",
            "Epoch [122/1500], Loss: 0.3033389151096344\n",
            "Epoch [124/1500], Loss: 0.5258776545524597\n",
            "Epoch [126/1500], Loss: 0.26679670810699463\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.5117284655570984\n",
            "Epoch [4/1500], Loss: 0.6567477583885193\n",
            "Epoch [6/1500], Loss: 0.3551965057849884\n",
            "Epoch [8/1500], Loss: 0.29039764404296875\n",
            "Epoch [10/1500], Loss: 0.45560944080352783\n",
            "Epoch [12/1500], Loss: 0.40293818712234497\n",
            "Epoch [14/1500], Loss: 0.4271939992904663\n",
            "Epoch [16/1500], Loss: 0.3868785500526428\n",
            "Epoch [18/1500], Loss: 0.5515918135643005\n",
            "Epoch [20/1500], Loss: 0.26924172043800354\n",
            "Epoch [22/1500], Loss: 0.5160329937934875\n",
            "Epoch [24/1500], Loss: 0.42549410462379456\n",
            "Epoch [26/1500], Loss: 0.4249167740345001\n",
            "Epoch [28/1500], Loss: 0.44343888759613037\n",
            "Epoch [30/1500], Loss: 0.2422911822795868\n",
            "Epoch [32/1500], Loss: 0.4904458522796631\n",
            "Epoch [34/1500], Loss: 0.3815133571624756\n",
            "Epoch [36/1500], Loss: 0.3136066198348999\n",
            "Epoch [38/1500], Loss: 0.3075559437274933\n",
            "Epoch [40/1500], Loss: 0.3900313079357147\n",
            "Epoch [42/1500], Loss: 0.578701376914978\n",
            "Epoch [44/1500], Loss: 0.3447025418281555\n",
            "Epoch [46/1500], Loss: 0.2816917896270752\n",
            "Epoch [48/1500], Loss: 0.4238389730453491\n",
            "Epoch [50/1500], Loss: 0.32074469327926636\n",
            "Epoch [52/1500], Loss: 0.5264472365379333\n",
            "Epoch [54/1500], Loss: 0.4389062523841858\n",
            "Epoch [56/1500], Loss: 0.3495626151561737\n",
            "Epoch [58/1500], Loss: 0.18633650243282318\n",
            "Epoch [60/1500], Loss: 0.4935285449028015\n",
            "Epoch [62/1500], Loss: 0.3378984332084656\n",
            "Epoch [64/1500], Loss: 0.319082647562027\n",
            "Epoch [66/1500], Loss: 0.5039014220237732\n",
            "Epoch [68/1500], Loss: 0.3051944971084595\n",
            "Epoch [70/1500], Loss: 0.3110760450363159\n",
            "Epoch [72/1500], Loss: 0.2495448738336563\n",
            "Epoch [74/1500], Loss: 0.392161101102829\n",
            "Epoch [76/1500], Loss: 0.3717881739139557\n",
            "Epoch [78/1500], Loss: 0.3835393786430359\n",
            "Epoch [80/1500], Loss: 0.42128846049308777\n",
            "Epoch [82/1500], Loss: 0.2426930069923401\n",
            "Epoch [84/1500], Loss: 0.4020458161830902\n",
            "Epoch [86/1500], Loss: 0.5564472675323486\n",
            "Epoch [88/1500], Loss: 0.3931026756763458\n",
            "Epoch [90/1500], Loss: 0.2758154571056366\n",
            "Epoch [92/1500], Loss: 0.3573056757450104\n",
            "Epoch [94/1500], Loss: 0.3457059860229492\n",
            "Epoch [96/1500], Loss: 0.4423174560070038\n",
            "Epoch [98/1500], Loss: 0.33007147908210754\n",
            "Epoch [100/1500], Loss: 0.36348384618759155\n",
            "Epoch [102/1500], Loss: 0.4303658604621887\n",
            "Epoch [104/1500], Loss: 0.3339208960533142\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.7878524661064148\n",
            "Epoch [4/1500], Loss: 0.3397367596626282\n",
            "Epoch [6/1500], Loss: 0.7120708227157593\n",
            "Epoch [8/1500], Loss: 0.35408806800842285\n",
            "Epoch [10/1500], Loss: 0.3538721799850464\n",
            "Epoch [12/1500], Loss: 0.3693093955516815\n",
            "Epoch [14/1500], Loss: 0.362102746963501\n",
            "Epoch [16/1500], Loss: 0.4242393970489502\n",
            "Epoch [18/1500], Loss: 0.3618650734424591\n",
            "Epoch [20/1500], Loss: 0.6057490110397339\n",
            "Epoch [22/1500], Loss: 0.2963857054710388\n",
            "Epoch [24/1500], Loss: 0.3533530533313751\n",
            "Epoch [26/1500], Loss: 0.42521464824676514\n",
            "Epoch [28/1500], Loss: 0.3544292747974396\n",
            "Epoch [30/1500], Loss: 0.3659311532974243\n",
            "Epoch [32/1500], Loss: 0.33128422498703003\n",
            "Epoch [34/1500], Loss: 0.45514845848083496\n",
            "Epoch [36/1500], Loss: 0.6078983545303345\n",
            "Epoch [38/1500], Loss: 0.358494371175766\n",
            "Epoch [40/1500], Loss: 0.17336179316043854\n",
            "Epoch [42/1500], Loss: 0.4272255599498749\n",
            "Epoch [44/1500], Loss: 0.3407439887523651\n",
            "Epoch [46/1500], Loss: 0.44036614894866943\n",
            "Epoch [48/1500], Loss: 0.4577990472316742\n",
            "Epoch [50/1500], Loss: 0.45378416776657104\n",
            "Epoch [52/1500], Loss: 0.26747581362724304\n",
            "Epoch [54/1500], Loss: 0.36497974395751953\n",
            "Epoch [56/1500], Loss: 0.36583060026168823\n",
            "Epoch [58/1500], Loss: 0.3076859712600708\n",
            "Epoch [60/1500], Loss: 0.3105652332305908\n",
            "Epoch [62/1500], Loss: 0.42669883370399475\n",
            "Epoch [64/1500], Loss: 0.5163730382919312\n",
            "Epoch [66/1500], Loss: 0.583454966545105\n",
            "Epoch [68/1500], Loss: 0.3854103088378906\n",
            "Epoch [70/1500], Loss: 0.6874673366546631\n",
            "Epoch [72/1500], Loss: 0.4905873239040375\n",
            "Epoch [74/1500], Loss: 0.4036717116832733\n",
            "Epoch [76/1500], Loss: 0.5650222301483154\n",
            "Epoch [78/1500], Loss: 0.36805903911590576\n",
            "Epoch [80/1500], Loss: 0.3480692207813263\n",
            "Epoch [82/1500], Loss: 0.49292197823524475\n",
            "Epoch [84/1500], Loss: 0.5119657516479492\n",
            "Epoch [86/1500], Loss: 0.6113995313644409\n",
            "Epoch [88/1500], Loss: 0.483402818441391\n",
            "Epoch [90/1500], Loss: 0.3443199098110199\n",
            "Epoch [92/1500], Loss: 0.3038593530654907\n",
            "Epoch [94/1500], Loss: 0.2518622577190399\n",
            "Epoch [96/1500], Loss: 0.2993658781051636\n",
            "Epoch [98/1500], Loss: 0.23797163367271423\n",
            "Epoch [100/1500], Loss: 0.42234116792678833\n",
            "Epoch [102/1500], Loss: 0.35856398940086365\n",
            "Epoch [104/1500], Loss: 0.3440913259983063\n",
            "Epoch [106/1500], Loss: 0.4223463833332062\n",
            "Epoch [108/1500], Loss: 0.44223037362098694\n",
            "Epoch [110/1500], Loss: 0.35895344614982605\n",
            "Epoch [112/1500], Loss: 0.222469761967659\n",
            "Epoch [114/1500], Loss: 0.33912914991378784\n",
            "Epoch [116/1500], Loss: 0.39697209000587463\n",
            "patience exceeded, loading best model\n",
            "Average accuracy: 0.42697263\n",
            "Average top_k_average_accuracies 0.8529457\n"
          ]
        }
      ],
      "source": [
        "# prompt: train and test my model on the Xs and ys in a 10 fold cross validation\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split data into 10 folds\n",
        "k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n",
        "# Scale data, not enabled right now.\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(Xs)\n",
        "# Xs = scaler.transform(Xs)\n",
        "\n",
        "# Train and test model on each fold\n",
        "best_model = None\n",
        "PATH = 'best_regressor_model.h5'\n",
        "best_accuracy = None\n",
        "best_accuracies = []\n",
        "accuracies = []\n",
        "top_k_average_accuracies = []\n",
        "knn_accuracies = []\n",
        "count = 0\n",
        "for train_index, test_index in k_fold.split(Xs):\n",
        "  # Get training and testing data\n",
        "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "  y_train, y_test = ys[train_index], ys[test_index]\n",
        "\n",
        "  # lmnn = metric_learn.MLKR()\n",
        "  # lmnn.fit(X_train,y_train)\n",
        "  # knn = KNeighborsRegressor(n_neighbors=5,metric=lmnn.get_metric())\n",
        "  # knn.fit(X_train,y_train)\n",
        "  # knn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n",
        "\n",
        "  # continue\n",
        "\n",
        "  #build a train loader with my X_train and X_test\n",
        "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Train model\n",
        "  model = NN_k_NN_regression(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing,\n",
        "                             hidden_layers)\n",
        "  # model = model.to(dev)\n",
        "  # print(\"number of parameters in model: \",len(list(model.parameters())))\n",
        "  # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #, weight_decay=1e-5)\n",
        "  # print_model_features(model)\n",
        "  #Training loop\n",
        "  patience_counter = 0\n",
        "  for epoch in range(training_epochs):\n",
        "    # break # no training\n",
        "    epoch_msg = True\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "      # print(\"training\")\n",
        "      ##DO NOT USE train() or eval() mode for regression\n",
        "      ##top k selection will mess up the final layer output\n",
        "      ##unlike in classification, each case's output is meaningful\n",
        "      ##here in regresion, the collective output is meaningful.\n",
        "      ## So if you topK case selection needs to be enabled or disabled the whole time.\n",
        "      model.train()\n",
        "      feature_activations, case_activations, output, predicted_number = model(X_train_batch)\n",
        "      # break\n",
        "      loss = criterion(predicted_number.squeeze(), y_train_batch)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if epoch_msg and (epoch + 1) % 2 == 0:\n",
        "        epoch_msg = False\n",
        "        print(f'Epoch [{epoch + 1}/{training_epochs}], Loss: {loss.item()}')\n",
        "        #inspecting the case activations\n",
        "        # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "        # print(top_case_indices)\n",
        "    # print(\"evaluating\")\n",
        "    # break\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      feature_activations, case_activations, output, predicted_number = model(X_test)\n",
        "\n",
        "    # inspecting the case activations\n",
        "    # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "    # print(top_case_indices)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    # accuracy_temp = mean_squared_error(y_test, predicted_number.squeeze().detach())\n",
        "    accuracy_temp = criterion(y_test, predicted_number.squeeze().detach())\n",
        "    if epoch == 0:\n",
        "      best_accuracy = accuracy_temp\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "    elif accuracy_temp < best_accuracy:\n",
        "      #memorize best model\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "\n",
        "      best_accuracy = accuracy_temp\n",
        "      patience_counter = 0\n",
        "    elif patience_counter > patience:\n",
        "      model = NN_k_NN_regression(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing)\n",
        "      model.load_state_dict(torch.load(PATH))\n",
        "      model.eval()\n",
        "      print(\"patience exceeded, loading best model\")\n",
        "      break\n",
        "    else:\n",
        "      patience_counter += 1\n",
        "    # temp.append(accuracy_temp)\n",
        "    # if len(temp) > patience and accuracy_temp > min(temp[-patience:]):\n",
        "    #   ##patience runs out\n",
        "    #   break\n",
        "  # best_accuracies.append(min(temp))\n",
        "  best_accuracies.append(best_accuracy)\n",
        "\n",
        "  # print(\"testing\")\n",
        "  # print_model_features(model)\n",
        "  # Test model\n",
        "  feature_activations, case_activations, output, predicted_number = model(X_test)\n",
        "\n",
        "  #inspecting the case activations\n",
        "  top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "  # print(top_case_indices)\n",
        "\n",
        "  # Using top case solution as final solution\n",
        "  # Calculate accuracy\n",
        "  # accuracy = mean_squared_error(y_test, predicted_number)\n",
        "  accuracy = criterion(y_test, predicted_number.squeeze())\n",
        "  # Add accuracy to list\n",
        "  accuracies.append(accuracy)\n",
        "\n",
        "  top_k_average_accuracies.append(mean_squared_error(torch.mean(y_train[top_case_indices], dim=1),\n",
        "                                                     y_test))\n",
        "\n",
        "  ##compare with a normal k-nn\n",
        "  knn =  KNeighborsRegressor(n_neighbors=top_k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  knn_accuracies.append( mean_squared_error(knn.predict(X_test), y_test))\n",
        "\n",
        "  # break\n",
        "  # count += 1\n",
        "  # if count == 5:\n",
        "  #   break\n",
        "\n",
        "# Print average accuracy\n",
        "# print(\"Average accuracy:\", np.mean(accuracies.detach().numpy()))\n",
        "print(\"Average accuracy:\", np.mean([acc.detach().numpy() for acc in accuracies]))\n",
        "\n",
        "# print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n",
        "print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n",
        "# torch.cat(torch.mean(top_k_average_accuracies)).item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(knn_accuracies))"
      ],
      "metadata": {
        "id": "smeb3Z1O2RRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edeb4afe-d4bf-485b-8e7a-b3a4d884edd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4803648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kiHFacefPBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47aee1d5-e19b-46b8-fa27-a32731626737"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "X_train[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shTwkGdc2bDV"
      },
      "outputs": [],
      "source": [
        "# top_k_average_accuracies.append(mean_squared_error(torch.mean(y_train[top_case_indices], dim=1),\n",
        "#                                                      y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guA1k-Fd490p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c362c9-4816-480b-e21e-cd65c73a377f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "predicted_number.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muGHh0IpcSF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfd024c-118a-4d22-bdc2-d4541a18f0eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9792, -0.4128, -0.5223,  0.6196,  0.0159,  0.1886,  0.1584,  0.7702,\n",
              "         0.5717,  0.8601,  0.5000, -0.0822,  0.0110,  1.3116,  1.2979,  0.5427,\n",
              "         0.1696,  0.8904, -0.7151, -0.6171,  0.6625, -1.3483,  0.1151, -1.4584,\n",
              "        -0.4694,  1.7975,  1.1913,  1.1509,  1.2303,  0.1610,  0.9336,  0.3741,\n",
              "         0.4592,  0.3835,  1.6024,  1.2535,  0.2925,  0.4133,  0.5119, -0.7664,\n",
              "         1.0422,  1.0946,  1.6598,  0.3048,  1.2885, -0.1864, -0.2541, -0.2693,\n",
              "        -0.3201, -0.6668,  0.8749,  0.5075,  0.4138, -0.3991, -0.3158, -0.3661,\n",
              "        -0.5243, -1.7325, -0.1869, -0.6002, -0.6069, -0.1109,  0.0723,  0.2147,\n",
              "         0.7042, -1.8926,  1.1591,  1.8327, -0.1121,  0.0377,  0.6198,  1.1728,\n",
              "         1.3388,  1.6657,  0.8525,  0.8914,  0.8506, -0.3312, -0.0271,  1.6513,\n",
              "        -0.5845,  0.2124,  0.5536, -0.3095, -0.0584,  1.2706,  0.2393,  0.5529,\n",
              "        -1.5109, -1.1205, -0.9736, -1.0065, -0.9713, -0.8817, -0.9582, -0.7773,\n",
              "        -0.6953, -0.1028,  0.1769, -0.1665,  0.0631, -0.0434,  0.3288,  0.2494,\n",
              "         0.8260,  0.0367,  0.4008,  0.5197,  0.1621, -1.7227, -1.1375, -0.7662,\n",
              "        -0.5308, -0.3378, -0.5790, -0.2624, -0.5187, -0.1017,  0.0658, -0.2220,\n",
              "        -0.2173,  0.2011, -0.2505, -0.0968, -0.1547, -0.2334, -0.0711, -0.0155,\n",
              "         0.1019,  0.5232,  0.5905, -0.9560, -0.8700, -0.9069, -0.9304, -0.8557,\n",
              "        -0.6254,  0.0951, -0.4402, -0.2120, -0.0920, -0.3131, -0.3667,  0.0853,\n",
              "         0.2201, -0.0322, -0.2022,  0.3808, -0.1332, -0.0123,  0.0047,  0.3335,\n",
              "         0.3474,  0.1565,  0.2196,  0.4720,  0.4772, -0.3356, -0.5413, -0.3787,\n",
              "         0.0915,  0.0060,  0.1000,  0.5251, -0.9157, -0.7029, -0.7519, -0.5615,\n",
              "        -0.4039, -0.3420, -0.3649,  0.0879,  0.6138,  0.0742, -0.1739,  0.2990,\n",
              "        -0.2711, -0.3383, -0.0826, -0.0704, -0.0175, -0.0663,  0.0498,  0.2253,\n",
              "         0.5302,  0.3228,  0.3672, -1.0577, -0.3676, -0.1262,  0.3032, -0.3826,\n",
              "        -0.0540,  0.3207,  0.3881, -0.2423,  0.2854,  0.2389, -0.1004, -0.1671,\n",
              "         0.1916,  0.2901,  0.0534,  0.3002,  0.5045,  0.3208, -0.2496, -0.8705,\n",
              "        -0.8260, -0.2144,  0.0000, -0.6730,  0.0431, -0.3867, -0.4199, -0.0027,\n",
              "         0.8155, -0.0574, -0.5622, -0.1659, -1.2700,  2.1416,  1.4018,  1.5072,\n",
              "        -1.0732,  1.2467, -0.2696,  0.6901,  0.8471,  0.9827, -0.0739,  1.2602,\n",
              "         1.0632,  0.1440,  1.3166,  0.6749,  1.6885,  0.0313, -0.6534,  0.9025,\n",
              "         1.2200,  0.0812,  1.8744,  1.4282,  0.4272, -0.0956, -0.6358,  0.4021,\n",
              "        -0.4117,  1.1554, -0.6853, -0.0930,  0.1284, -0.5847,  1.7322, -0.9339,\n",
              "         0.9577,  0.7230, -1.1728, -0.7575, -0.6353,  0.6463,  0.4825, -1.3311,\n",
              "        -0.7106, -0.3516, -0.1127,  0.1889,  0.7960,  1.1591,  0.6495, -1.0416,\n",
              "        -0.8066, -0.4283,  0.1228, -0.2343, -0.1445,  0.1304,  0.1102,  0.1633,\n",
              "        -1.7126, -0.8554, -0.5998, -0.3925, -0.4722,  0.0189, -0.4858,  0.0310,\n",
              "        -0.6845, -0.3380, -0.1156,  0.0965,  0.1343,  0.1066,  0.1713,  0.3134,\n",
              "         0.1526, -0.3403, -0.0929,  0.1662,  1.3149, -1.4851, -0.9221, -0.8693,\n",
              "        -0.8741, -0.7067,  0.2146,  0.9773, -0.0060,  0.1108,  0.8533, -0.3339,\n",
              "        -0.0226,  0.1203, -0.0681, -0.0629,  0.2581,  1.8652,  0.3369, -0.3996,\n",
              "         0.1136,  0.0563,  0.0664,  0.2759,  0.1501,  1.1568,  0.8765,  0.4100,\n",
              "         1.6227, -1.1050,  0.1726,  1.5967,  0.8530, -0.2988, -0.0275,  1.5092,\n",
              "        -0.5206, -0.1178,  0.2125,  2.1895,  0.6025,  0.5508, -0.0247, -0.2980,\n",
              "        -0.8621, -0.6731,  1.4350, -1.2534,  0.7963, -1.0065, -0.9560,  0.7736,\n",
              "         0.5064, -0.8956, -0.4251,  0.0062,  0.1344, -0.6720, -0.5465,  0.2231,\n",
              "        -0.8908, -0.5561, -0.7472,  0.4201,  0.0771, -1.7788,  0.2265, -1.1649,\n",
              "        -0.9399, -0.8090, -0.4395, -0.2433,  0.1693,  0.8520,  0.1882, -1.0681,\n",
              "        -0.6598,  0.1436,  0.2383, -0.9411, -0.3756,  0.0403, -0.0720, -0.2385,\n",
              "         0.2419,  0.4500,  0.8957,  0.2578, -0.1629, -0.4228,  1.0995, -0.3053,\n",
              "        -1.9188,  0.8567, -1.0453, -0.5677, -1.1977, -0.8735, -0.8579, -0.0592,\n",
              "         0.9280, -0.7975, -0.5382, -0.0060,  0.1486, -0.1668,  1.2526, -0.2766,\n",
              "        -0.2156,  0.4299,  0.2504,  0.0417,  0.2737,  0.1170, -1.2423, -0.7295,\n",
              "        -0.8574], grad_fn=<MvBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "predicted_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1mhapnXSVsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d60f05-af10-41ec-a1ee-03d38fa95c8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "model.cases.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nBkRvYdcWEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0357811d-c9d2-4b71-94c3-31ac57c16623"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "X_test.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m0Dco_SOHMb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW_Fy4y1WqNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a42c26d-52c2-42de-d3f0-c056bb907ae2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.4020),\n",
              " tensor(0.3928),\n",
              " tensor(0.4481),\n",
              " tensor(0.3528),\n",
              " tensor(0.4417),\n",
              " tensor(0.4436),\n",
              " tensor(0.4102),\n",
              " tensor(0.4863),\n",
              " tensor(0.4740),\n",
              " tensor(0.4183)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "best_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apW8H37ZmIUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34442ee-f843-4a7f-f527-85c504090abc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9802677,\n",
              " 0.8038256,\n",
              " 0.7498629,\n",
              " 0.8159676,\n",
              " 1.308656,\n",
              " 0.9834068,\n",
              " 0.630725,\n",
              " 0.7439191,\n",
              " 0.83556765,\n",
              " 0.67725897]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "top_k_average_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8DchB3qkqAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546c1092-c658-450c-bdd6-c6e71e4ea2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average top_k_average_accuracies 0.8529457\n"
          ]
        }
      ],
      "source": [
        "# print(\"Average top_k_average_accuracies\", torch.stack(torch.mean(top_k_average_accuracies)).item())\n",
        "print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooy_sIumpQEr"
      },
      "outputs": [],
      "source": [
        "tens_1 = torch.Tensor([[10, 20], [30, 40]])\n",
        "tens_2 = torch.Tensor([[2], [4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9nYCnL7pTZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8966903d-57f4-4f78-998a-9b15af9d4f25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 20.,  40.],\n",
              "        [120., 160.]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "tens_1 * tens_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lehnuufypYMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223f5a3b-0064-45cb-e412-f6f7eb7ffa35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 20.,  40.],\n",
              "        [120., 160.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "torch.mul(tens_1, tens_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWRu_gbNI5NA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuNUzhWmpa7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346c5035-acc1-4ee1-9d5d-2c0f3a9986f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[100.],\n",
              "        [220.]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "torch.matmul(tens_1, tens_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvDGfi_Rp1mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba1bdda-b245-4a8d-830f-6f5632aaf2db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 60., 280.])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "torch.sum(tens_1 * tens_2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS1hCvnB44zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ff2fdf-5773-4f65-9b57-0c20d7e07649"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.0959984 , -0.28958914, -0.6617781 ,  0.76494616, -0.16552618,\n",
              "        0.39275724,  0.39275724,  1.0751035 ,  0.45478868,  1.3232296 ,\n",
              "        0.39275718,  0.14463127, -0.16552617,  1.3852609 ,  0.8890091 ,\n",
              "        1.509324  , -0.16552615,  0.5168202 , -0.4756836 , -0.909904  ,\n",
              "        1.3852612 , -1.2820928 ,  0.39275718, -1.5302188 , -0.41365212,\n",
              "        1.7574499 ,  1.1991665 ,  0.45478874,  1.261198  ,  0.33072573,\n",
              "        1.5713555 ,  0.39275724,  0.33072573,  0.08259977,  1.137135  ,\n",
              "        1.5713555 ,  0.14463127, -0.04146319, -0.10349468, -0.97193545,\n",
              "        0.39275724,  1.0751036 ,  1.0130721 , -0.04146319,  1.3232296 ,\n",
              "       -0.0414632 , -0.5377151 , -0.16552618, -0.41365212, -0.5997466 ,\n",
              "        0.39275724,  0.64088315,  0.70291466,  0.20666274, -0.3516206 ,\n",
              "       -0.41365212, -0.22755766, -1.6542819 , -0.4756836 , -0.16552617,\n",
              "       -0.3516206 , -0.4756836 ,  0.26869422,  0.08259979,  0.08259977,\n",
              "       -1.9024079 ,  1.3232296 ,  1.8194813 , -0.10349467,  0.45478868,\n",
              "        0.8890091 ,  0.26869425,  0.7649461 ,  1.3232296 ,  0.7029147 ,\n",
              "        0.20666274,  0.7649461 ,  0.02056831, -0.0414632 ,  1.4472926 ,\n",
              "       -0.72380954,  0.5168202 ,  0.02056829, -0.47568363, -0.5377151 ,\n",
              "        1.261198  ,  0.33072573,  1.1991665 , -1.5302188 , -1.0339669 ,\n",
              "       -1.0339669 , -1.0959984 , -1.0339669 , -0.84787256, -0.97193545,\n",
              "       -0.909904  , -0.5997466 ,  0.0205683 ,  0.14463125, -0.0414632 ,\n",
              "       -0.0414632 ,  0.02056829,  0.26869422, -0.0414632 ,  0.5168202 ,\n",
              "        0.33072573,  0.39275724,  0.26869425, -0.10349468, -1.9644394 ,\n",
              "       -1.0959984 , -0.5377151 , -0.84787256, -0.4756836 , -0.6617781 ,\n",
              "       -0.41365212, -0.5997466 , -0.2895891 ,  0.14463124, -0.10349468,\n",
              "       -0.28958914,  0.08259977, -0.28958917, -0.22755766, -0.22755766,\n",
              "       -0.3516206 , -0.22755766, -0.04146319,  0.76494616,  0.5168202 ,\n",
              "        0.5788517 , -1.0959985 , -1.2200614 , -0.84787256, -0.909904  ,\n",
              "       -0.909904  , -0.6617781 ,  0.7649461 , -0.5997466 ,  0.02056829,\n",
              "       -0.3516206 , -0.3516206 , -0.28958914,  0.02056829,  0.64088315,\n",
              "        0.26869422,  0.0205683 ,  0.26869422, -0.22755766, -0.10349468,\n",
              "        0.14463124, -0.10349468, -0.04146319,  0.20666274,  0.08259977,\n",
              "        0.5788517 ,  0.5788517 , -0.4136521 , -0.6617781 , -0.22755766,\n",
              "        0.33072573, -0.16552618,  0.14463127,  0.39275718, -1.0959985 ,\n",
              "       -0.84787256, -0.7238096 , -0.5377151 , -0.3516206 , -0.4756836 ,\n",
              "       -0.22755766, -0.22755763,  0.14463124, -0.22755766, -0.16552615,\n",
              "       -0.22755766, -0.3516206 , -0.3516206 ,  0.26869422, -0.22755766,\n",
              "        0.20666274,  0.20666274,  0.14463127,  0.0205683 ,  0.51682013,\n",
              "        0.33072573,  0.39275718, -0.72380954, -0.5997466 , -0.16552617,\n",
              "        0.45478868, -0.41365212,  0.14463127,  0.14463124,  0.9510406 ,\n",
              "       -0.41365212,  0.5168202 ,  0.45478868, -0.28958914,  0.02056829,\n",
              "        0.26869425, -0.04146319,  0.14463127,  0.20666274,  0.26869425,\n",
              "        1.261198  , -0.10349469, -0.909904  , -0.909904  , -0.04146321,\n",
              "        0.33072573, -0.4756836 ,  0.76494616, -0.4756836 , -0.7238096 ,\n",
              "        0.08259978,  0.14463127,  0.33072573, -0.72380954, -0.10349467,\n",
              "       -1.3441244 ,  1.6954186 ,  1.4472924 ,  1.0751035 , -1.2200614 ,\n",
              "        0.51682025, -0.35162067,  1.261198  ,  0.8269776 ,  0.95104057,\n",
              "        0.14463127,  0.9510406 ,  0.8269776 ,  0.45478874,  1.509324  ,\n",
              "        0.14463127,  1.3232296 ,  0.08259977, -0.72380954,  0.08259977,\n",
              "        1.6954186 ,  0.08259977,  1.137135  ,  1.0751035 ,  0.8890091 ,\n",
              "        0.0825998 , -0.41365212,  0.64088315, -0.7238096 ,  1.3232296 ,\n",
              "       -0.84787244, -0.10349469,  0.20666274, -0.41365203,  0.5788517 ,\n",
              "       -1.2820929 ,  0.5788517 ,  0.39275724, -1.0339669 , -1.0959984 ,\n",
              "       -0.3516206 ,  0.5168202 ,  0.3927572 , -1.3441244 , -0.84787256,\n",
              "       -0.6617781 , -0.3516206 ,  0.33072573,  1.4472926 ,  1.1371351 ,\n",
              "        1.0751036 , -1.0959985 , -0.909904  , -0.41365212,  0.08259979,\n",
              "       -0.3516206 , -0.22755766,  0.14463128,  0.0205683 ,  0.20666274,\n",
              "       -1.8403763 , -0.97193545,  0.26869422, -0.10349467, -0.6617781 ,\n",
              "        0.33072573, -0.28958917, -0.10349468, -0.97193545, -0.3516206 ,\n",
              "       -0.16552615,  0.45478874,  0.14463124,  0.0205683 ,  0.14463128,\n",
              "        0.26869422,  0.08259976, -0.16552615, -0.10349467, -0.16552617,\n",
              "        1.633387  , -1.530219  , -1.0339669 , -0.97193545, -0.97193545,\n",
              "       -0.5997466 , -0.22755769,  1.013072  , -0.16552617,  0.45478868,\n",
              "        1.261198  , -0.41365212, -0.0414632 , -0.22755766, -0.10349468,\n",
              "        0.20666274,  0.76494616,  2.0055757 ,  0.08259977, -0.5377151 ,\n",
              "       -0.0414632 , -0.10349469, -0.10349468, -0.28958914, -0.22755766,\n",
              "        1.3232296 ,  0.57885164,  0.8269777 ,  1.2611979 , -1.0339669 ,\n",
              "        0.08259979,  1.6954186 ,  0.7029146 , -0.16552617, -0.35162067,\n",
              "        2.129639  , -0.6617781 , -0.28958917,  0.08259979,  1.7574499 ,\n",
              "        1.4472926 ,  1.3232296 ,  0.8890091 , -0.5377151 , -1.1580299 ,\n",
              "       -0.909904  ,  1.3232296 , -1.3441244 ,  0.8269776 , -1.2200614 ,\n",
              "       -1.0959985 ,  0.64088315,  0.39275724, -1.1580299 , -0.5377151 ,\n",
              "       -0.0414632 ,  0.14463127, -0.84787256, -0.53771514,  0.14463127,\n",
              "       -0.97193545, -0.72380954, -0.3516206 ,  0.14463127,  0.39275724,\n",
              "       -1.7783448 ,  0.26869422, -1.4681873 , -1.033967  , -0.4756836 ,\n",
              "       -0.6617781 , -0.5377151 ,  0.02056829,  0.8269776 ,  0.08259978,\n",
              "       -0.7858411 , -0.6617781 ,  0.39275724,  0.26869422, -0.909904  ,\n",
              "       -0.16552617, -0.22755766, -0.16552618, -0.16552618,  0.3927572 ,\n",
              "        0.7649462 ,  0.7649461 ,  0.39275724, -0.28958917,  0.02056829,\n",
              "        1.385261  , -0.22755766, -2.0885022 ,  0.20666274, -0.97193545,\n",
              "       -0.22755766, -1.3441244 , -0.84787256, -1.0339669 ,  0.20666274,\n",
              "        0.57885164, -1.0339669 , -0.6617781 , -0.16552618,  0.0205683 ,\n",
              "       -0.28958914,  0.8890091 , -0.41365212, -0.16552617,  0.39275724,\n",
              "        0.08259977, -0.22755763,  0.57885164,  0.08259977, -1.4681875 ,\n",
              "       -0.72380954, -0.84787256], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d60j7sj46Ar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94f23fc-c216-4cd3-8a61-b4b26a0b1ce5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9099, -0.9099,  0.3307,  1.5714,  0.3307, -0.9099,  0.9510, -0.2896,\n",
              "         0.0206, -0.2896,  0.6409, -0.2896,  0.0206,  0.0206,  0.9510, -0.9099,\n",
              "         0.3307,  0.9510, -0.2896,  0.0206,  1.5714, -1.5302,  0.3307, -1.8404,\n",
              "         0.3307,  3.1221,  0.0206,  0.6409,  0.9510,  1.2612,  2.5018,  3.1221,\n",
              "         1.8815,  1.2612,  3.1221,  2.8120,  1.8815,  2.1917,  1.5714,  0.0206,\n",
              "         0.9510,  2.1917,  3.4323,  1.5714,  2.1917, -0.2896,  0.0206,  0.3307,\n",
              "         0.9510, -0.2896,  0.9510,  0.3307,  0.0206,  0.3307,  0.9510,  0.9510,\n",
              "         0.6409, -1.8404,  2.5018,  0.0206, -0.9099, -0.2896,  0.0206,  2.8120,\n",
              "         0.3307, -2.1505,  0.9510,  1.5714,  0.6409,  1.5714,  0.6409,  2.1917,\n",
              "         0.9510,  1.5714,  1.2612,  2.1917,  0.6409,  0.0206, -0.9099,  1.5714,\n",
              "        -0.5997, -0.5997,  0.0206,  0.0206, -1.2201,  1.2612, -0.2896,  0.6409,\n",
              "        -1.2201, -1.5302, -1.2201, -0.9099, -0.9099, -1.2201, -1.2201, -0.9099,\n",
              "        -0.9099, -0.9099,  0.0206, -0.5997,  0.0206, -0.5997,  0.3307, -0.2896,\n",
              "         0.3307, -1.2201,  0.3307,  0.6409, -0.5997, -1.8404, -1.2201, -0.9099,\n",
              "        -0.9099, -0.2896, -0.2896, -0.2896, -0.2896, -0.5997, -0.2896,  0.0206,\n",
              "        -0.2896, -0.2896, -0.2896, -0.9099, -0.5997, -0.5997, -0.2896, -0.5997,\n",
              "        -0.2896,  0.6409,  0.0206, -0.9099, -1.2201, -0.5997, -0.9099, -1.5302,\n",
              "         0.3307, -0.5997, -0.2896,  0.3307,  0.3307, -0.5997,  0.0206,  0.0206,\n",
              "         0.3307,  0.3307,  0.3307,  0.0206,  0.0206,  0.6409,  0.6409,  0.3307,\n",
              "         0.0206,  0.0206,  0.9510, -0.2896,  0.3307, -1.2201, -0.5997, -0.5997,\n",
              "        -0.2896, -0.5997, -0.2896,  0.6409, -0.5997, -0.9099, -0.5997, -0.9099,\n",
              "        -0.2896, -0.9099, -0.9099,  0.0206, -0.2896, -0.2896, -0.5997,  0.3307,\n",
              "        -0.2896,  0.0206,  0.0206, -0.2896, -0.2896,  0.6409, -0.2896,  0.0206,\n",
              "         0.3307,  0.0206,  0.6409, -1.2201, -0.2896,  0.0206, -0.2896, -0.2896,\n",
              "        -0.5997, -0.2896,  0.3307, -0.2896, -0.2896, -0.2896,  0.0206,  0.3307,\n",
              "         0.3307,  0.0206,  0.3307,  0.0206,  0.9510,  0.9510,  0.0206, -0.5997,\n",
              "        -0.9099, -0.5997, -0.5997, -0.5997, -0.5997, -0.2896, -0.2896, -0.5997,\n",
              "         0.0206,  0.0206, -0.2896, -0.5997, -1.5302,  2.1917,  0.6409,  0.9510,\n",
              "        -1.2201,  0.9510, -0.5997,  0.9510,  0.6409,  2.1917, -0.2896,  0.0206,\n",
              "         2.1917,  0.9510,  1.2612,  0.9510,  1.5714, -0.2896, -0.2896,  4.0526,\n",
              "         2.1917,  0.3307,  3.1221,  1.5714,  1.8815,  0.3307,  0.0206,  1.2612,\n",
              "         1.2612,  0.6409, -0.9099,  0.0206, -0.2896, -0.5997,  1.2612, -0.5997,\n",
              "         0.3307,  1.5714, -1.2201, -1.2201, -0.5997,  0.3307,  0.0206, -1.5302,\n",
              "        -1.2201, -0.9099, -0.5997,  0.0206,  0.3307,  0.9510,  0.0206, -1.2201,\n",
              "        -0.9099, -0.5997, -0.2896, -0.5997, -0.5997, -0.2896, -0.2896,  0.0206,\n",
              "        -1.8404, -0.5997, -0.5997, -0.5997, -0.5997,  0.0206, -0.2896, -0.2896,\n",
              "        -0.9099, -0.5997,  0.3307,  0.3307,  0.0206,  0.0206,  0.9510,  0.3307,\n",
              "        -0.5997,  0.0206, -0.2896,  0.3307,  0.6409, -1.8404, -1.2201, -0.5997,\n",
              "        -1.2201, -0.9099, -0.2896, -0.2896,  0.3307, -0.2896,  0.3307, -0.5997,\n",
              "         0.0206,  0.3307, -0.5997,  0.0206, -0.2896,  3.1221,  0.9510, -0.2896,\n",
              "        -0.2896, -0.2896, -0.2896,  0.3307,  0.6409,  0.9510,  1.8815,  0.6409,\n",
              "         2.5018, -0.9099,  0.3307,  1.5714,  0.3307, -0.2896,  0.0206,  0.9510,\n",
              "         0.6409, -0.9099,  2.1917,  2.8120,  1.8815,  1.8815,  2.1917,  0.0206,\n",
              "        -1.2201,  0.0206,  2.8120, -0.5997,  0.6409, -0.9099, -1.2201, -0.5997,\n",
              "         0.0206, -1.2201, -0.9099, -0.5997, -0.2896, -0.9099, -0.5997,  0.0206,\n",
              "        -0.9099, -0.5997, -0.5997, -0.5997,  0.0206, -1.8404,  0.3307, -1.5302,\n",
              "        -1.2201, -0.5997, -0.2896, -0.5997,  0.0206,  0.0206,  0.3307, -0.9099,\n",
              "        -0.2896,  0.0206,  0.6409, -0.9099,  0.0206,  0.3307,  0.0206,  0.0206,\n",
              "         0.3307, -0.2896,  0.0206,  0.3307,  0.3307, -0.2896,  1.5714,  0.6409,\n",
              "        -1.8404,  1.2612, -1.5302, -0.5997, -1.2201, -1.2201, -1.2201,  0.0206,\n",
              "         0.3307, -0.9099, -0.5997, -0.2896,  0.0206,  0.3307,  0.9510, -0.5997,\n",
              "         0.0206,  0.3307, -0.2896, -0.5997,  0.0206,  0.3307, -1.2201, -0.5997,\n",
              "        -0.9099])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pclYFL115U-Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(knn.predict(X_test), y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik_1t2154sZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95c2707-1945-408a-8179-b18dc14ffe51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5110886"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "517-keElDvpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b6c556-587d-4ef5-fe83-e0a9d384f86f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RegressionActivation_3_Layer()"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "model.class_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWU8UiHDD_PO"
      },
      "outputs": [],
      "source": [
        "# model.class_layer.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Eu3P0zELJ4"
      },
      "outputs": [],
      "source": [
        "# model.class_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvZmmgJaXd85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a471cb-968a-4832-8d6f-94dc65a5ee04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3780\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case Base Maintenance"
      ],
      "metadata": {
        "id": "ZoyxUHmTUDhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The case base maintenance procedure will be different from classification and regression, because as of now, the regression activation layer, is not assigning weights. Instead it is using case activation as a direction indication of the case's weight, and taking a weighted average of case activation* case label to predict the final label.\n",
        "\n",
        "Classification's class activation layer is easier, where we do assign each case with weights for every classes."
      ],
      "metadata": {
        "id": "mUejxRrtVzfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "jiUoLHP0XBXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters here"
      ],
      "metadata": {
        "id": "FGtUDD9NumRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percent = 0.1"
      ],
      "metadata": {
        "id": "ofsviLEIuk9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trim_count = int(percent * len(X_train))"
      ],
      "metadata": {
        "id": "4nGCgEn1ylYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "uw0kyu-FatGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find rows of the lowest percent% values, that are non-zero,  in case2class_weights in every column\n",
        "\n",
        "import torch\n",
        "\n",
        "def classwise_find_lowest_percent_non_zero_rows(matrix, trim_count):\n",
        "  \"\"\"\n",
        "  Finds the rows of the lowest percent% values, that are non-zero, in every column of a matrix.\n",
        "\n",
        "  Args:\n",
        "    matrix: A torch tensor representing the matrix.\n",
        "    percent: A float representing the percentage of the lowest values to find.\n",
        "\n",
        "  Returns:\n",
        "    A list of lists, where each inner list contains the indices of the rows with the lowest percent% non-zero values in each column.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get the number of rows and columns in the matrix.\n",
        "  num_rows, num_cols = matrix.shape\n",
        "\n",
        "  # Initialize a list to store the indices of the lowest percent% non-zero rows in each column.\n",
        "  lowest_percent_rows = []\n",
        "  masked_matrix = matrix.clone()\n",
        "  masked_matrix[matrix == 0] = float('inf')\n",
        "\n",
        "  # Iterate over each column in the matrix.\n",
        "  for col in range(num_cols):\n",
        "\n",
        "    # Get the non-zero elements in the column.\n",
        "    non_zero_elements = matrix[matrix[:, col] != 0] #[:, col]\n",
        "\n",
        "    # Get the number of lowest percent% values to find.\n",
        "    num_lowest_values = (int) (trim_count / num_cols)#int(percent * len(non_zero_elements))\n",
        "    if(num_lowest_values == 0):\n",
        "      num_lowest_values = 1\n",
        "    # Find the indices of the lowest percent% non-zero values in the column.\n",
        "    lowest_values_indices = torch.argsort(masked_matrix[:,col])[:num_lowest_values]\n",
        "\n",
        "    # Append the indices of the lowest percent% non-zero rows in the column to the list.\n",
        "    lowest_percent_rows.append(lowest_values_indices)\n",
        "\n",
        "  combined_list = []\n",
        "  for indices in lowest_percent_rows:\n",
        "    combined_list.extend(indices.tolist())\n",
        "  # combined_list\n",
        "  # Return the list of lists of indices.\n",
        "  return combined_list\n",
        "\n",
        "# Example usage:\n",
        "# matrix = torch.tensor([[1, 2, 3], [4, 5, 0], [6, 0, 7]])\n",
        "# percent = 0.25\n",
        "# lowest_percent_rows = find_lowest_percent_non_zero_rows(matrix, percent)\n",
        "# print(lowest_percent_rows)\n"
      ],
      "metadata": {
        "id": "6anlNSNCk1Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find rows of the lowest percent% values, that are non-zero,  in case2class_weights in every column\n",
        "\n",
        "import torch\n",
        "\n",
        "def global_find_lowest_percent_non_zero_rows(matrix, trim_count):\n",
        "  \"\"\"\n",
        "  Finds the rows of the lowest percent% values, that are non-zero, in every column of a matrix.\n",
        "\n",
        "  Args:\n",
        "    matrix: A torch tensor representing the matrix.\n",
        "    percent: A float representing the percentage of the lowest values to find.\n",
        "\n",
        "  Returns:\n",
        "    A list of lists, where each inner list contains the indices of the rows with the lowest percent% non-zero values in each column.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get the number of rows and columns in the matrix.\n",
        "  num_rows, num_cols = matrix.shape\n",
        "\n",
        "  # Initialize a list to store the indices of the lowest percent% non-zero rows in each column.\n",
        "  lowest_percent_rows = []\n",
        "\n",
        "  #filter out all zero entries.\n",
        "  ## This is due to the nature of class activaiton layer.\n",
        "  non_zero_elements = matrix[matrix != 0]\n",
        "  print(non_zero_elements)\n",
        "  # Get the number of lowest percent% values to find.\n",
        "  num_lowest_values = trim_count # int(percent * len(non_zero_elements))\n",
        "\n",
        "  # Find the indices of the lowest percent% non-zero values in the column.\n",
        "  lowest_values_indices = torch.argsort(non_zero_elements)[:num_lowest_values]\n",
        "  print(lowest_values_indices)\n",
        "  # Append the indices of the lowest percent% non-zero rows in the column to the list.\n",
        "  # lowest_percent_rows.append(lowest_values_indices)\n",
        "\n",
        "  # Return the list of lists of indices.\n",
        "  return lowest_values_indices.to_list()\n",
        "\n",
        "# Example usage:\n",
        "# matrix = torch.tensor([[1, 2, 3], [4, 5, 0], [6, 0, 7]])\n",
        "# percent = 0.25\n",
        "# lowest_percent_rows = find_lowest_percent_non_zero_rows(matrix, percent)\n",
        "# print(lowest_percent_rows)\n"
      ],
      "metadata": {
        "id": "FrfZuXLrXOjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim(X_train, y_train, case2class_weights, trim_count):\n",
        "\n",
        "  # prompt: combined the list_of_indices into a single list\n",
        "\n",
        "  list_of_indices = classwise_find_lowest_percent_non_zero_rows(case2class_weights, trim_count)\n",
        "  X_train_trimmed = np.delete(X_train, list_of_indices, axis=0)\n",
        "  y_train_trimmed = np.delete(y_train, list_of_indices, axis=0)\n",
        "  return X_train_trimmed, y_train_trimmed\n"
      ],
      "metadata": {
        "id": "6MzOO9hAmB_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: trim X_train and y_train to only include the combined_list indices\n"
      ],
      "metadata": {
        "id": "1yYyAz6ZmH8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "ZT63PsG_purf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: wipe accuracies to only keep the last one\n",
        "\n",
        "accuracies = accuracies[-1:]\n",
        "klmnn_accuracies = klmnn_accuracies[-1:]\n",
        "knn_accuracies = knn_accuracies[-1:]\n"
      ],
      "metadata": {
        "id": "3CSQTLWMq8Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "id": "eWHaXc5NrEVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_trimmed = X_train\n",
        "y_train_trimmed = y_train\n",
        "for times in range(9):\n",
        "  case2class_weights = model.class_layer.get_constrained_weight()\n",
        "  #trime\n",
        "  X_train_trimmed, y_train_trimmed= trim(X_train_trimmed, y_train_trimmed, case2class_weights, trim_count)\n",
        "  #train models again, get the new accuracies\n",
        "  klmnn_acc, best_accuracy, knn_acc, model = trainModels(X_train_trimmed,y_train_trimmed)\n",
        "  #after the above step, model has changed to a new one.\n",
        "  klmnn_accuracies.append(klmnn_acc)\n",
        "  accuracies.append(best_accuracy)\n",
        "  knn_accuracies.append(knn_acc)\n",
        "  #break loop if best_accuracy is lower than last\n",
        "  # if best_accuracy < accuracies[-2]:\n",
        "  #   break\n"
      ],
      "metadata": {
        "id": "ZiemHEaTq2Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "id": "PkbYgKylzBaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koSz4phP7J0z"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYQCH4N6I8zO"
      },
      "source": [
        "##Result look up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gARxSXiq-_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ee6ebf-e2a4-4020-a898-d94aa661a7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fa_layer.f1weight\n",
            "tensor([1.1435, 1.1289, 1.1223, 1.1164, 1.1260, 1.1187, 1.1289, 1.1340])\n",
            "ca_layer.fa_weight\n",
            "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "        ...,\n",
            "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "        [1.0074, 1.0039, 1.0000,  ..., 1.0026, 1.0062, 1.0015]])\n",
            "ca_layer.bias\n",
            "tensor([0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.7985])\n"
          ]
        }
      ],
      "source": [
        "print_model_features(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y90VH3xzsTEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a4bddb-016f-4af9-ea8f-7630489e9e19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16376"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT0y7FUq2FNC"
      },
      "source": [
        "####TODO testing here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awqOomGo1orp"
      },
      "outputs": [],
      "source": [
        "# for regression only. for classification is different\n",
        "#feature_activations, case_activations, predicted_number\n",
        "model.eval()\n",
        "feature_activations, case_activations, output, predicted_class = model(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM8cUC_37Rq1",
        "outputId": "7931232c-77ce-497e-bc7e-c962df43ee85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.6000, 2.6000, 3.8000, 2.2000, 1.8000, 1.0000, 5.0000, 3.2000, 2.6000,\n",
              "        1.2000, 2.6000, 2.6000, 3.6000, 2.2000, 1.0000, 1.0000, 3.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFkiqBp91san",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939a9b6e-cec5-45e8-aa08-41431f11eb3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.9189, 2.5603, 3.9753, 1.8536, 1.6518, 1.0000, 4.9998, 3.9955, 3.9436,\n",
              "        1.1133, 2.5762, 3.1909, 4.0000, 2.5791, 1.0007, 1.0000, 3.8713],\n",
              "       grad_fn=<MvBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ],
      "source": [
        "predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7o4dGeF1uEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9f0016-2bff-47e1-c4f2-5dd77610eef1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3., 3., 3., 3., 1., 5., 4., 2., 3., 2., 4., 4., 4., 1., 1., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FClgau3yY32o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1137b3aa-378b-44f2-a07a-0c346297ca7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1.,  ..., 1., 1., 0.],\n",
              "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
              "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
              "        [0., 1., 1.,  ..., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 285
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inspecting the case activations\n",
        "top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n"
      ],
      "metadata": {
        "id": "48QBYZrbgOQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_case_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j00wGXvagfHP",
        "outputId": "473d00a5-1978-43a9-a306-bb5cf9164a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[119,  69,  52,   3, 127],\n",
              "        [ 31,  65,  29,  66,  45],\n",
              "        [ 52, 138,  68,  44,  40],\n",
              "        [116,   5,  11,  45, 123],\n",
              "        [ 31, 116,  19, 104,  91],\n",
              "        [118, 114, 103, 111, 110],\n",
              "        [135,  33, 147, 140, 149],\n",
              "        [121, 127, 123,  11,  84],\n",
              "        [ 63,  26,  64,  68,  85],\n",
              "        [116,   5,  35,  92, 109],\n",
              "        [ 83,  84,   6,  31, 123],\n",
              "        [ 65,  31,  83,  30,  35],\n",
              "        [125,  27,  86,  44,  30],\n",
              "        [ 83,   6,  11,  31,   3],\n",
              "        [117, 101,  95,  90,  96],\n",
              "        [ 99, 110, 106, 115, 112],\n",
              "        [ 67, 129,  65,  31, 144]])"
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvdRN1O-gks5",
        "outputId": "6d043886-1e49-43c1-c98e-fb5315a14614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvo_SHzxgrsR",
        "outputId": "61dc3c39-8ace-4b99-ba12-389d2eaa6fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[top_case_indices[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPHo0xGuggkw",
        "outputId": "6961d35e-769b-42c3-87f5-894dc4c57abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
              "         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
              "         0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
              "         0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
              "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.],\n",
              "        [1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
              "         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
              "         0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
              "         0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
              "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.],\n",
              "        [1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
              "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
              "         0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
              "         0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.],\n",
              "        [0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
              "         0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
              "         0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.],\n",
              "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
              "         0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
              "         1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
              "         0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[top_case_indices[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ugt-xZ0gwCl",
        "outputId": "1d29ccd5-00c0-4fce-96eb-97631a49497a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 4., 4., 2., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIMYkbGc-Gv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbe97ac-b9e1-4eb1-c443-479830d21eba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ],
      "source": [
        "knn.predict(X_test)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = knn.kneighbors(X_test)[1][0]"
      ],
      "metadata": {
        "id": "DEzwWx_UlcZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTU2dZyBlvs-",
        "outputId": "a8a5824e-842e-4654-f468-e2723d3f5c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([119, 127,  60,  69,   3])"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkCaYV47lxnA",
        "outputId": "b781a7e0-5b67-4849-ff6f-0d75ad5b46c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 4., 2., 4., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UklELrXlZNvm"
      },
      "source": [
        "###Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DAHjmSILFnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe63325-8938-4465-ea8a-3e1c928284fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.6363636363636364, 1.0, 0.5454545454545454, 1.0, 1.0, 0.8181818181818182]\n"
          ]
        }
      ],
      "source": [
        "print(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThviV3f3w6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8480ae-4026-4aae-b102-b064369f3eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.4233803, 1.8344585, 1.306988, 1.4484789, 1.1512542, 1.3633262, 1.4535525, 1.5095526, 1.3163508, 1.6558268]\n"
          ]
        }
      ],
      "source": [
        "print(knn_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(klmnn_accuracies)"
      ],
      "metadata": {
        "id": "jdP6ssRXyIb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58ac21d-9ade-42bc-80c3-f3386b200991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 0.8333333333333334, 1.0, 1.0, 0.9444444444444444, 0.8888888888888888, 1.0, 0.8888888888888888, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10AORGQInY-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe2449c-e02f-4c3e-8449-9a1bf8a34f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222221\n",
            "0.6967320261437908\n",
            "0.9555555555555555\n"
          ]
        }
      ],
      "source": [
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "print(np.mean(klmnn_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nllY5LQDDKse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24999e1a-88c9-4020-df6e-0ba6fa8adbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4270, grad_fn=<MeanBackward0>)\n",
            "0.8529457\n",
            "0.4803648\n"
          ]
        }
      ],
      "source": [
        "# For regression\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVHtGRsrUu7H"
      },
      "outputs": [],
      "source": [
        "# prompt: get the number of unique values in the list of tensor y_train\n",
        "\n",
        "print(torch.unique(y_train).shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ub7LFE5UdDP"
      },
      "outputs": [],
      "source": [
        "torch.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qeV7IHLDpRR"
      },
      "source": [
        "##Results, classification, zebra\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzEwyfuOGElJ"
      },
      "source": [
        "we are using 110 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDn2UR_hDs2_"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9090909090909092\n",
        "0.36363636363636365\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99OeX0wUD5EG"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.8727272727272727\n",
        "0.3181818181818182\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxMcxPBaE4T1"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.8727272727272727\n",
        "0.34545454545454546\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xBV-JBRFnsH"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9181818181818182\n",
        "0.3090909090909091\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSOsMVN0q5oO"
      },
      "source": [
        "##Results, classification, double zebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4NG5pN4rJKs"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.85\n",
        "0.36818181818181817\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM0I_QD7s00D"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = False\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9727272727272729\n",
        "0.39545454545454545\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FkVw9gg2qi6"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "case weighting can compensate!! Even\n",
        "\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9681818181818181\n",
        "0.36818181818181817\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv1COulNQdAY"
      },
      "source": [
        "##Results, classification, fetch_olivetti_faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggj0rN4XQhBt"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "0s\n",
        "123\n",
        "# For classificationprint(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.8625\n",
        "0.85\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1C9EQRsHflK"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classificationprint(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9349999999999998\n",
        "0.85\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7RU-RRcDC6F"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9875\n",
        "0.8875\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCYyxPiJ3sl7"
      },
      "source": [
        "##Results, classification, digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJnfWHP1j57Y"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "During this run, we noticed that the training stopped pretty fast after patience. If patience is 40, we often stop before 50. This means that this data set is actually easy.\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9833022967101179\n",
        "0.9872004965859714\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkXc8OnvMI2m"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "0.9872036002482931\n",
        "0.9872098075729362\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ts8a5_o_gP"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "0.988\n",
        "0.985\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_srOl9RDC4E_"
      },
      "source": [
        "##Results, classification, iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d87QjdBtP7w"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9266666666666667\n",
        "0.9666666666666668\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivz0QCjhzpqM"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9666666666666668\n",
        "0.96\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ02ethFJ0B0"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.96\n",
        "0.9666666666666668\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqSg9--skYJy"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = False\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9533333333333334\n",
        "0.96\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp1OFAQOziaS"
      },
      "source": [
        "##Results, classification, wine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-vxACVlzk1u"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classificationprint(np.mean(accuracies))print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.961111111111111\n",
        "0.6803921568627451\n",
        "```\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.8663398692810457\n",
        "0.6980392156862746\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI9xXzSz0yyL"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "new\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9944444444444445\n",
        "0.6839869281045752\n",
        "```\n",
        "\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.8052287581699347\n",
        "0.684640522875817\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRgzE4jl1E_v"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "new\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9666666666666666\n",
        "0.7026143790849673\n",
        "```\n",
        "\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.6781045751633987\n",
        "0.6915032679738562\n",
        "```\n",
        "This result is to be expected. As we don't share weighting, topk may not work because different cases can be trained during training. They do not work collectively as well during testing, as different set of cases could be retrieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASjRV9422nzQ"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "new\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9833333333333334\n",
        "0.7189542483660131\n",
        "```\n",
        "\n",
        "\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.6970588235294117\n",
        "0.696078431372549\n",
        "```\n",
        "It seems that, weight sharing is really good for this data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ7e2l7lX6Es"
      },
      "source": [
        "##Results, classification, breast cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Dz9gwyzLiR"
      },
      "source": [
        "I reran things below. Something wasn't right with the kNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YrhxmJ3lxVr"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9455513784461151\n",
        "0.4514411027568922\n",
        "```\n",
        "????\n",
        "Old result below, it is so different from our new one?\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9104323308270675\n",
        "0.9297932330827068\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmQz3W1fqkOL"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9420112781954886\n",
        "0.47092731829573936\n",
        "```\n",
        "This 0.47 is probably something going wrong... We can ignore this one and use other knn accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFEv9cn6rnhS"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9227130325814535\n",
        "0.9332706766917293\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfLJaow_rq36"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9595551378446114\n",
        "0.9332080200501254\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox8LsmRb5DnI"
      },
      "source": [
        "##Results, regression, Diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiyPBzOA15nV"
      },
      "source": [
        "DOING, continue rerun my experiments from here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnS97fLj5Kj7"
      },
      "source": [
        "loss is MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGQbG9nj5FqQ"
      },
      "source": [
        "```\n",
        "For the vanilla neural network\n",
        "\n",
        "Average loss on the test set: 5937.172802734375\n",
        "```\n",
        "\n",
        "This is a bad neural net as it only outputs a single number as output\n",
        "\n",
        "I am not sure why. I also tried training with batch but no good result`\n",
        "\n",
        "This neural network should be powerful enough as it has about 900 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNvHmRF_D1ul"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(6001.3232, grad_fn=<MeanBackward0>)\n",
        "3944.142\n",
        "3665.2056\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvLNHcb9GKqJ"
      },
      "source": [
        "Here we see that our model performs worse than baseline knn. I think it's because top k is enabled.\n",
        "\n",
        "Let's turn off top k and see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwKD02lLXNcr"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "```\n",
        "12345\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(3819.3308, grad_fn=<MeanBackward0>)\n",
        "4752.0566\n",
        "3813.1926\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2BEzJkmhlZc"
      },
      "source": [
        "Turning off ca_weight_sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0_FoDeMhtLt"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "12345\n",
        "# For regression\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(3560.3840, grad_fn=<MeanBackward0>)\n",
        "5768.119\n",
        "3647.5903\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_6SF2ysoNuF"
      },
      "source": [
        "```\n",
        "1234\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(3385.0386, grad_fn=<MeanBackward0>)\n",
        "5738.933\n",
        "3572.9805\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4rIyA5RWxac"
      },
      "source": [
        "##Results, regression, California\n",
        "\n",
        "All the leaves are brown ~\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUSW7u59VumL"
      },
      "source": [
        "California Housing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_-P0DljcmNQ"
      },
      "source": [
        "```\n",
        "For the vanilla neural network\n",
        "\n",
        "Loss on the test set: 1.3838589191436768\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD86mIVQW4tJ"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "tensor(0.8763, grad_fn=<MeanBackward0>)\n",
        "0.7351894\n",
        "1.4575317\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMXym3iMigp"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "```\n",
        "12345\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "account_circle\n",
        "tensor(0.8593, grad_fn=<MeanBackward0>)\n",
        "0.8837114\n",
        "1.4449631\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXdfL9-aN15y"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(0.6832, grad_fn=<MeanBackward0>)\n",
        "1.179338\n",
        "1.448509\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak6-DEZeZxcb"
      },
      "source": [
        "#Interesting notes  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGpEt98hZzzJ"
      },
      "source": [
        "There are lots and lots of design decisions to make for your neural network. Here I list a few:\n",
        "\n",
        "*   For activation functions in each layer, do you use relu (only positive), or leakyRelu (to avoid dead neuron)\n",
        "*   For certain weights, do you want them to be positive only?\n",
        "*   For case->class activation, do you do a ``torch.topk`` to only allow a few top cases to activate or all cases to activate?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzavJN1pNh-y"
      },
      "source": [
        "#Tensor operation broadcasting experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkFyuBTFNamy"
      },
      "source": [
        "The above code relies on the broadcasting feature of pytorch tensor operations:\n",
        "\n",
        "PyTorch broadcasts the smaller tensor (tensor2) along the missing dimensions so that its shape matches that of the larger tensor (tensor1). The element-wise multiplication is then performed as usual.\n",
        "\n",
        "Keep in mind that broadcasting has some rules, and the dimensions of the tensors need to be compatible. The trailing dimensions of the tensors must either be the same or one of them should be 1. If a dimension is missing in one of the tensors, PyTorch considers it to be of size 1 for the purpose of broadcasting. If broadcasting cannot be done, PyTorch will raise a RuntimeError."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_u4VvM4MPAf"
      },
      "outputs": [],
      "source": [
        "tensor_1 = torch.randn(1)\n",
        "tensor_2 = torch.randn(10,5)\n",
        "tensor_3 = torch.randn(10,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wWjRKRlnPA7"
      },
      "outputs": [],
      "source": [
        "# result = torch.matmul(tensor_2, tensor_3)\n",
        "result = tensor_2 * tensor_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-nboj_-nYzk"
      },
      "outputs": [],
      "source": [
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM9C_MKBMaLh"
      },
      "outputs": [],
      "source": [
        "print(tensor_1)\n",
        "print(tensor_2)\n",
        "print(tensor_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5TOI7qIMrsh"
      },
      "outputs": [],
      "source": [
        "tensor_2* tensor_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU6XNBTHNHh4"
      },
      "outputs": [],
      "source": [
        "tensor_1 * tensor_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og0Vd1GdMCcG"
      },
      "outputs": [],
      "source": [
        "tensor1 = torch.tensor([5.3712e-01, 1.6094e-01, -8.2879e-02, 7.0379e-01, -1.9436e-01,\n",
        "                        -8.6414e-02, -1.1221e+00, 1.7594e+00, -6.0423e-01, 1.9192e-02])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSuD7F5BNsat"
      },
      "outputs": [],
      "source": [
        "torch.matmul(tensor1, tensor1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcbrjZMhMJKF"
      },
      "outputs": [],
      "source": [
        "tensor2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJgs8II-Lm05"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the 1D tensor\n",
        "tensor1 = torch.tensor([[5.3712e-01, 1.6094e-01, -8.2879e-02, 7.0379e-01, -1.9436e-01,\n",
        "                        -8.6414e-02, -1.1221e+00, 1.7594e+00, -6.0423e-01, 1.9192e-02]])\n",
        "\n",
        "# Define the column vector (2D tensor)\n",
        "tensor2 = torch.tensor([[0.1390],\n",
        "                        [0.0275],\n",
        "                        [-0.5462],\n",
        "                        [-1.9431],\n",
        "                        [-0.0524],\n",
        "                        [0.9949],\n",
        "                        [-2.7876],\n",
        "                        [0.3611],\n",
        "                        [0.0308],\n",
        "                        [0.0524]])\n",
        "\n",
        "# Element-wise multiplication\n",
        "result = torch.matmul(tensor1 , tensor2)\n",
        "\n",
        "\n",
        "\n",
        "# Alternatively, you can use the * operator\n",
        "# result = tensor1 * tensor2.squeeze()\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXEY0twgLP9K"
      },
      "outputs": [],
      "source": [
        "torch.matmul(tensor_2, tensor_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yjIqbApsCLk"
      },
      "outputs": [],
      "source": [
        "torch.matmul(tensor_3, tensor_2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "hdMkoZr_KgZX",
        "8vxYpkSzeWw_",
        "f2NnlWpVWqn6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}