{"cells":[{"cell_type":"markdown","metadata":{"id":"rxoHWNZEmG4g"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"CsfLxAVaPOTf"},"source":["On google colab, you have to restart runtime after running the following line"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6548,"status":"ok","timestamp":1718884970535,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"T3Y5CLhymG4l","outputId":"98667d8d-88b4-4880-a2eb-13d3c6bf4eb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n"]}],"source":["!pip install omegaconf"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4426,"status":"ok","timestamp":1718884974959,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"BxzI25zfmG4n","outputId":"64e59338-2247-48a3-dde3-5cd3d09f6d44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")\n","#\"/content/drive/My Drive/NN-kNN/\"\n","folder_name = \"/content/drive/Othercomputers/My MacBook Pro/GitHub/NN-kNN/\"\n","import sys\n","sys.path.insert(0,folder_name)"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"q-0ffsjpbDLE","executionInfo":{"status":"ok","timestamp":1718884974959,"user_tz":-480,"elapsed":5,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["##This is added because my Rdata uses Cdata for the covid data set.\n","##Rdata use Cdata function to load the data set, then convert it to regression problem\n","import os\n","import sys\n","sys.path.append('/content/drive/Othercomputers/My MacBook Pro/GitHub/NN-kNN/dataset')\n"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"joYfU4jLmG4o","executionInfo":{"status":"ok","timestamp":1718884974959,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# folder_name = os.getcwd()"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"oDGrACwVmG4o","executionInfo":{"status":"ok","timestamp":1718884974959,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","from tqdm import tqdm\n","from omegaconf import DictConfig, OmegaConf\n","\n","from dataset import cls_small_data as Cdata\n","import model.cls_model as Cmodel\n","from dataset import cls_medium_data\n","\n","from dataset import reg_data as Rdata\n","import model.reg_model as Rmodel"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"bzTxZZJnmG4p","executionInfo":{"status":"ok","timestamp":1718884974959,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["conf_file = OmegaConf.load(os.path.join(folder_name, 'config.yaml'))"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"yT3W4iqSmG4p","executionInfo":{"status":"ok","timestamp":1718884974959,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcsdY45IjhSV","executionInfo":{"status":"ok","timestamp":1718884974959,"user_tz":-480,"elapsed":3,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"911466fa-8c8b-4fdc-f9d3-25c688d7138c"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"oaKz8Ns3mG4q"},"source":["# NCA and LMNN setup"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5626,"status":"ok","timestamp":1718883716619,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"cWQ8I6icoU8X","outputId":"533b05b1-b5b3-4181-d794-8a0fc617b369"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting metric-learn\n","  Downloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.25.2)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (3.5.0)\n","Installing collected packages: metric-learn\n","Successfully installed metric-learn-0.7.0\n"]}],"source":["pip install metric-learn"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"zmjbXrjQmG4q","executionInfo":{"status":"ok","timestamp":1718883716620,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["import metric_learn\n","from metric_learn import LMNN,NCA"]},{"cell_type":"markdown","metadata":{"id":"46TjfSz-mG4q"},"source":["# Data Sets"]},{"cell_type":"markdown","metadata":{"id":"ZkDrsvjWmG4r"},"source":["Supported small dataset for classification:  \n","'zebra',\n","'zebra_special',\n","'bal',\n","'digits',\n","'iris',\n","'wine',\n","'breast_cancer',\n","\n","for regression:\n","'califonia_housing',\n","'abalone',\n","'diabets',\n","'body_fat',\n","'ziweifaces'\n","\n","\n","Newly added data sets for mental health (psychology):\n","\n","Classification:\n","'psych_depression_physical_symptons',\n","'covid_anxious',\n","'covid_depressed'\n"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25163,"status":"ok","timestamp":1718885328838,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"lxq-FY3FmG4r","outputId":"1cc4bf5e-6684-450a-8722-5532f0700fb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the dataset: Index(['SU_ID', 'P_PANEL', 'NATIONAL_WEIGHT', 'REGION_WEIGHT',\n","       'NATIONAL_WEIGHT_POP', 'REGION_WEIGHT_POP', 'NAT_WGT_COMB_POP',\n","       'REG_WGT_COMB_POP', 'P_GEO', 'SOC1',\n","       ...\n","       'REGION9', 'P_DENSE', 'MODE', 'LANGUAGE', 'MAIL50', 'RACE1_BANNER',\n","       'RACE2_BANNER', 'INC_BANNER', 'AGE_BANNER', 'HH_BANNER'],\n","      dtype='object', length=177)\n","   SOC1  SOC2A  SOC2B  SOC3A  SOC3B  SOC4A  SOC4B  PHYS8  PHYS1A  PHYS1B  \\\n","0   3.0    4.0    4.0    1.0    1.0    2.0    2.0    3.0     1.0     1.0   \n","1   3.0    3.0    3.0    1.0    3.0    2.0    2.0    2.0     2.0     2.0   \n","2   3.0    3.0    3.0    1.0    1.0    2.0    1.0    2.0     2.0     1.0   \n","3   3.0    2.0    2.0    1.0    1.0    2.0    1.0    3.0     2.0     2.0   \n","4   3.0    1.0    1.0    1.0    1.0    2.0    2.0    3.0     1.0     1.0   \n","5   2.0    3.0    3.0    1.0    1.0    1.0    1.0    2.0     2.0     2.0   \n","6   2.0    4.0    3.0    1.0    1.0    2.0    2.0    NaN     1.0     2.0   \n","7   3.0    3.0    3.0    1.0    2.0    2.0    2.0    3.0     2.0     2.0   \n","8   1.0    4.0    5.0    3.0    3.0    2.0    2.0    1.0     2.0     2.0   \n","9   3.0    4.0    3.0    3.0    4.0    1.0    2.0    2.0     2.0     2.0   \n","\n","   PHYS1C  PHYS1D  PHYS1E  PHYS1F  PHYS1G  PHYS1H  PHYS1I  PHYS1J  PHYS1K  \\\n","0     2.0     1.0     1.0     2.0     2.0     1.0     1.0     1.0     1.0   \n","1     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","2     2.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0     2.0   \n","3     2.0     2.0     2.0     2.0     1.0     2.0     1.0     1.0     2.0   \n","4     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0   \n","5     2.0     2.0     1.0     2.0     NaN     2.0     2.0     NaN     2.0   \n","6     1.0     2.0     1.0     2.0     2.0     2.0     2.0     1.0     2.0   \n","7     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","8     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","9     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","\n","   PHYS1L  PHYS1M  PHYS1N  PHYS1O  PHYS1P  PHYS1Q  SOC5A  SOC5B  SOC5C  SOC5D  \\\n","0     1.0     2.0     1.0     1.0     2.0     1.0    1.0    1.0    1.0    2.0   \n","1     2.0     2.0     1.0     2.0     2.0     2.0    1.0    1.0    1.0    1.0   \n","2     2.0     2.0     2.0     2.0     2.0     2.0    1.0    1.0    1.0    2.0   \n","3     2.0     1.0     2.0     2.0     2.0     1.0    1.0    1.0    2.0    1.0   \n","4     1.0     2.0     1.0     2.0     2.0     2.0    1.0    1.0    1.0    1.0   \n","5     1.0     NaN     2.0     2.0     2.0     2.0    2.0    4.0    4.0    1.0   \n","6     2.0     2.0     1.0     1.0     2.0     2.0    3.0    4.0    4.0    3.0   \n","7     2.0     2.0     2.0     2.0     2.0     2.0    1.0    2.0    2.0    1.0   \n","8     2.0     2.0     2.0     2.0     2.0     2.0    1.0    1.0    1.0    1.0   \n","9     2.0     2.0     2.0     2.0     2.0     2.0    3.0    1.0    3.0    3.0   \n","\n","   SOC5E  PHYS2_1  PHYS2_2  PHYS2_3  PHYS2_4  PHYS2_5  PHYS2_6  PHYS2_7  \\\n","0    2.0        0        1        0        0        0        0        0   \n","1    1.0        0        1        0        0        0        0        0   \n","2    1.0        0        1        0        1        0        0        1   \n","3    1.0        0        1        0        1        0        1        0   \n","4    1.0        1        1        0        1        0        0        0   \n","5    1.0        0        1        0        1        0        1        0   \n","6    3.0        1        1        0        0        0        1        0   \n","7    1.0        0        1        0        0        0        0        0   \n","8    1.0        0        1        0        1        0        0        1   \n","9    1.0        0        1        0        1        1        0        0   \n","\n","   PHYS2_8  PHYS2_9  PHYS2_10  PHYS2_11  PHYS2_12  PHYS2_13  PHYS2_14  \\\n","0        1        1         0         1         0         1         0   \n","1        1        0         0         1         1         1         1   \n","2        1        1         0         1         0         1         0   \n","3        1        1         0         1         0         1         0   \n","4        1        0         0         1         0         1         0   \n","5        1        1         0         1         1         1         0   \n","6        1        0         0         1         1         1         1   \n","7        1        1         0         1         1         1         0   \n","8        0        0         0         0         0         0         0   \n","9        1        1         1         1         0         1         0   \n","\n","   PHYS2_15  PHYS2_16  PHYS2_17  PHYS2_18  PHYS2_19  PHYS2_DK  PHYS2_SKP  \\\n","0         1         1         1         1         0         0          0   \n","1         1         1         1         0         1         0          0   \n","2         0         1         1         0         1         0          0   \n","3         1         1         1         0         1         0          0   \n","4         0         1         1         0         1         0          0   \n","5         1         1         1         0         0         0          0   \n","6         1         1         1         1         1         0          0   \n","7         0         1         0         0         1         0          0   \n","8         0         0         1         0         1         0          0   \n","9         1         1         1         0         0         0          0   \n","\n","   PHYS2_REF  PHYS10A  PHYS10B  PHYS10C  PHYS10D  PHYS10E  ECON8A  ECON8B  \\\n","0          0      3.0      3.0      4.0      1.0      1.0     2.0     2.0   \n","1          0      2.0      3.0      3.0      1.0      1.0     1.0     1.0   \n","2          0      1.0      2.0      1.0      1.0      1.0     2.0     2.0   \n","3          0      5.0      5.0      4.0      2.0      2.0     2.0     2.0   \n","4          0      4.0      4.0      4.0      2.0      1.0     2.0     2.0   \n","5          0      5.0      4.0      3.0      1.0      1.0     1.0     2.0   \n","6          0      1.0      1.0      1.0      1.0      1.0     1.0     1.0   \n","7          0      4.0      4.0      1.0      1.0      1.0     2.0     1.0   \n","8          0      1.0      1.0      1.0      1.0      1.0     2.0     2.0   \n","9          0      3.0      3.0      4.0      3.0      2.0     1.0     2.0   \n","\n","   ECON8C  ECON8D  ECON8E  ECON8F  ECON8G  ECON8H  ECON8I  ECON8J  ECON8K  \\\n","0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","1     1.0     1.0     1.0     1.0     1.0     2.0     2.0     1.0     1.0   \n","2     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","3     1.0     2.0     1.0     2.0     2.0     2.0     1.0     1.0     2.0   \n","4     2.0     2.0     2.0     2.0     2.0     1.0     1.0     2.0     1.0   \n","5     1.0     2.0     1.0     1.0     2.0     1.0     2.0     1.0     1.0   \n","6     2.0     1.0     2.0     1.0     1.0     2.0     1.0     2.0     1.0   \n","7     2.0     1.0     2.0     2.0     1.0     1.0     2.0     1.0     2.0   \n","8     2.0     2.0     1.0     2.0     1.0     1.0     1.0     1.0     2.0   \n","9     1.0     1.0     2.0     2.0     1.0     1.0     1.0     2.0     2.0   \n","\n","   ECON8L  ECON8M  ECON8N  ECON8O  ECON8P  ECON8Q  ECON8R  ECON8S  ECON7_1  \\\n","0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0        0   \n","1     2.0     2.0     2.0     1.0     2.0     2.0     1.0     1.0        1   \n","2     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0        1   \n","3     2.0     1.0     1.0     2.0     2.0     2.0     1.0     1.0        1   \n","4     1.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0        1   \n","5     2.0     2.0     1.0     1.0     2.0     2.0     1.0     1.0        1   \n","6     1.0     2.0     2.0     2.0     1.0     2.0     1.0     1.0        0   \n","7     2.0     2.0     1.0     2.0     1.0     1.0     1.0     2.0        1   \n","8     2.0     2.0     2.0     2.0     1.0     1.0     1.0     1.0        0   \n","9     2.0     2.0     2.0     1.0     2.0     2.0     1.0     1.0        1   \n","\n","   ECON7_2  ECON7_3  ECON7_4  ECON7_5  ECON7_6  ECON7_7  ECON7_8  ECON7_DK  \\\n","0        1        1        0        0        0        0        0         0   \n","1        0        0        0        0        0        0        0         0   \n","2        0        0        0        0        0        0        0         0   \n","3        0        1        0        0        0        0        0         0   \n","4        0        1        0        0        0        0        0         0   \n","5        0        1        0        0        0        0        0         0   \n","6        0        0        0        0        0        0        1         0   \n","7        1        0        0        0        0        0        0         0   \n","8        0        0        0        0        0        0        1         0   \n","9        0        1        0        0        0        0        0         0   \n","\n","   ECON7_SKP  ECON7_REF  ECON1  ECON2  ECON4  ECON3  ECON4A  ECON4B  ECON6A  \\\n","0          0          0    3.0    0.0    7.0   40.0     1.0     1.0     4.0   \n","1          0          0    3.0    0.0    2.0    0.0     5.0     5.0     4.0   \n","2          0          0    2.0    0.0    0.0   40.0     1.0     1.0     4.0   \n","3          0          0    3.0    0.0    2.0   16.0     3.0     3.0     4.0   \n","4          0          0    3.0    0.0    2.0   25.0     2.0     1.0     4.0   \n","5          0          0    1.0   60.0    0.0   50.0     1.0     1.0     4.0   \n","6          0          0    3.0    0.0    2.0    0.0     5.0     5.0     4.0   \n","7          0          0    1.0   40.0    0.0   40.0     1.0     1.0     4.0   \n","8          0          0    1.0   10.0    0.0   20.0     1.0     2.0     4.0   \n","9          0          0    1.0   35.0    0.0   55.0     1.0     2.0     2.0   \n","\n","   ECON6B  ECON6C  ECON6D  ECON6E  ECON6F  ECON6G  ECON6H  ECON6I  ECON6J  \\\n","0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0   \n","1     4.0     4.0     1.0     4.0     1.0     4.0     4.0     4.0     4.0   \n","2     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0   \n","3     4.0     4.0     1.0     4.0     4.0     4.0     4.0     4.0     4.0   \n","4     4.0     4.0     1.0     4.0     1.0     4.0     4.0     4.0     4.0   \n","5     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0   \n","6     3.0     4.0     1.0     1.0     1.0     4.0     4.0     4.0     4.0   \n","7     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0   \n","8     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0   \n","9     4.0     4.0     4.0     4.0     4.0     1.0     4.0     4.0     4.0   \n","\n","   ECON6K  ECON6L  ECON5A_A  ECON5A_B  PHYS7_1  PHYS7_2  PHYS7_3  PHYS7_4  \\\n","0     4.0     4.0       2.0       3.0        0        1        0        0   \n","1     4.0     4.0       3.0       3.0        0        0        0        1   \n","2     4.0     4.0       3.0       3.0        0        0        0        1   \n","3     4.0     4.0       3.0       3.0        0        0        0        1   \n","4     4.0     4.0       3.0       3.0        0        0        0        1   \n","5     4.0     4.0       3.0       3.0        0        0        0        1   \n","6     4.0     4.0       1.0       2.0        0        0        0        1   \n","7     4.0     4.0       3.0       3.0        0        0        0        1   \n","8     4.0     4.0       1.0       1.0        0        0        0        1   \n","9     4.0     4.0       3.0       3.0        0        0        0        1   \n","\n","   PHYS7_DK  PHYS7_SKP  PHYS7_REF  PHYS11  PHYS9A  PHYS9B  PHYS9C  PHYS9D  \\\n","0         0          0          0     1.0     2.0     2.0     2.0     1.0   \n","1         0          0          0     1.0     2.0     1.0     2.0     2.0   \n","2         0          0          0     2.0     2.0     1.0     2.0     2.0   \n","3         0          0          0     2.0     2.0     2.0     2.0     2.0   \n","4         0          0          0     1.0     2.0     1.0     2.0     2.0   \n","5         0          0          0     1.0     1.0     2.0     2.0     2.0   \n","6         0          0          0     1.0     2.0     2.0     2.0     1.0   \n","7         0          0          0     2.0     1.0     2.0     2.0     2.0   \n","8         0          0          0     2.0     1.0     2.0     2.0     2.0   \n","9         0          0          0     1.0     1.0     2.0     2.0     2.0   \n","\n","   PHYS9E  PHYS9F  PHYS9G  PHYS9H  PHYS3A  PHYS3B  PHYS3C  PHYS3D  PHYS3E  \\\n","0     2.0     2.0     2.0     1.0     2.0     2.0     2.0     1.0     2.0   \n","1     1.0     2.0     2.0     2.0     2.0     1.0     2.0     2.0     2.0   \n","2     1.0     2.0     2.0     1.0     2.0     2.0     2.0     2.0     2.0   \n","3     1.0     2.0     2.0     2.0     2.0     1.0     2.0     2.0     2.0   \n","4     1.0     2.0     2.0     2.0     2.0     1.0     2.0     1.0     1.0   \n","5     2.0     2.0     2.0     2.0     2.0     2.0     2.0     NaN     2.0   \n","6     2.0     2.0     2.0     2.0     1.0     1.0     2.0     1.0     2.0   \n","7     2.0     2.0     2.0     2.0     1.0     1.0     1.0     2.0     2.0   \n","8     1.0     1.0     1.0     1.0     2.0     2.0     2.0     2.0     2.0   \n","9     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","\n","   PHYS3F  PHYS3G  PHYS3H  PHYS3I  PHYS3J  PHYS3K  PHYS3L  PHYS3M  PHYS4  \\\n","0     1.0     2.0     1.0     2.0     2.0     2.0     2.0     1.0    1.0   \n","1     2.0     2.0     2.0     2.0     2.0     1.0     1.0     2.0    2.0   \n","2     2.0     1.0     2.0     2.0     2.0     2.0     2.0     1.0    2.0   \n","3     2.0     2.0     1.0     2.0     2.0     2.0     2.0     1.0    2.0   \n","4     1.0     1.0     2.0     2.0     2.0     2.0     2.0     1.0    2.0   \n","5     NaN     NaN     2.0     2.0     2.0     2.0     2.0     2.0    2.0   \n","6     2.0     1.0     1.0     2.0     2.0     2.0     1.0     1.0    2.0   \n","7     2.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0    2.0   \n","8     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0    2.0   \n","9     NaN     1.0     2.0     2.0     2.0     2.0     2.0     2.0    2.0   \n","\n","   PHYS5  PHYS6  AGE4  AGE7  GENDER  RACETH  RACE_R2 HHINCOME  EDUCATION  \\\n","0    2.0    2.0   2.0   3.0     2.0     1.0      1.0        3        3.0   \n","1    2.0    2.0   4.0   6.0     1.0     1.0      1.0        9        5.0   \n","2    2.0    2.0   4.0   6.0     1.0     NaN      1.0        9        5.0   \n","3    2.0    2.0   4.0   6.0     1.0     2.0      2.0        7        5.0   \n","4    2.0    2.0   4.0   6.0     1.0     1.0      1.0        7        2.0   \n","5    2.0    2.0   2.0   2.0     1.0     1.0      1.0        7        6.0   \n","6    2.0    2.0   3.0   4.0     2.0     3.0      2.0        2        5.0   \n","7    2.0    2.0   4.0   5.0     1.0     1.0      1.0        9        5.0   \n","8    2.0    2.0   1.0   1.0     2.0     2.0      2.0        1        2.0   \n","9    2.0    2.0   2.0   2.0     1.0     NaN      1.0        8        5.0   \n","\n","   EDUC4  HHSIZE1  HH01S  HH25S  HH612S  HH1317S  HH18OVS  REGION4  REGION9  \\\n","0    3.0      4.0      0      1       0        2        1        1        2   \n","1    4.0      2.0      0      0       0        0        2        4        8   \n","2    4.0      2.0      0      0       0        0        2        4        9   \n","3    4.0      2.0      0      0       0        0        2        1        2   \n","4    2.0      2.0      0      0       0        0        2        2        3   \n","5    4.0      2.0      0      0       0        0        2        1        2   \n","6    4.0      4.0      0      0       0        0        4        3        5   \n","7    4.0      2.0      0      0       0        0        2        2        3   \n","8    2.0      1.0      0      0       0        0        1        2        3   \n","9    4.0      3.0      0      0       1        0        2        4        8   \n","\n","   P_DENSE  MODE  LANGUAGE  RACE1_BANNER  INC_BANNER  AGE_BANNER  HH_BANNER  \n","0      3.0     2         1           1.0         1.0         2.0        4.0  \n","1      3.0     2         1           1.0         4.0         3.0        2.0  \n","2      3.0     2         1           NaN         4.0         3.0        2.0  \n","3      3.0     2         1           2.0         3.0         3.0        2.0  \n","4      3.0     2         1           1.0         3.0         3.0        2.0  \n","5      3.0     2         1           1.0         3.0         2.0        2.0  \n","6      3.0     2         2           3.0         1.0         2.0        5.0  \n","7      3.0     2         1           1.0         4.0         2.0        2.0  \n","8      3.0     2         1           2.0         1.0         2.0        1.0  \n","9      2.0     2         1           NaN         3.0         2.0        3.0  \n","[ 3.  4.  4.  1.  1.  2.  2.  3.  1.  1.  2.  1.  1.  2.  2.  1.  1.  1.\n","  1.  1.  2.  1.  1.  2.  1.  1.  1.  2.  2.  0.  1.  0.  0.  0.  0.  0.\n","  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  3.  3.  4.\n","  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  2.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  7. 40.\n","  1.  1.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  2.  3.  0.  1.\n","  0.  0.  0.  0.  0.  1.  2.  2.  2.  1.  2.  2.  2.  1.  2.  2.  2.  1.\n","  2.  1.  2.  1.  2.  2.  2.  2.  1.  1.  2.  2.  2.  3.  2.  1.  1.  3.\n","  3.  3.  4.  0.  1.  0.  2.  1.  1.  2.  3.  2.  1.  1.  1.  2.  4.]\n"]}],"source":["dataset_name = 'covid_depressed'\n","cfg = conf_file['dataset'][dataset_name]\n","#TODO need to add other covid data sets here.\n","if dataset_name in ['covid_depressed','covid_anxious','covid_physical','covid_lonely','covid_hopeless',\n","                    'psych_depression_physical_symptons',\n","                    'zebra','zebra_special','bal','digits','iris','wine','breast_cancer']:\n","    criterion = torch.nn.CrossEntropyLoss()\n","    Xs, ys = Cdata.Cls_small_data(dataset_name)\n","elif dataset_name in []:\n","    criterion = torch.nn.CrossEntropyLoss()\n","    Xs, ys = cls_medium_data.Cls_medium_data(dataset_name)\n","else:\n","    criterion = torch.nn.MSELoss()\n","    Xs, ys = Rdata.Reg_data(dataset_name)"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718885328838,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"aOIbeEfEmG4s","outputId":"b3edcce9-00f6-4156-e5ab-f674019af8d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'dataset.cls_small_data' from '/content/drive/Othercomputers/My MacBook Pro/GitHub/NN-kNN/dataset/cls_small_data.py'>"]},"metadata":{},"execution_count":88}],"source":["# This section is used to reload the imported module.\n","# For example, if you made any changes in the model.cls_model, you should run importlib.reload(Cmodel) as long as you set import model.cls_model as Cmodel.\n","import importlib\n","importlib.reload(Rdata)\n","importlib.reload(Cdata)"]},{"cell_type":"code","source":["#for reloading config file, in case you modified it for experimenting\n","conf_file = OmegaConf.load(os.path.join(folder_name, 'config.yaml'))\n","cfg = conf_file['dataset'][dataset_name]"],"metadata":{"id":"cNPWBqvve8v6","executionInfo":{"status":"ok","timestamp":1718886587753,"user_tz":-480,"elapsed":1960,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["cfg"],"metadata":{"id":"t9fvmf2QztcF","executionInfo":{"status":"ok","timestamp":1718886589216,"user_tz":-480,"elapsed":2,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"0551d520-5b31-45ef-bcb4-2169db9c0d96","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ca_weight_sharing': True, 'top_case_enabled': True, 'training_epochs': 1000, 'learning_rate': 0.001, 'batch_size': 100, 'top_k': 5, 'class_weight_sharing': True, 'patience': 40, 'discount': 180}"]},"metadata":{},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"EOEnGkKnmG4s"},"source":["# Classification with NNKNN"]},{"cell_type":"code","execution_count":116,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2783,"status":"ok","timestamp":1718886598035,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"fisqdp27w9fg","outputId":"f32769f4-27b5-4e5d-a46b-69a489d7f00c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique values: [0 1 2 3]\n","Counts: [776 776 776 776]\n","Xs.size(): torch.Size([3104, 161])\n"]}],"source":["# prompt: get the unique y values and their counts\n","\n","unique_values, counts = np.unique(ys, return_counts=True)\n","print(f\"Unique values: {unique_values}\")\n","print(f\"Counts: {counts}\")\n","print(f\"Xs.size(): {Xs.size()}\")\n"]},{"cell_type":"code","execution_count":117,"metadata":{"id":"xOraZstjmG4t","executionInfo":{"status":"ok","timestamp":1718886598732,"user_tz":-480,"elapsed":5,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def train_cls(X_train,y_train, X_test, y_test, cfg:DictConfig):\n","  X_train = X_train.to(device)\n","  y_train = y_train.to(device)\n","  X_test = X_test.to(device)\n","\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=cfg.batch_size, shuffle=True)\n","\n","  # Train model\n","  model = Cmodel.NN_k_NN(X_train,\n","                         y_train,\n","                         cfg.ca_weight_sharing,\n","                         cfg.top_case_enabled,\n","                         cfg.top_k,\n","                         cfg.discount,\n","                         device=device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate) #, weight_decay=1e-5)\n","\n","  patience_counter = 0\n","  for epoch in range(cfg.training_epochs):\n","    epoch_msg = True\n","\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      _, _, output, predicted_class = model(X_train_batch)\n","      loss = criterion(output, y_train_batch)\n","\n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      if epoch_msg and (epoch + 1) % 2 == 0:\n","        print(f'Epoch [{epoch + 1}/{cfg.training_epochs}], Loss: {loss.item():.4f}')\n","\n","        epoch_msg = False\n","      # print(\"evaluating\")\n","    model.eval()\n","    with torch.no_grad():\n","      _, _, output, predicted_class = model(X_test)\n","\n","      # Calculate accuracy\n","      accuracy_temp = accuracy_score(y_test.numpy(), predicted_class.cpu().numpy())\n","    if epoch == 0:\n","      best_accuracy = accuracy_temp\n","      torch.save(model.state_dict(), cfg.PATH)\n","\n","    elif accuracy_temp > best_accuracy:\n","      #memorize best model\n","      torch.save(model.state_dict(), cfg.PATH)\n","      best_accuracy = accuracy_temp\n","      patience_counter = 0\n","\n","    elif patience_counter > cfg.patience:\n","      model.eval()\n","      print(\"patience exceeded, loading best model\")\n","      break\n","    else:\n","      patience_counter += 1\n","\n","  return best_accuracy, model"]},{"cell_type":"code","execution_count":118,"metadata":{"id":"DMKsNhL3ItkK","executionInfo":{"status":"ok","timestamp":1718886598733,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def load_model_cls(X_train,y_train,cfg):\n","  # Define the model architecture\n","  model = Cmodel.NN_k_NN(\n","      X_train,\n","      y_train,\n","      cfg.ca_weight_sharing,\n","      cfg.top_case_enabled,\n","      cfg.top_k,\n","      cfg.discount,\n","      device=device\n","  )\n","  # Load the state dictionary\n","  model.load_state_dict(torch.load(cfg.path))\n","  model.to(device)\n","  model.eval()\n","  return model"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21846,"status":"ok","timestamp":1718886620574,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"qPIxfcO-mG4t","outputId":"260bb9ba-6e9f-4904-df81-ea7bd0153853"},"outputs":[{"output_type":"stream","name":"stdout","text":["knn_acc:  0.3762057877813505\n","Epoch [2/1000], Loss: 1.3861\n","Epoch [4/1000], Loss: 1.3861\n","Epoch [6/1000], Loss: 1.3865\n","Epoch [8/1000], Loss: 1.3868\n","Epoch [10/1000], Loss: 1.3866\n","Epoch [12/1000], Loss: 1.3864\n","Epoch [14/1000], Loss: 1.3854\n","Epoch [16/1000], Loss: 1.3847\n","Epoch [18/1000], Loss: 1.3877\n","Epoch [20/1000], Loss: 1.3862\n","Epoch [22/1000], Loss: 1.3858\n","Epoch [24/1000], Loss: 1.3871\n","Epoch [26/1000], Loss: 1.3879\n","Epoch [28/1000], Loss: 1.3845\n","Epoch [30/1000], Loss: 1.3857\n","Epoch [32/1000], Loss: 1.3838\n","Epoch [34/1000], Loss: 1.3844\n","Epoch [36/1000], Loss: 1.3856\n","Epoch [38/1000], Loss: 1.3869\n","Epoch [40/1000], Loss: 1.3872\n","Epoch [42/1000], Loss: 1.3867\n","patience exceeded, loading best model\n","nnknn acc:  0.2057877813504823\n","Average accuracy:0.206\n","KNN accuracy:0.376\n","LMNN/NCA accuracy:nan\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]}],"source":["accuracies = []\n","knn_accuracies = []\n","lmnn_accuracies = []\n","PATH = os.path.join(folder_name, f'checkpoints/classifier_{dataset_name}.h5')\n","cfg.PATH = PATH\n","k_fold = KFold(n_splits=10, shuffle=True, random_state = None)\n","enable_lmnn = False\n","\n","for train_index, test_index in k_fold.split(Xs):\n","  # Get training and testing data\n","  X_train, X_test = Xs[train_index], Xs[test_index]\n","  y_train, y_test = ys[train_index], ys[test_index]\n","  if(enable_lmnn):\n","    # https://contrib.scikit-learn.org/metric-learn/supervised.html#lmnn\n","    lmnn = LMNN(n_neighbors=5, learn_rate=1e-6)\n","    ##TODO, change here if you need to use a different one\n","    # lmnn = metric_learn.MLKR()\n","    # lmnn = metric_learn.NCA(max_iter=1000)\n","    lmnn.fit(X_train,y_train)\n","    knn = KNeighborsClassifier(n_neighbors=5,metric=lmnn.get_metric())\n","    knn.fit(X_train,y_train)\n","    # klmnn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n","    lmnn_acc = accuracy_score(knn.predict(X_test), y_test)\n","    lmnn_accuracies.append(lmnn_acc)\n","    print(\"lmnn_acc: \",lmnn_acc)\n","    # continue\n","\n","  knn =  KNeighborsClassifier(n_neighbors=cfg.top_k)\n","  knn.fit(X_train, y_train)\n","  knn_acc  = accuracy_score(knn.predict(X_test), y_test)\n","  knn_accuracies.append(knn_acc)\n","  print(\"knn_acc: \", knn_acc)\n","\n","  best_accuracy, model = train_cls(X_train,y_train, X_test, y_test, cfg)\n","  accuracies.append(best_accuracy)\n","  print(\"nnknn acc: \", best_accuracy)\n","  break\n","\n","print(f\"Average accuracy:{np.mean(accuracies):.3f}\")\n","print(f\"KNN accuracy:{np.mean(knn_accuracies):.3f}\")\n","print(f\"LMNN/NCA accuracy:{np.mean(lmnn_accuracies):.3f}\")\n"]},{"cell_type":"code","source":["print(f\"Average accuracy:{np.mean(accuracies):.3f}\")\n","print(f\"KNN accuracy:{np.mean(knn_accuracies):.3f}\")\n","print(f\"LMNN/NCA accuracy:{np.mean(lmnn_accuracies):.3f}\")"],"metadata":{"id":"hmEeDk1RXAwp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718886620574,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"2d3c9786-13fa-44a2-e92b-573f18ed8235"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy:0.206\n","KNN accuracy:0.376\n","LMNN/NCA accuracy:nan\n"]}]},{"cell_type":"markdown","metadata":{"id":"QVO3SDp9mG4u"},"source":["# Regression with NNKNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1060,"status":"ok","timestamp":1718857161398,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"x2uuoXv9ceJL","outputId":"64d5ad8a-9d96-435d-ff17-fa2c7a66f050"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique values: [-1.341415   -0.44713837  0.44713837  1.341415  ]\n","Counts: [743 743 743 743]\n","Xs.size(): torch.Size([2972, 157])\n"]}],"source":["unique_values, counts = np.unique(ys, return_counts=True)\n","print(f\"Unique values: {unique_values}\")\n","print(f\"Counts: {counts}\")\n","print(f\"Xs.size(): {Xs.size()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ll4n1HNmG4u"},"outputs":[],"source":["def train_reg(X_train,y_train, X_test, y_test, cfg:DictConfig):\n","  X_train = X_train.to(device)\n","  y_train = y_train.to(device)\n","  X_test = X_test.to(device)\n","\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=cfg.batch_size, shuffle=True)\n","  test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=cfg.batch_size, shuffle=False)\n","\n","\n","    # Train model\n","  model = Rmodel.NN_k_NN_regression(X_train,\n","                                    y_train,\n","                                    cfg.ca_weight_sharing,\n","                                    cfg.top_case_enabled,\n","                                    cfg.top_k,\n","                                    cfg.discount,\n","                                    cfg.class_weight_sharing,\n","                                    device=device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate) #, weight_decay=1e-5)\n","\n","  patience_counter = 0\n","  for epoch in range(cfg.training_epochs):\n","    # break # no training\n","    epoch_msg = True\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      _, _, _, predicted_number = model(X_train_batch)\n","      # break\n","      loss = criterion(predicted_number.squeeze(), y_train_batch)\n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      if epoch_msg and (epoch + 1) % 2 == 0:\n","        epoch_msg = False\n","        print(f'Epoch [{epoch + 1}/{cfg.training_epochs}], Loss: {loss.item():.4f}')\n","\n","    model.eval()\n","    with torch.no_grad():\n","      predicted_numbers = []\n","      for X_test_batch, _ in test_loader:\n","        X_test_batch = X_test_batch.to(device)\n","        _, _, _, predicted_number = model(X_test_batch)\n","        predicted_numbers.extend(predicted_number.squeeze().cpu().detach())\n","\n","      predicted_numbers = torch.Tensor(predicted_numbers)\n","      accuracy_temp = criterion(y_test, predicted_numbers)\n","\n","    if epoch == 0:\n","      best_accuracy = accuracy_temp\n","      torch.save(model.state_dict(), cfg.PATH)\n","    elif accuracy_temp < best_accuracy:\n","      torch.save(model.state_dict(), cfg.PATH)\n","      best_accuracy = accuracy_temp\n","      patience_counter = 0\n","    elif patience_counter > cfg.patience:\n","      model.eval()\n","      print(\"patience exceeded, loading best model\")\n","      break\n","    else:\n","      patience_counter += 1\n","\n","  _, case_activations, topk_avg, predicted_number = model(X_test)\n","\n","  top_case_indices = torch.topk(case_activations, 5, dim=1)[1].cpu()\n","\n","  accuracy = criterion(y_test, predicted_number.squeeze().cpu())\n","  y_train = y_train.cpu()\n","  top_k_average_accuracy = mean_squared_error(torch.mean(y_train[top_case_indices], dim=1), y_test)\n","  # top_k_average_accuracy = mean_squared_error(topk_avg.cpu(), y_test)\n","  return best_accuracy, accuracy, top_k_average_accuracy, model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yCDMIuMRN38"},"outputs":[],"source":["# prompt: load_model_reg()\n","\n","def load_model_reg(X_train,y_train,cfg):\n","  # Define the model architecture\n","  model = Rmodel.NN_k_NN_regression(\n","      X_train,\n","      y_train,\n","      cfg.ca_weight_sharing,\n","      cfg.top_case_enabled,\n","      cfg.top_k,\n","      cfg.discount,\n","      cfg.class_weight_sharing,\n","      device=device\n","  )\n","  # Load the state dictionary\n","  model.load_state_dict(torch.load(cfg.path))\n","  model.to(device)\n","  model.eval()\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DN8UlUVpmG4v","executionInfo":{"status":"ok","timestamp":1718860395427,"user_tz":-480,"elapsed":3233393,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"e1cd6aab-7cd0-4e9a-b8de-e70498945ddb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [2/1000], Loss: 0.9939\n","Epoch [4/1000], Loss: 0.8766\n","Epoch [6/1000], Loss: 1.1007\n","Epoch [8/1000], Loss: 0.9243\n","Epoch [10/1000], Loss: 0.8962\n","Epoch [12/1000], Loss: 0.9663\n","Epoch [14/1000], Loss: 0.9343\n","Epoch [16/1000], Loss: 0.7975\n","Epoch [18/1000], Loss: 1.1810\n","Epoch [20/1000], Loss: 0.9395\n","Epoch [22/1000], Loss: 0.6575\n","Epoch [24/1000], Loss: 0.7441\n","Epoch [26/1000], Loss: 1.0825\n","Epoch [28/1000], Loss: 0.8471\n","Epoch [30/1000], Loss: 1.2029\n","Epoch [32/1000], Loss: 0.9471\n","Epoch [34/1000], Loss: 0.9496\n","Epoch [36/1000], Loss: 1.1201\n","Epoch [38/1000], Loss: 1.0395\n","Epoch [40/1000], Loss: 0.7947\n","Epoch [42/1000], Loss: 0.9133\n","Epoch [44/1000], Loss: 1.0327\n","Epoch [46/1000], Loss: 1.0092\n","Epoch [48/1000], Loss: 0.9860\n","Epoch [50/1000], Loss: 0.6595\n","Epoch [52/1000], Loss: 0.9183\n","Epoch [54/1000], Loss: 0.9224\n","Epoch [56/1000], Loss: 0.9680\n","Epoch [58/1000], Loss: 0.8421\n","Epoch [60/1000], Loss: 1.0980\n","Epoch [62/1000], Loss: 0.8510\n","Epoch [64/1000], Loss: 1.1691\n","Epoch [66/1000], Loss: 1.1269\n","Epoch [68/1000], Loss: 0.9277\n","Epoch [70/1000], Loss: 1.0959\n","Epoch [72/1000], Loss: 1.1242\n","Epoch [74/1000], Loss: 0.8663\n","Epoch [76/1000], Loss: 0.9227\n","Epoch [78/1000], Loss: 1.0840\n","Epoch [80/1000], Loss: 0.9900\n","Epoch [82/1000], Loss: 0.9782\n","Epoch [84/1000], Loss: 0.9452\n","Epoch [86/1000], Loss: 1.0353\n","Epoch [88/1000], Loss: 1.2399\n","Epoch [90/1000], Loss: 0.7757\n","Epoch [92/1000], Loss: 0.7447\n","Epoch [94/1000], Loss: 0.8317\n","Epoch [96/1000], Loss: 1.0283\n","Epoch [98/1000], Loss: 1.1393\n","Epoch [100/1000], Loss: 0.9880\n","Epoch [102/1000], Loss: 1.1265\n","Epoch [104/1000], Loss: 0.9902\n","Epoch [106/1000], Loss: 1.2131\n","Epoch [108/1000], Loss: 1.0628\n","Epoch [110/1000], Loss: 1.1088\n","Epoch [112/1000], Loss: 0.8527\n","Epoch [114/1000], Loss: 0.8342\n","Epoch [116/1000], Loss: 0.7881\n","Epoch [118/1000], Loss: 1.0562\n","Epoch [120/1000], Loss: 0.9159\n","Epoch [122/1000], Loss: 1.1497\n","Epoch [124/1000], Loss: 1.1355\n","Epoch [126/1000], Loss: 0.9068\n","Epoch [128/1000], Loss: 0.9959\n","Epoch [130/1000], Loss: 0.9885\n","Epoch [132/1000], Loss: 0.8210\n","Epoch [134/1000], Loss: 0.8201\n","Epoch [136/1000], Loss: 1.0487\n","Epoch [138/1000], Loss: 0.9371\n","Epoch [140/1000], Loss: 1.0898\n","Epoch [142/1000], Loss: 0.7340\n","Epoch [144/1000], Loss: 1.4339\n","Epoch [146/1000], Loss: 0.8275\n","Epoch [148/1000], Loss: 0.8735\n","Epoch [150/1000], Loss: 1.0451\n","Epoch [152/1000], Loss: 1.1482\n","Epoch [154/1000], Loss: 0.9894\n","Epoch [156/1000], Loss: 1.2729\n","Epoch [158/1000], Loss: 1.1470\n","Epoch [160/1000], Loss: 0.9089\n","Epoch [162/1000], Loss: 0.9995\n","Epoch [164/1000], Loss: 0.9573\n","Epoch [166/1000], Loss: 0.7693\n","Epoch [168/1000], Loss: 1.2273\n","Epoch [170/1000], Loss: 0.8777\n","Epoch [172/1000], Loss: 0.8908\n","Epoch [174/1000], Loss: 1.1045\n","Epoch [176/1000], Loss: 0.8154\n","Epoch [178/1000], Loss: 0.7992\n","Epoch [180/1000], Loss: 1.0119\n","Epoch [182/1000], Loss: 1.0867\n","Epoch [184/1000], Loss: 1.0470\n","Epoch [186/1000], Loss: 0.7571\n","Epoch [188/1000], Loss: 1.0472\n","Epoch [190/1000], Loss: 0.7755\n","Epoch [192/1000], Loss: 0.8107\n","Epoch [194/1000], Loss: 0.8969\n","Epoch [196/1000], Loss: 0.9150\n","Epoch [198/1000], Loss: 0.9171\n","Epoch [200/1000], Loss: 0.9631\n","Epoch [202/1000], Loss: 1.0153\n","Epoch [204/1000], Loss: 0.8665\n","Epoch [206/1000], Loss: 1.0305\n","Epoch [208/1000], Loss: 0.8532\n","Epoch [210/1000], Loss: 0.9857\n","Epoch [212/1000], Loss: 0.9728\n","Epoch [214/1000], Loss: 0.9005\n","Epoch [216/1000], Loss: 0.8114\n","Epoch [218/1000], Loss: 0.8134\n","Epoch [220/1000], Loss: 1.0791\n","Epoch [222/1000], Loss: 0.8648\n","Epoch [224/1000], Loss: 0.9259\n","Epoch [226/1000], Loss: 0.7650\n","Epoch [228/1000], Loss: 0.8188\n","Epoch [230/1000], Loss: 0.8553\n","Epoch [232/1000], Loss: 0.9055\n","Epoch [234/1000], Loss: 0.8026\n","Epoch [236/1000], Loss: 1.0990\n","Epoch [238/1000], Loss: 0.9196\n","Epoch [240/1000], Loss: 1.0442\n","Epoch [242/1000], Loss: 1.0274\n","Epoch [244/1000], Loss: 1.0009\n","Epoch [246/1000], Loss: 0.7814\n","Epoch [248/1000], Loss: 0.7147\n","Epoch [250/1000], Loss: 0.9085\n","Epoch [252/1000], Loss: 0.8988\n","Epoch [254/1000], Loss: 1.0109\n","Epoch [256/1000], Loss: 0.9406\n","Epoch [258/1000], Loss: 0.5403\n","Epoch [260/1000], Loss: 0.8584\n","Epoch [262/1000], Loss: 0.9178\n","Epoch [264/1000], Loss: 0.8058\n","Epoch [266/1000], Loss: 1.1177\n","Epoch [268/1000], Loss: 0.8059\n","Epoch [270/1000], Loss: 1.0252\n","Epoch [272/1000], Loss: 0.7373\n","Epoch [274/1000], Loss: 1.2574\n","Epoch [276/1000], Loss: 0.5115\n","Epoch [278/1000], Loss: 0.8399\n","Epoch [280/1000], Loss: 0.8867\n","Epoch [282/1000], Loss: 0.9144\n","Epoch [284/1000], Loss: 0.9549\n","Epoch [286/1000], Loss: 0.7422\n","Epoch [288/1000], Loss: 0.8608\n","Epoch [290/1000], Loss: 0.7659\n","Epoch [292/1000], Loss: 0.9481\n","Epoch [294/1000], Loss: 1.0554\n","Epoch [296/1000], Loss: 0.8263\n","Epoch [298/1000], Loss: 0.8958\n","Epoch [300/1000], Loss: 0.8791\n","Epoch [302/1000], Loss: 0.8494\n","Epoch [304/1000], Loss: 0.9170\n","Epoch [306/1000], Loss: 0.9278\n","Epoch [308/1000], Loss: 0.8748\n","Epoch [310/1000], Loss: 0.7849\n","Epoch [312/1000], Loss: 1.0039\n","Epoch [314/1000], Loss: 0.6777\n","Epoch [316/1000], Loss: 0.8176\n","Epoch [318/1000], Loss: 0.9861\n","Epoch [320/1000], Loss: 1.0294\n","Epoch [322/1000], Loss: 0.8058\n","Epoch [324/1000], Loss: 1.1348\n","Epoch [326/1000], Loss: 0.8314\n","Epoch [328/1000], Loss: 0.8366\n","Epoch [330/1000], Loss: 0.8153\n","Epoch [332/1000], Loss: 0.8903\n","Epoch [334/1000], Loss: 0.8464\n","Epoch [336/1000], Loss: 1.2050\n","Epoch [338/1000], Loss: 0.9591\n","Epoch [340/1000], Loss: 0.7823\n","Epoch [342/1000], Loss: 0.9189\n","Epoch [344/1000], Loss: 0.7978\n","Epoch [346/1000], Loss: 0.8652\n","Epoch [348/1000], Loss: 0.7996\n","Epoch [350/1000], Loss: 1.2052\n","Epoch [352/1000], Loss: 0.9444\n","Epoch [354/1000], Loss: 0.9620\n","Epoch [356/1000], Loss: 0.9492\n","Epoch [358/1000], Loss: 0.8332\n","Epoch [360/1000], Loss: 0.6574\n","Epoch [362/1000], Loss: 0.9323\n","Epoch [364/1000], Loss: 0.8779\n","Epoch [366/1000], Loss: 1.0530\n","Epoch [368/1000], Loss: 0.8652\n","Epoch [370/1000], Loss: 0.8125\n","Epoch [372/1000], Loss: 0.7750\n","Epoch [374/1000], Loss: 1.0174\n","Epoch [376/1000], Loss: 0.8882\n","Epoch [378/1000], Loss: 0.9438\n","Epoch [380/1000], Loss: 1.1525\n","Epoch [382/1000], Loss: 0.9509\n","Epoch [384/1000], Loss: 1.0891\n","Epoch [386/1000], Loss: 0.6962\n","Epoch [388/1000], Loss: 0.9784\n","Epoch [390/1000], Loss: 0.9549\n","Epoch [392/1000], Loss: 0.9116\n","Epoch [394/1000], Loss: 0.8512\n","Epoch [396/1000], Loss: 0.8306\n","Epoch [398/1000], Loss: 0.9662\n","Epoch [400/1000], Loss: 0.9032\n","Epoch [402/1000], Loss: 0.8356\n","Epoch [404/1000], Loss: 0.7760\n","Epoch [406/1000], Loss: 1.0099\n","Epoch [408/1000], Loss: 0.7324\n","Epoch [410/1000], Loss: 0.8532\n","Epoch [412/1000], Loss: 0.8721\n","Epoch [414/1000], Loss: 0.6072\n","Epoch [416/1000], Loss: 0.8116\n","Epoch [418/1000], Loss: 1.0705\n","Epoch [420/1000], Loss: 0.9676\n","Epoch [422/1000], Loss: 0.9500\n","Epoch [424/1000], Loss: 1.0552\n","Epoch [426/1000], Loss: 0.7989\n","Epoch [428/1000], Loss: 0.8349\n","Epoch [430/1000], Loss: 0.7609\n","Epoch [432/1000], Loss: 0.8522\n","Epoch [434/1000], Loss: 0.8965\n","Epoch [436/1000], Loss: 1.1222\n","Epoch [438/1000], Loss: 0.9271\n","Epoch [440/1000], Loss: 0.9624\n","Epoch [442/1000], Loss: 0.7357\n","Epoch [444/1000], Loss: 0.8898\n","Epoch [446/1000], Loss: 0.7181\n","Epoch [448/1000], Loss: 0.8620\n","Epoch [450/1000], Loss: 0.9629\n","Epoch [452/1000], Loss: 0.7545\n","Epoch [454/1000], Loss: 0.9928\n","Epoch [456/1000], Loss: 0.6911\n","Epoch [458/1000], Loss: 0.8052\n","Epoch [460/1000], Loss: 0.7398\n","Epoch [462/1000], Loss: 0.6879\n","Epoch [464/1000], Loss: 0.7577\n","Epoch [466/1000], Loss: 0.9580\n","Epoch [468/1000], Loss: 0.9359\n","Epoch [470/1000], Loss: 0.6383\n","Epoch [472/1000], Loss: 0.7993\n","Epoch [474/1000], Loss: 0.8576\n","Epoch [476/1000], Loss: 0.7901\n","Epoch [478/1000], Loss: 0.9246\n","Epoch [480/1000], Loss: 0.6962\n","Epoch [482/1000], Loss: 1.0038\n","Epoch [484/1000], Loss: 0.9653\n","Epoch [486/1000], Loss: 0.7515\n","Epoch [488/1000], Loss: 0.6550\n","Epoch [490/1000], Loss: 0.7944\n","Epoch [492/1000], Loss: 0.8837\n","Epoch [494/1000], Loss: 0.9289\n","Epoch [496/1000], Loss: 0.9551\n","Epoch [498/1000], Loss: 0.8927\n","Epoch [500/1000], Loss: 0.6787\n","Epoch [502/1000], Loss: 0.6149\n","Epoch [504/1000], Loss: 1.0635\n","Epoch [506/1000], Loss: 0.8441\n","Epoch [508/1000], Loss: 0.9722\n","Epoch [510/1000], Loss: 0.9701\n","Epoch [512/1000], Loss: 1.1389\n","Epoch [514/1000], Loss: 0.8485\n","Epoch [516/1000], Loss: 0.9649\n","Epoch [518/1000], Loss: 0.8095\n","Epoch [520/1000], Loss: 0.7813\n","Epoch [522/1000], Loss: 0.8059\n","Epoch [524/1000], Loss: 0.5608\n","Epoch [526/1000], Loss: 0.7742\n","Epoch [528/1000], Loss: 0.7084\n","Epoch [530/1000], Loss: 0.8212\n","Epoch [532/1000], Loss: 0.8821\n","Epoch [534/1000], Loss: 0.5945\n","Epoch [536/1000], Loss: 0.7295\n","Epoch [538/1000], Loss: 0.7799\n","Epoch [540/1000], Loss: 0.7869\n","Epoch [542/1000], Loss: 0.7892\n","Epoch [544/1000], Loss: 0.9936\n","Epoch [546/1000], Loss: 0.5245\n","Epoch [548/1000], Loss: 0.7380\n","Epoch [550/1000], Loss: 0.8230\n","Epoch [552/1000], Loss: 0.9040\n","Epoch [554/1000], Loss: 0.6927\n","Epoch [556/1000], Loss: 0.8508\n","Epoch [558/1000], Loss: 0.8471\n","Epoch [560/1000], Loss: 0.6095\n","Epoch [562/1000], Loss: 0.8108\n","Epoch [564/1000], Loss: 0.4252\n","Epoch [566/1000], Loss: 0.8819\n","Epoch [568/1000], Loss: 0.8933\n","Epoch [570/1000], Loss: 0.8833\n","Epoch [572/1000], Loss: 0.6498\n","Epoch [574/1000], Loss: 0.8768\n","Epoch [576/1000], Loss: 0.8910\n","Epoch [578/1000], Loss: 0.4992\n","Epoch [580/1000], Loss: 0.8296\n","Epoch [582/1000], Loss: 0.8665\n","Epoch [584/1000], Loss: 0.9552\n","Epoch [586/1000], Loss: 0.8064\n","Epoch [588/1000], Loss: 0.7892\n","Epoch [590/1000], Loss: 0.9190\n","Epoch [592/1000], Loss: 1.3458\n","Epoch [594/1000], Loss: 0.7331\n","Epoch [596/1000], Loss: 1.1115\n","Epoch [598/1000], Loss: 1.0465\n","Epoch [600/1000], Loss: 0.6063\n","Epoch [602/1000], Loss: 1.0771\n","Epoch [604/1000], Loss: 0.8049\n","Epoch [606/1000], Loss: 0.9544\n","Epoch [608/1000], Loss: 0.5764\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.2414\n","Epoch [4/1000], Loss: 0.7559\n","Epoch [6/1000], Loss: 1.1713\n","Epoch [8/1000], Loss: 1.0682\n","Epoch [10/1000], Loss: 0.9287\n","Epoch [12/1000], Loss: 0.8316\n","Epoch [14/1000], Loss: 0.8109\n","Epoch [16/1000], Loss: 1.0400\n","Epoch [18/1000], Loss: 1.0088\n","Epoch [20/1000], Loss: 1.0838\n","Epoch [22/1000], Loss: 0.9723\n","Epoch [24/1000], Loss: 1.0129\n","Epoch [26/1000], Loss: 1.0793\n","Epoch [28/1000], Loss: 1.0876\n","Epoch [30/1000], Loss: 1.1078\n","Epoch [32/1000], Loss: 1.2538\n","Epoch [34/1000], Loss: 0.8705\n","Epoch [36/1000], Loss: 0.9080\n","Epoch [38/1000], Loss: 1.0969\n","Epoch [40/1000], Loss: 1.1509\n","Epoch [42/1000], Loss: 0.9285\n","Epoch [44/1000], Loss: 0.8930\n","Epoch [46/1000], Loss: 1.1003\n","Epoch [48/1000], Loss: 1.0118\n","Epoch [50/1000], Loss: 0.7753\n","Epoch [52/1000], Loss: 1.2936\n","Epoch [54/1000], Loss: 1.1270\n","Epoch [56/1000], Loss: 0.8894\n","Epoch [58/1000], Loss: 1.1208\n","Epoch [60/1000], Loss: 0.9762\n","Epoch [62/1000], Loss: 0.8900\n","Epoch [64/1000], Loss: 0.9697\n","Epoch [66/1000], Loss: 0.9465\n","Epoch [68/1000], Loss: 0.9332\n","Epoch [70/1000], Loss: 0.9545\n","Epoch [72/1000], Loss: 0.9765\n","Epoch [74/1000], Loss: 0.9792\n","Epoch [76/1000], Loss: 0.8425\n","Epoch [78/1000], Loss: 0.7312\n","Epoch [80/1000], Loss: 1.1079\n","Epoch [82/1000], Loss: 1.0078\n","Epoch [84/1000], Loss: 1.0300\n","Epoch [86/1000], Loss: 1.2076\n","Epoch [88/1000], Loss: 1.1442\n","Epoch [90/1000], Loss: 1.0238\n","Epoch [92/1000], Loss: 0.9453\n","Epoch [94/1000], Loss: 0.9154\n","Epoch [96/1000], Loss: 0.7262\n","Epoch [98/1000], Loss: 0.9420\n","Epoch [100/1000], Loss: 0.9891\n","Epoch [102/1000], Loss: 1.0687\n","Epoch [104/1000], Loss: 0.9262\n","Epoch [106/1000], Loss: 1.1549\n","Epoch [108/1000], Loss: 0.7585\n","Epoch [110/1000], Loss: 1.1236\n","Epoch [112/1000], Loss: 1.1705\n","Epoch [114/1000], Loss: 0.8813\n","Epoch [116/1000], Loss: 1.1707\n","Epoch [118/1000], Loss: 0.9093\n","Epoch [120/1000], Loss: 1.0297\n","Epoch [122/1000], Loss: 1.0273\n","Epoch [124/1000], Loss: 1.1791\n","Epoch [126/1000], Loss: 0.6142\n","Epoch [128/1000], Loss: 0.8605\n","Epoch [130/1000], Loss: 1.0035\n","Epoch [132/1000], Loss: 1.0364\n","Epoch [134/1000], Loss: 0.8533\n","Epoch [136/1000], Loss: 0.8551\n","Epoch [138/1000], Loss: 1.0039\n","Epoch [140/1000], Loss: 1.0091\n","Epoch [142/1000], Loss: 1.0005\n","Epoch [144/1000], Loss: 1.1649\n","Epoch [146/1000], Loss: 1.0992\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 0.9399\n","Epoch [4/1000], Loss: 1.3237\n","Epoch [6/1000], Loss: 0.7731\n","Epoch [8/1000], Loss: 1.0325\n","Epoch [10/1000], Loss: 0.9141\n","Epoch [12/1000], Loss: 1.1913\n","Epoch [14/1000], Loss: 1.0698\n","Epoch [16/1000], Loss: 1.0836\n","Epoch [18/1000], Loss: 1.3245\n","Epoch [20/1000], Loss: 1.0264\n","Epoch [22/1000], Loss: 1.0802\n","Epoch [24/1000], Loss: 0.8578\n","Epoch [26/1000], Loss: 1.0669\n","Epoch [28/1000], Loss: 1.1734\n","Epoch [30/1000], Loss: 1.0122\n","Epoch [32/1000], Loss: 0.9071\n","Epoch [34/1000], Loss: 1.3363\n","Epoch [36/1000], Loss: 0.9138\n","Epoch [38/1000], Loss: 1.1887\n","Epoch [40/1000], Loss: 0.9993\n","Epoch [42/1000], Loss: 0.9872\n","Epoch [44/1000], Loss: 0.8764\n","Epoch [46/1000], Loss: 0.8331\n","Epoch [48/1000], Loss: 1.1187\n","Epoch [50/1000], Loss: 0.6648\n","Epoch [52/1000], Loss: 1.1554\n","Epoch [54/1000], Loss: 1.1797\n","Epoch [56/1000], Loss: 1.0519\n","Epoch [58/1000], Loss: 0.8576\n","Epoch [60/1000], Loss: 0.8913\n","Epoch [62/1000], Loss: 0.9941\n","Epoch [64/1000], Loss: 0.8824\n","Epoch [66/1000], Loss: 1.0918\n","Epoch [68/1000], Loss: 1.2390\n","Epoch [70/1000], Loss: 1.0340\n","Epoch [72/1000], Loss: 0.6964\n","Epoch [74/1000], Loss: 0.9799\n","Epoch [76/1000], Loss: 0.9837\n","Epoch [78/1000], Loss: 0.9074\n","Epoch [80/1000], Loss: 0.5585\n","Epoch [82/1000], Loss: 1.0159\n","Epoch [84/1000], Loss: 0.8727\n","Epoch [86/1000], Loss: 0.9665\n","Epoch [88/1000], Loss: 0.8924\n","Epoch [90/1000], Loss: 1.2008\n","Epoch [92/1000], Loss: 1.0064\n","Epoch [94/1000], Loss: 1.1531\n","Epoch [96/1000], Loss: 0.7651\n","Epoch [98/1000], Loss: 0.7501\n","Epoch [100/1000], Loss: 1.0872\n","Epoch [102/1000], Loss: 1.1898\n","Epoch [104/1000], Loss: 1.1760\n","Epoch [106/1000], Loss: 0.7498\n","Epoch [108/1000], Loss: 0.8245\n","Epoch [110/1000], Loss: 1.2418\n","Epoch [112/1000], Loss: 0.9423\n","Epoch [114/1000], Loss: 1.1065\n","Epoch [116/1000], Loss: 0.8535\n","Epoch [118/1000], Loss: 0.6173\n","Epoch [120/1000], Loss: 0.9847\n","Epoch [122/1000], Loss: 0.9730\n","Epoch [124/1000], Loss: 1.0238\n","Epoch [126/1000], Loss: 1.0643\n","Epoch [128/1000], Loss: 1.1486\n","Epoch [130/1000], Loss: 0.8868\n","Epoch [132/1000], Loss: 0.8661\n","Epoch [134/1000], Loss: 1.0974\n","Epoch [136/1000], Loss: 0.8863\n","Epoch [138/1000], Loss: 0.8332\n","Epoch [140/1000], Loss: 0.8973\n","Epoch [142/1000], Loss: 0.9775\n","Epoch [144/1000], Loss: 1.2041\n","Epoch [146/1000], Loss: 1.0157\n","Epoch [148/1000], Loss: 1.0742\n","Epoch [150/1000], Loss: 0.7278\n","Epoch [152/1000], Loss: 1.2075\n","Epoch [154/1000], Loss: 0.9759\n","Epoch [156/1000], Loss: 0.8290\n","Epoch [158/1000], Loss: 0.8450\n","Epoch [160/1000], Loss: 0.9950\n","Epoch [162/1000], Loss: 0.8758\n","Epoch [164/1000], Loss: 1.0773\n","Epoch [166/1000], Loss: 1.0342\n","Epoch [168/1000], Loss: 1.0172\n","Epoch [170/1000], Loss: 0.9225\n","Epoch [172/1000], Loss: 1.0812\n","Epoch [174/1000], Loss: 0.8208\n","Epoch [176/1000], Loss: 1.1047\n","Epoch [178/1000], Loss: 0.8636\n","Epoch [180/1000], Loss: 1.1831\n","Epoch [182/1000], Loss: 0.8609\n","Epoch [184/1000], Loss: 1.0047\n","Epoch [186/1000], Loss: 1.0706\n","Epoch [188/1000], Loss: 1.4239\n","Epoch [190/1000], Loss: 0.9620\n","Epoch [192/1000], Loss: 0.9843\n","Epoch [194/1000], Loss: 0.9709\n","Epoch [196/1000], Loss: 0.6636\n","Epoch [198/1000], Loss: 0.9109\n","Epoch [200/1000], Loss: 0.9766\n","Epoch [202/1000], Loss: 0.8067\n","Epoch [204/1000], Loss: 1.2082\n","Epoch [206/1000], Loss: 0.9648\n","Epoch [208/1000], Loss: 0.8074\n","Epoch [210/1000], Loss: 0.9878\n","Epoch [212/1000], Loss: 1.0666\n","Epoch [214/1000], Loss: 0.9624\n","Epoch [216/1000], Loss: 1.0765\n","Epoch [218/1000], Loss: 1.0519\n","Epoch [220/1000], Loss: 0.8222\n","Epoch [222/1000], Loss: 0.9367\n","Epoch [224/1000], Loss: 1.3012\n","Epoch [226/1000], Loss: 0.7465\n","Epoch [228/1000], Loss: 1.0623\n","Epoch [230/1000], Loss: 0.8863\n","Epoch [232/1000], Loss: 0.8849\n","Epoch [234/1000], Loss: 1.0872\n","Epoch [236/1000], Loss: 0.8863\n","Epoch [238/1000], Loss: 0.9144\n","Epoch [240/1000], Loss: 0.7163\n","Epoch [242/1000], Loss: 1.0392\n","Epoch [244/1000], Loss: 0.7506\n","Epoch [246/1000], Loss: 1.1044\n","Epoch [248/1000], Loss: 1.0279\n","Epoch [250/1000], Loss: 0.9513\n","Epoch [252/1000], Loss: 0.9317\n","Epoch [254/1000], Loss: 0.8750\n","Epoch [256/1000], Loss: 0.6432\n","Epoch [258/1000], Loss: 1.1040\n","Epoch [260/1000], Loss: 0.9534\n","Epoch [262/1000], Loss: 1.0226\n","Epoch [264/1000], Loss: 1.0965\n","Epoch [266/1000], Loss: 0.8610\n","Epoch [268/1000], Loss: 0.7047\n","Epoch [270/1000], Loss: 0.8403\n","Epoch [272/1000], Loss: 0.7712\n","Epoch [274/1000], Loss: 0.9802\n","Epoch [276/1000], Loss: 1.1089\n","Epoch [278/1000], Loss: 1.0352\n","Epoch [280/1000], Loss: 0.7058\n","Epoch [282/1000], Loss: 1.0009\n","Epoch [284/1000], Loss: 0.9697\n","Epoch [286/1000], Loss: 0.9461\n","Epoch [288/1000], Loss: 0.8262\n","Epoch [290/1000], Loss: 1.1997\n","Epoch [292/1000], Loss: 1.0174\n","Epoch [294/1000], Loss: 1.1822\n","Epoch [296/1000], Loss: 1.1602\n","Epoch [298/1000], Loss: 0.8467\n","Epoch [300/1000], Loss: 0.7836\n","Epoch [302/1000], Loss: 0.9913\n","Epoch [304/1000], Loss: 1.0943\n","Epoch [306/1000], Loss: 0.8265\n","Epoch [308/1000], Loss: 0.8527\n","Epoch [310/1000], Loss: 0.6976\n","Epoch [312/1000], Loss: 0.8900\n","Epoch [314/1000], Loss: 0.7304\n","Epoch [316/1000], Loss: 1.1097\n","Epoch [318/1000], Loss: 0.9937\n","Epoch [320/1000], Loss: 0.7411\n","Epoch [322/1000], Loss: 0.8968\n","Epoch [324/1000], Loss: 0.8429\n","Epoch [326/1000], Loss: 0.9941\n","Epoch [328/1000], Loss: 1.0233\n","Epoch [330/1000], Loss: 0.7703\n","Epoch [332/1000], Loss: 0.9190\n","Epoch [334/1000], Loss: 0.9019\n","Epoch [336/1000], Loss: 0.7523\n","Epoch [338/1000], Loss: 0.8555\n","Epoch [340/1000], Loss: 0.8978\n","Epoch [342/1000], Loss: 1.0768\n","Epoch [344/1000], Loss: 0.9190\n","Epoch [346/1000], Loss: 0.9574\n","Epoch [348/1000], Loss: 0.5065\n","Epoch [350/1000], Loss: 0.7956\n","Epoch [352/1000], Loss: 0.9190\n","Epoch [354/1000], Loss: 0.8846\n","Epoch [356/1000], Loss: 0.5572\n","Epoch [358/1000], Loss: 0.8612\n","Epoch [360/1000], Loss: 0.9299\n","Epoch [362/1000], Loss: 0.8602\n","Epoch [364/1000], Loss: 0.9765\n","Epoch [366/1000], Loss: 0.7474\n","Epoch [368/1000], Loss: 1.0700\n","Epoch [370/1000], Loss: 1.0079\n","Epoch [372/1000], Loss: 0.7953\n","Epoch [374/1000], Loss: 1.0044\n","Epoch [376/1000], Loss: 0.8538\n","Epoch [378/1000], Loss: 0.8965\n","Epoch [380/1000], Loss: 1.0670\n","Epoch [382/1000], Loss: 1.1339\n","Epoch [384/1000], Loss: 0.5795\n","Epoch [386/1000], Loss: 1.0580\n","Epoch [388/1000], Loss: 0.9866\n","Epoch [390/1000], Loss: 0.7524\n","Epoch [392/1000], Loss: 0.9499\n","Epoch [394/1000], Loss: 1.0015\n","Epoch [396/1000], Loss: 0.7681\n","Epoch [398/1000], Loss: 0.8968\n","Epoch [400/1000], Loss: 0.7510\n","Epoch [402/1000], Loss: 0.9106\n","Epoch [404/1000], Loss: 0.7514\n","Epoch [406/1000], Loss: 0.6647\n","Epoch [408/1000], Loss: 0.5657\n","Epoch [410/1000], Loss: 0.7299\n","Epoch [412/1000], Loss: 0.8862\n","Epoch [414/1000], Loss: 0.9430\n","Epoch [416/1000], Loss: 0.7597\n","Epoch [418/1000], Loss: 1.1839\n","Epoch [420/1000], Loss: 1.0770\n","Epoch [422/1000], Loss: 1.0111\n","Epoch [424/1000], Loss: 0.9695\n","Epoch [426/1000], Loss: 0.8580\n","Epoch [428/1000], Loss: 1.1357\n","Epoch [430/1000], Loss: 0.8969\n","Epoch [432/1000], Loss: 0.7096\n","Epoch [434/1000], Loss: 0.9777\n","Epoch [436/1000], Loss: 1.1873\n","Epoch [438/1000], Loss: 0.9076\n","Epoch [440/1000], Loss: 0.9238\n","Epoch [442/1000], Loss: 1.0361\n","Epoch [444/1000], Loss: 0.8608\n","Epoch [446/1000], Loss: 1.1943\n","Epoch [448/1000], Loss: 0.8524\n","Epoch [450/1000], Loss: 0.9039\n","Epoch [452/1000], Loss: 1.0266\n","Epoch [454/1000], Loss: 1.0335\n","Epoch [456/1000], Loss: 0.7720\n","Epoch [458/1000], Loss: 0.9843\n","Epoch [460/1000], Loss: 0.6648\n","Epoch [462/1000], Loss: 0.8505\n","Epoch [464/1000], Loss: 0.8161\n","Epoch [466/1000], Loss: 0.8560\n","Epoch [468/1000], Loss: 0.7462\n","Epoch [470/1000], Loss: 0.7483\n","Epoch [472/1000], Loss: 0.9615\n","Epoch [474/1000], Loss: 1.0065\n","Epoch [476/1000], Loss: 1.0922\n","Epoch [478/1000], Loss: 0.9815\n","Epoch [480/1000], Loss: 0.7849\n","Epoch [482/1000], Loss: 0.7511\n","Epoch [484/1000], Loss: 0.8244\n","Epoch [486/1000], Loss: 0.9516\n","Epoch [488/1000], Loss: 0.9519\n","Epoch [490/1000], Loss: 0.8236\n","Epoch [492/1000], Loss: 0.9590\n","Epoch [494/1000], Loss: 0.7762\n","Epoch [496/1000], Loss: 1.0111\n","Epoch [498/1000], Loss: 0.8600\n","Epoch [500/1000], Loss: 1.1064\n","Epoch [502/1000], Loss: 0.9217\n","Epoch [504/1000], Loss: 0.6987\n","Epoch [506/1000], Loss: 1.3059\n","Epoch [508/1000], Loss: 0.8392\n","Epoch [510/1000], Loss: 0.8120\n","Epoch [512/1000], Loss: 1.0019\n","Epoch [514/1000], Loss: 0.7699\n","Epoch [516/1000], Loss: 0.8105\n","Epoch [518/1000], Loss: 0.7724\n","Epoch [520/1000], Loss: 0.8709\n","Epoch [522/1000], Loss: 0.8155\n","Epoch [524/1000], Loss: 1.0707\n","Epoch [526/1000], Loss: 1.0858\n","Epoch [528/1000], Loss: 0.9098\n","Epoch [530/1000], Loss: 0.9305\n","Epoch [532/1000], Loss: 0.7923\n","Epoch [534/1000], Loss: 0.8666\n","Epoch [536/1000], Loss: 0.8746\n","Epoch [538/1000], Loss: 0.9671\n","Epoch [540/1000], Loss: 0.7119\n","Epoch [542/1000], Loss: 0.8643\n","Epoch [544/1000], Loss: 0.7879\n","Epoch [546/1000], Loss: 0.8364\n","Epoch [548/1000], Loss: 0.9515\n","Epoch [550/1000], Loss: 0.8620\n","Epoch [552/1000], Loss: 0.8467\n","Epoch [554/1000], Loss: 0.5973\n","Epoch [556/1000], Loss: 0.7008\n","Epoch [558/1000], Loss: 1.0237\n","Epoch [560/1000], Loss: 0.8335\n","Epoch [562/1000], Loss: 0.8262\n","Epoch [564/1000], Loss: 0.7204\n","Epoch [566/1000], Loss: 1.0236\n","Epoch [568/1000], Loss: 0.8273\n","Epoch [570/1000], Loss: 0.7491\n","Epoch [572/1000], Loss: 0.6447\n","Epoch [574/1000], Loss: 0.8591\n","Epoch [576/1000], Loss: 0.7548\n","Epoch [578/1000], Loss: 0.7969\n","Epoch [580/1000], Loss: 0.8018\n","Epoch [582/1000], Loss: 1.0230\n","Epoch [584/1000], Loss: 0.5763\n","Epoch [586/1000], Loss: 0.6694\n","Epoch [588/1000], Loss: 0.7589\n","Epoch [590/1000], Loss: 1.0951\n","Epoch [592/1000], Loss: 1.1043\n","Epoch [594/1000], Loss: 0.7536\n","Epoch [596/1000], Loss: 0.7353\n","Epoch [598/1000], Loss: 1.0698\n","Epoch [600/1000], Loss: 1.1372\n","Epoch [602/1000], Loss: 0.6688\n","Epoch [604/1000], Loss: 0.9513\n","Epoch [606/1000], Loss: 0.8219\n","Epoch [608/1000], Loss: 1.0181\n","Epoch [610/1000], Loss: 0.8747\n","Epoch [612/1000], Loss: 0.7757\n","Epoch [614/1000], Loss: 0.6888\n","Epoch [616/1000], Loss: 0.6398\n","Epoch [618/1000], Loss: 0.9075\n","Epoch [620/1000], Loss: 0.7090\n","Epoch [622/1000], Loss: 1.1725\n","Epoch [624/1000], Loss: 0.8742\n","Epoch [626/1000], Loss: 0.7125\n","Epoch [628/1000], Loss: 0.7197\n","Epoch [630/1000], Loss: 0.5029\n","Epoch [632/1000], Loss: 0.8621\n","Epoch [634/1000], Loss: 0.7042\n","Epoch [636/1000], Loss: 0.7310\n","Epoch [638/1000], Loss: 0.8599\n","Epoch [640/1000], Loss: 0.7367\n","Epoch [642/1000], Loss: 0.7406\n","Epoch [644/1000], Loss: 1.0683\n","Epoch [646/1000], Loss: 0.6798\n","Epoch [648/1000], Loss: 0.5990\n","Epoch [650/1000], Loss: 1.0361\n","Epoch [652/1000], Loss: 0.7610\n","Epoch [654/1000], Loss: 0.8867\n","Epoch [656/1000], Loss: 0.7678\n","Epoch [658/1000], Loss: 1.1521\n","Epoch [660/1000], Loss: 1.0017\n","Epoch [662/1000], Loss: 1.0279\n","Epoch [664/1000], Loss: 0.8390\n","Epoch [666/1000], Loss: 0.8517\n","Epoch [668/1000], Loss: 0.9139\n","Epoch [670/1000], Loss: 0.7731\n","Epoch [672/1000], Loss: 0.8537\n","Epoch [674/1000], Loss: 1.0239\n","Epoch [676/1000], Loss: 0.7451\n","Epoch [678/1000], Loss: 0.5665\n","Epoch [680/1000], Loss: 0.7904\n","Epoch [682/1000], Loss: 0.7030\n","Epoch [684/1000], Loss: 0.7036\n","Epoch [686/1000], Loss: 0.8972\n","Epoch [688/1000], Loss: 0.6240\n","Epoch [690/1000], Loss: 0.8562\n","Epoch [692/1000], Loss: 1.0219\n","Epoch [694/1000], Loss: 0.6761\n","Epoch [696/1000], Loss: 0.8030\n","Epoch [698/1000], Loss: 0.7543\n","Epoch [700/1000], Loss: 0.7878\n","Epoch [702/1000], Loss: 0.6946\n","Epoch [704/1000], Loss: 1.0793\n","Epoch [706/1000], Loss: 0.9517\n","Epoch [708/1000], Loss: 0.8506\n","Epoch [710/1000], Loss: 0.9208\n","Epoch [712/1000], Loss: 0.9080\n","Epoch [714/1000], Loss: 0.9981\n","Epoch [716/1000], Loss: 0.8356\n","Epoch [718/1000], Loss: 0.6292\n","Epoch [720/1000], Loss: 0.7270\n","Epoch [722/1000], Loss: 0.8819\n","Epoch [724/1000], Loss: 0.4838\n","Epoch [726/1000], Loss: 0.9173\n","Epoch [728/1000], Loss: 0.9817\n","Epoch [730/1000], Loss: 0.7607\n","Epoch [732/1000], Loss: 0.9189\n","Epoch [734/1000], Loss: 0.7111\n","Epoch [736/1000], Loss: 0.6155\n","Epoch [738/1000], Loss: 0.6765\n","Epoch [740/1000], Loss: 0.7569\n","Epoch [742/1000], Loss: 0.5911\n","Epoch [744/1000], Loss: 0.8168\n","Epoch [746/1000], Loss: 1.0485\n","Epoch [748/1000], Loss: 0.8897\n","Epoch [750/1000], Loss: 0.7572\n","Epoch [752/1000], Loss: 0.6813\n","Epoch [754/1000], Loss: 0.8474\n","Epoch [756/1000], Loss: 0.7672\n","Epoch [758/1000], Loss: 0.6953\n","Epoch [760/1000], Loss: 0.7165\n","Epoch [762/1000], Loss: 0.5210\n","Epoch [764/1000], Loss: 0.8053\n","Epoch [766/1000], Loss: 0.9616\n","Epoch [768/1000], Loss: 0.6583\n","Epoch [770/1000], Loss: 0.7047\n","Epoch [772/1000], Loss: 0.8766\n","Epoch [774/1000], Loss: 0.9660\n","Epoch [776/1000], Loss: 0.6681\n","Epoch [778/1000], Loss: 1.0592\n","Epoch [780/1000], Loss: 0.8712\n","Epoch [782/1000], Loss: 0.9208\n","Epoch [784/1000], Loss: 0.6886\n","Epoch [786/1000], Loss: 0.8448\n","Epoch [788/1000], Loss: 0.5407\n","Epoch [790/1000], Loss: 0.8496\n","Epoch [792/1000], Loss: 0.8803\n","Epoch [794/1000], Loss: 0.7034\n","Epoch [796/1000], Loss: 0.8196\n","Epoch [798/1000], Loss: 0.9200\n","Epoch [800/1000], Loss: 0.7464\n","Epoch [802/1000], Loss: 0.5738\n","Epoch [804/1000], Loss: 0.7279\n","Epoch [806/1000], Loss: 0.7465\n","Epoch [808/1000], Loss: 0.6918\n","Epoch [810/1000], Loss: 1.0485\n","Epoch [812/1000], Loss: 0.6567\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.0866\n","Epoch [4/1000], Loss: 1.1005\n","Epoch [6/1000], Loss: 0.8506\n","Epoch [8/1000], Loss: 0.8740\n","Epoch [10/1000], Loss: 0.9845\n","Epoch [12/1000], Loss: 0.8342\n","Epoch [14/1000], Loss: 1.1178\n","Epoch [16/1000], Loss: 1.1213\n","Epoch [18/1000], Loss: 0.9549\n","Epoch [20/1000], Loss: 1.0622\n","Epoch [22/1000], Loss: 1.0484\n","Epoch [24/1000], Loss: 0.8867\n","Epoch [26/1000], Loss: 0.6601\n","Epoch [28/1000], Loss: 0.8710\n","Epoch [30/1000], Loss: 0.8952\n","Epoch [32/1000], Loss: 1.1154\n","Epoch [34/1000], Loss: 0.7993\n","Epoch [36/1000], Loss: 0.9009\n","Epoch [38/1000], Loss: 1.0609\n","Epoch [40/1000], Loss: 0.8464\n","Epoch [42/1000], Loss: 1.2406\n","Epoch [44/1000], Loss: 1.1142\n","Epoch [46/1000], Loss: 1.1009\n","Epoch [48/1000], Loss: 1.0880\n","Epoch [50/1000], Loss: 1.0826\n","Epoch [52/1000], Loss: 1.0379\n","Epoch [54/1000], Loss: 1.1608\n","Epoch [56/1000], Loss: 0.7366\n","Epoch [58/1000], Loss: 1.0012\n","Epoch [60/1000], Loss: 0.9502\n","Epoch [62/1000], Loss: 0.9181\n","Epoch [64/1000], Loss: 0.8836\n","Epoch [66/1000], Loss: 0.7720\n","Epoch [68/1000], Loss: 1.0776\n","Epoch [70/1000], Loss: 0.8120\n","Epoch [72/1000], Loss: 0.9843\n","Epoch [74/1000], Loss: 1.0855\n","Epoch [76/1000], Loss: 1.2287\n","Epoch [78/1000], Loss: 1.0056\n","Epoch [80/1000], Loss: 0.9168\n","Epoch [82/1000], Loss: 1.1910\n","Epoch [84/1000], Loss: 0.7465\n","Epoch [86/1000], Loss: 0.7856\n","Epoch [88/1000], Loss: 0.8584\n","Epoch [90/1000], Loss: 1.0588\n","Epoch [92/1000], Loss: 1.3598\n","Epoch [94/1000], Loss: 0.7661\n","Epoch [96/1000], Loss: 1.1504\n","Epoch [98/1000], Loss: 0.9117\n","Epoch [100/1000], Loss: 0.8386\n","Epoch [102/1000], Loss: 1.0261\n","Epoch [104/1000], Loss: 0.8923\n","Epoch [106/1000], Loss: 0.9272\n","Epoch [108/1000], Loss: 0.9590\n","Epoch [110/1000], Loss: 1.1314\n","Epoch [112/1000], Loss: 0.8816\n","Epoch [114/1000], Loss: 1.1960\n","Epoch [116/1000], Loss: 1.0393\n","Epoch [118/1000], Loss: 1.1474\n","Epoch [120/1000], Loss: 1.1737\n","Epoch [122/1000], Loss: 0.9555\n","Epoch [124/1000], Loss: 1.0434\n","Epoch [126/1000], Loss: 0.8743\n","Epoch [128/1000], Loss: 0.9054\n","Epoch [130/1000], Loss: 0.9294\n","Epoch [132/1000], Loss: 0.9893\n","Epoch [134/1000], Loss: 1.1647\n","Epoch [136/1000], Loss: 1.0537\n","Epoch [138/1000], Loss: 0.9842\n","Epoch [140/1000], Loss: 0.8994\n","Epoch [142/1000], Loss: 0.9332\n","Epoch [144/1000], Loss: 1.2638\n","Epoch [146/1000], Loss: 0.9135\n","Epoch [148/1000], Loss: 0.9383\n","Epoch [150/1000], Loss: 0.9221\n","Epoch [152/1000], Loss: 1.0073\n","Epoch [154/1000], Loss: 1.0051\n","Epoch [156/1000], Loss: 0.9371\n","Epoch [158/1000], Loss: 0.8810\n","Epoch [160/1000], Loss: 1.1991\n","Epoch [162/1000], Loss: 0.8056\n","Epoch [164/1000], Loss: 1.2187\n","Epoch [166/1000], Loss: 0.9921\n","Epoch [168/1000], Loss: 0.8328\n","Epoch [170/1000], Loss: 0.8308\n","Epoch [172/1000], Loss: 1.0876\n","Epoch [174/1000], Loss: 1.1012\n","Epoch [176/1000], Loss: 0.9691\n","Epoch [178/1000], Loss: 0.9663\n","Epoch [180/1000], Loss: 0.9119\n","Epoch [182/1000], Loss: 0.8583\n","Epoch [184/1000], Loss: 0.9344\n","Epoch [186/1000], Loss: 0.9557\n","Epoch [188/1000], Loss: 0.9390\n","Epoch [190/1000], Loss: 0.8834\n","Epoch [192/1000], Loss: 1.1792\n","Epoch [194/1000], Loss: 0.8650\n","Epoch [196/1000], Loss: 0.8681\n","Epoch [198/1000], Loss: 0.8355\n","Epoch [200/1000], Loss: 0.8810\n","Epoch [202/1000], Loss: 0.9399\n","Epoch [204/1000], Loss: 1.0586\n","Epoch [206/1000], Loss: 1.0642\n","Epoch [208/1000], Loss: 0.8438\n","Epoch [210/1000], Loss: 0.9201\n","Epoch [212/1000], Loss: 0.9549\n","Epoch [214/1000], Loss: 0.7204\n","Epoch [216/1000], Loss: 1.0276\n","Epoch [218/1000], Loss: 0.8523\n","Epoch [220/1000], Loss: 0.9053\n","Epoch [222/1000], Loss: 0.8739\n","Epoch [224/1000], Loss: 0.9624\n","Epoch [226/1000], Loss: 1.0267\n","Epoch [228/1000], Loss: 0.8502\n","Epoch [230/1000], Loss: 1.0938\n","Epoch [232/1000], Loss: 0.9271\n","Epoch [234/1000], Loss: 0.9230\n","Epoch [236/1000], Loss: 1.0325\n","Epoch [238/1000], Loss: 0.4977\n","Epoch [240/1000], Loss: 0.8944\n","Epoch [242/1000], Loss: 1.1466\n","Epoch [244/1000], Loss: 1.0854\n","Epoch [246/1000], Loss: 0.7691\n","Epoch [248/1000], Loss: 0.9861\n","Epoch [250/1000], Loss: 1.0689\n","Epoch [252/1000], Loss: 1.0156\n","Epoch [254/1000], Loss: 1.1715\n","Epoch [256/1000], Loss: 0.9251\n","Epoch [258/1000], Loss: 0.9065\n","Epoch [260/1000], Loss: 0.9466\n","Epoch [262/1000], Loss: 0.9952\n","Epoch [264/1000], Loss: 0.7775\n","Epoch [266/1000], Loss: 0.9126\n","Epoch [268/1000], Loss: 0.7910\n","Epoch [270/1000], Loss: 0.8431\n","Epoch [272/1000], Loss: 1.0512\n","Epoch [274/1000], Loss: 1.0046\n","Epoch [276/1000], Loss: 1.0166\n","Epoch [278/1000], Loss: 0.9355\n","Epoch [280/1000], Loss: 0.7320\n","Epoch [282/1000], Loss: 1.0734\n","Epoch [284/1000], Loss: 1.3891\n","Epoch [286/1000], Loss: 0.8729\n","Epoch [288/1000], Loss: 0.8326\n","Epoch [290/1000], Loss: 0.7252\n","Epoch [292/1000], Loss: 0.6610\n","Epoch [294/1000], Loss: 1.0368\n","Epoch [296/1000], Loss: 1.1221\n","Epoch [298/1000], Loss: 1.1248\n","Epoch [300/1000], Loss: 0.6888\n","Epoch [302/1000], Loss: 0.8434\n","Epoch [304/1000], Loss: 0.7820\n","Epoch [306/1000], Loss: 1.2473\n","Epoch [308/1000], Loss: 0.8809\n","Epoch [310/1000], Loss: 0.8176\n","Epoch [312/1000], Loss: 0.8259\n","Epoch [314/1000], Loss: 1.0093\n","Epoch [316/1000], Loss: 0.9067\n","Epoch [318/1000], Loss: 0.9534\n","Epoch [320/1000], Loss: 0.6526\n","Epoch [322/1000], Loss: 0.9975\n","Epoch [324/1000], Loss: 1.2041\n","Epoch [326/1000], Loss: 0.8171\n","Epoch [328/1000], Loss: 1.1674\n","Epoch [330/1000], Loss: 0.6371\n","Epoch [332/1000], Loss: 0.8483\n","Epoch [334/1000], Loss: 0.8447\n","Epoch [336/1000], Loss: 0.6270\n","Epoch [338/1000], Loss: 0.9587\n","Epoch [340/1000], Loss: 0.9781\n","Epoch [342/1000], Loss: 0.7465\n","Epoch [344/1000], Loss: 0.8244\n","Epoch [346/1000], Loss: 0.6923\n","Epoch [348/1000], Loss: 1.0640\n","Epoch [350/1000], Loss: 0.7510\n","Epoch [352/1000], Loss: 0.7818\n","Epoch [354/1000], Loss: 0.9406\n","Epoch [356/1000], Loss: 0.7914\n","Epoch [358/1000], Loss: 0.7751\n","Epoch [360/1000], Loss: 0.9431\n","Epoch [362/1000], Loss: 1.0087\n","Epoch [364/1000], Loss: 0.8054\n","Epoch [366/1000], Loss: 0.9006\n","Epoch [368/1000], Loss: 0.8804\n","Epoch [370/1000], Loss: 1.0635\n","Epoch [372/1000], Loss: 1.0376\n","Epoch [374/1000], Loss: 1.0229\n","Epoch [376/1000], Loss: 0.7428\n","Epoch [378/1000], Loss: 0.7569\n","Epoch [380/1000], Loss: 0.8835\n","Epoch [382/1000], Loss: 1.0212\n","Epoch [384/1000], Loss: 0.7962\n","Epoch [386/1000], Loss: 0.7031\n","Epoch [388/1000], Loss: 0.7619\n","Epoch [390/1000], Loss: 1.0488\n","Epoch [392/1000], Loss: 1.2525\n","Epoch [394/1000], Loss: 0.8458\n","Epoch [396/1000], Loss: 0.5903\n","Epoch [398/1000], Loss: 1.0038\n","Epoch [400/1000], Loss: 1.1750\n","Epoch [402/1000], Loss: 0.9505\n","Epoch [404/1000], Loss: 1.0688\n","Epoch [406/1000], Loss: 0.6163\n","Epoch [408/1000], Loss: 0.7681\n","Epoch [410/1000], Loss: 0.4978\n","Epoch [412/1000], Loss: 0.8281\n","Epoch [414/1000], Loss: 0.8092\n","Epoch [416/1000], Loss: 0.8794\n","Epoch [418/1000], Loss: 0.7726\n","Epoch [420/1000], Loss: 1.0167\n","Epoch [422/1000], Loss: 0.9003\n","Epoch [424/1000], Loss: 0.9602\n","Epoch [426/1000], Loss: 0.8291\n","Epoch [428/1000], Loss: 0.8163\n","Epoch [430/1000], Loss: 0.8114\n","Epoch [432/1000], Loss: 0.9970\n","Epoch [434/1000], Loss: 0.8409\n","Epoch [436/1000], Loss: 0.9156\n","Epoch [438/1000], Loss: 0.7565\n","Epoch [440/1000], Loss: 1.0371\n","Epoch [442/1000], Loss: 0.6009\n","Epoch [444/1000], Loss: 0.9607\n","Epoch [446/1000], Loss: 0.7282\n","Epoch [448/1000], Loss: 0.9104\n","Epoch [450/1000], Loss: 0.6659\n","Epoch [452/1000], Loss: 0.7329\n","Epoch [454/1000], Loss: 0.7683\n","Epoch [456/1000], Loss: 0.9504\n","Epoch [458/1000], Loss: 1.2579\n","Epoch [460/1000], Loss: 0.9483\n","Epoch [462/1000], Loss: 1.0294\n","Epoch [464/1000], Loss: 0.9423\n","Epoch [466/1000], Loss: 0.6557\n","Epoch [468/1000], Loss: 1.0828\n","Epoch [470/1000], Loss: 0.6552\n","Epoch [472/1000], Loss: 0.6154\n","Epoch [474/1000], Loss: 0.5232\n","Epoch [476/1000], Loss: 0.8021\n","Epoch [478/1000], Loss: 0.6482\n","Epoch [480/1000], Loss: 1.0271\n","Epoch [482/1000], Loss: 0.8235\n","Epoch [484/1000], Loss: 0.8425\n","Epoch [486/1000], Loss: 0.9210\n","Epoch [488/1000], Loss: 0.7531\n","Epoch [490/1000], Loss: 0.8654\n","Epoch [492/1000], Loss: 0.8937\n","Epoch [494/1000], Loss: 0.9333\n","Epoch [496/1000], Loss: 0.6829\n","Epoch [498/1000], Loss: 0.6124\n","Epoch [500/1000], Loss: 1.0609\n","Epoch [502/1000], Loss: 1.2803\n","Epoch [504/1000], Loss: 0.6685\n","Epoch [506/1000], Loss: 1.1853\n","Epoch [508/1000], Loss: 0.8397\n","Epoch [510/1000], Loss: 0.8111\n","Epoch [512/1000], Loss: 0.7665\n","Epoch [514/1000], Loss: 0.8389\n","Epoch [516/1000], Loss: 0.6634\n","Epoch [518/1000], Loss: 0.9669\n","Epoch [520/1000], Loss: 0.7670\n","Epoch [522/1000], Loss: 0.7127\n","Epoch [524/1000], Loss: 0.7581\n","Epoch [526/1000], Loss: 0.6784\n","Epoch [528/1000], Loss: 0.9097\n","Epoch [530/1000], Loss: 0.9091\n","Epoch [532/1000], Loss: 0.8722\n","Epoch [534/1000], Loss: 1.0172\n","Epoch [536/1000], Loss: 0.6947\n","Epoch [538/1000], Loss: 0.6310\n","Epoch [540/1000], Loss: 0.7701\n","Epoch [542/1000], Loss: 1.0157\n","Epoch [544/1000], Loss: 0.7991\n","Epoch [546/1000], Loss: 0.9338\n","Epoch [548/1000], Loss: 0.8099\n","Epoch [550/1000], Loss: 0.9204\n","Epoch [552/1000], Loss: 1.0715\n","Epoch [554/1000], Loss: 0.7991\n","Epoch [556/1000], Loss: 0.9821\n","Epoch [558/1000], Loss: 0.7178\n","Epoch [560/1000], Loss: 0.6991\n","Epoch [562/1000], Loss: 1.0176\n","Epoch [564/1000], Loss: 0.8793\n","Epoch [566/1000], Loss: 0.8550\n","Epoch [568/1000], Loss: 0.6369\n","Epoch [570/1000], Loss: 0.7225\n","Epoch [572/1000], Loss: 0.8850\n","Epoch [574/1000], Loss: 0.9757\n","Epoch [576/1000], Loss: 0.7843\n","Epoch [578/1000], Loss: 0.8225\n","Epoch [580/1000], Loss: 0.9610\n","Epoch [582/1000], Loss: 0.7420\n","Epoch [584/1000], Loss: 0.5451\n","Epoch [586/1000], Loss: 0.8588\n","Epoch [588/1000], Loss: 0.8211\n","Epoch [590/1000], Loss: 0.7543\n","Epoch [592/1000], Loss: 1.0552\n","Epoch [594/1000], Loss: 1.0317\n","Epoch [596/1000], Loss: 0.6875\n","Epoch [598/1000], Loss: 1.0251\n","Epoch [600/1000], Loss: 0.6201\n","Epoch [602/1000], Loss: 0.7595\n","Epoch [604/1000], Loss: 0.8590\n","Epoch [606/1000], Loss: 0.9659\n","Epoch [608/1000], Loss: 0.5649\n","Epoch [610/1000], Loss: 1.3353\n","Epoch [612/1000], Loss: 1.0829\n","Epoch [614/1000], Loss: 0.7303\n","Epoch [616/1000], Loss: 0.8595\n","Epoch [618/1000], Loss: 0.5949\n","Epoch [620/1000], Loss: 1.1905\n","Epoch [622/1000], Loss: 0.7413\n","Epoch [624/1000], Loss: 0.6926\n","Epoch [626/1000], Loss: 0.4950\n","Epoch [628/1000], Loss: 1.0685\n","Epoch [630/1000], Loss: 0.6353\n","Epoch [632/1000], Loss: 0.9738\n","Epoch [634/1000], Loss: 0.8566\n","Epoch [636/1000], Loss: 0.9128\n","Epoch [638/1000], Loss: 0.6423\n","Epoch [640/1000], Loss: 0.8642\n","Epoch [642/1000], Loss: 0.6421\n","Epoch [644/1000], Loss: 0.8374\n","Epoch [646/1000], Loss: 0.9012\n","Epoch [648/1000], Loss: 0.8245\n","Epoch [650/1000], Loss: 0.5390\n","Epoch [652/1000], Loss: 0.8818\n","Epoch [654/1000], Loss: 0.8456\n","Epoch [656/1000], Loss: 0.8687\n","Epoch [658/1000], Loss: 0.9758\n","Epoch [660/1000], Loss: 1.1240\n","Epoch [662/1000], Loss: 0.8326\n","Epoch [664/1000], Loss: 0.6171\n","Epoch [666/1000], Loss: 0.7753\n","Epoch [668/1000], Loss: 0.7162\n","Epoch [670/1000], Loss: 0.5031\n","Epoch [672/1000], Loss: 0.9969\n","Epoch [674/1000], Loss: 0.5957\n","Epoch [676/1000], Loss: 0.7015\n","Epoch [678/1000], Loss: 1.0878\n","Epoch [680/1000], Loss: 0.9608\n","Epoch [682/1000], Loss: 0.8039\n","Epoch [684/1000], Loss: 0.9214\n","Epoch [686/1000], Loss: 1.2108\n","Epoch [688/1000], Loss: 0.8853\n","Epoch [690/1000], Loss: 0.8555\n","Epoch [692/1000], Loss: 0.8367\n","Epoch [694/1000], Loss: 0.7655\n","Epoch [696/1000], Loss: 0.7179\n","Epoch [698/1000], Loss: 0.9885\n","Epoch [700/1000], Loss: 0.9261\n","Epoch [702/1000], Loss: 0.8060\n","Epoch [704/1000], Loss: 0.7851\n","Epoch [706/1000], Loss: 1.1458\n","Epoch [708/1000], Loss: 0.8284\n","Epoch [710/1000], Loss: 0.7756\n","Epoch [712/1000], Loss: 0.5964\n","Epoch [714/1000], Loss: 1.0187\n","Epoch [716/1000], Loss: 0.9748\n","Epoch [718/1000], Loss: 0.8642\n","Epoch [720/1000], Loss: 1.0742\n","Epoch [722/1000], Loss: 0.8912\n","Epoch [724/1000], Loss: 0.8249\n","Epoch [726/1000], Loss: 1.0271\n","Epoch [728/1000], Loss: 0.9144\n","Epoch [730/1000], Loss: 0.8278\n","Epoch [732/1000], Loss: 0.8493\n","Epoch [734/1000], Loss: 0.5514\n","Epoch [736/1000], Loss: 0.8631\n","Epoch [738/1000], Loss: 0.6513\n","Epoch [740/1000], Loss: 0.9658\n","Epoch [742/1000], Loss: 0.9175\n","Epoch [744/1000], Loss: 0.8394\n","Epoch [746/1000], Loss: 0.7079\n","Epoch [748/1000], Loss: 0.7824\n","Epoch [750/1000], Loss: 0.6888\n","Epoch [752/1000], Loss: 1.0009\n","Epoch [754/1000], Loss: 0.8514\n","Epoch [756/1000], Loss: 0.8150\n","Epoch [758/1000], Loss: 1.0633\n","Epoch [760/1000], Loss: 1.0991\n","Epoch [762/1000], Loss: 0.7727\n","Epoch [764/1000], Loss: 0.7480\n","Epoch [766/1000], Loss: 0.8659\n","Epoch [768/1000], Loss: 0.8151\n","Epoch [770/1000], Loss: 0.8422\n","Epoch [772/1000], Loss: 0.9705\n","Epoch [774/1000], Loss: 0.8000\n","Epoch [776/1000], Loss: 0.7178\n","Epoch [778/1000], Loss: 1.0556\n","Epoch [780/1000], Loss: 0.7105\n","Epoch [782/1000], Loss: 0.5787\n","Epoch [784/1000], Loss: 0.9335\n","Epoch [786/1000], Loss: 1.0570\n","Epoch [788/1000], Loss: 0.6682\n","Epoch [790/1000], Loss: 0.6249\n","Epoch [792/1000], Loss: 0.8383\n","Epoch [794/1000], Loss: 0.8146\n","Epoch [796/1000], Loss: 1.0546\n","Epoch [798/1000], Loss: 0.7557\n","Epoch [800/1000], Loss: 0.9433\n","Epoch [802/1000], Loss: 0.7523\n","Epoch [804/1000], Loss: 1.1971\n","Epoch [806/1000], Loss: 0.7860\n","Epoch [808/1000], Loss: 0.7485\n","Epoch [810/1000], Loss: 0.8157\n","Epoch [812/1000], Loss: 1.1032\n","Epoch [814/1000], Loss: 0.7613\n","Epoch [816/1000], Loss: 0.8785\n","Epoch [818/1000], Loss: 0.7551\n","Epoch [820/1000], Loss: 0.6808\n","Epoch [822/1000], Loss: 0.6355\n","Epoch [824/1000], Loss: 0.7640\n","Epoch [826/1000], Loss: 0.8318\n","Epoch [828/1000], Loss: 0.6902\n","Epoch [830/1000], Loss: 1.0895\n","Epoch [832/1000], Loss: 0.7398\n","Epoch [834/1000], Loss: 1.1976\n","Epoch [836/1000], Loss: 1.0149\n","Epoch [838/1000], Loss: 0.8346\n","Epoch [840/1000], Loss: 0.6870\n","Epoch [842/1000], Loss: 0.8633\n","Epoch [844/1000], Loss: 0.8588\n","Epoch [846/1000], Loss: 1.0138\n","Epoch [848/1000], Loss: 0.6261\n","Epoch [850/1000], Loss: 0.8344\n","Epoch [852/1000], Loss: 0.5992\n","Epoch [854/1000], Loss: 0.8238\n","Epoch [856/1000], Loss: 1.0344\n","Epoch [858/1000], Loss: 0.9480\n","Epoch [860/1000], Loss: 1.0260\n","Epoch [862/1000], Loss: 0.8870\n","Epoch [864/1000], Loss: 0.8418\n","Epoch [866/1000], Loss: 0.8961\n","Epoch [868/1000], Loss: 0.6759\n","Epoch [870/1000], Loss: 0.9047\n","Epoch [872/1000], Loss: 0.6471\n","Epoch [874/1000], Loss: 0.7716\n","Epoch [876/1000], Loss: 0.7589\n","Epoch [878/1000], Loss: 0.8182\n","Epoch [880/1000], Loss: 0.6312\n","Epoch [882/1000], Loss: 0.6347\n","Epoch [884/1000], Loss: 0.8500\n","Epoch [886/1000], Loss: 1.1154\n","Epoch [888/1000], Loss: 0.9000\n","Epoch [890/1000], Loss: 0.9052\n","Epoch [892/1000], Loss: 1.0954\n","Epoch [894/1000], Loss: 0.6096\n","Epoch [896/1000], Loss: 0.6546\n","Epoch [898/1000], Loss: 0.6692\n","Epoch [900/1000], Loss: 0.7462\n","Epoch [902/1000], Loss: 0.8655\n","Epoch [904/1000], Loss: 0.7156\n","Epoch [906/1000], Loss: 0.8334\n","Epoch [908/1000], Loss: 0.6159\n","Epoch [910/1000], Loss: 0.7672\n","Epoch [912/1000], Loss: 0.7115\n","Epoch [914/1000], Loss: 0.6179\n","Epoch [916/1000], Loss: 0.7108\n","Epoch [918/1000], Loss: 1.2626\n","Epoch [920/1000], Loss: 1.0294\n","Epoch [922/1000], Loss: 0.6867\n","Epoch [924/1000], Loss: 0.8921\n","Epoch [926/1000], Loss: 0.7201\n","Epoch [928/1000], Loss: 0.6826\n","Epoch [930/1000], Loss: 0.8735\n","Epoch [932/1000], Loss: 1.0983\n","Epoch [934/1000], Loss: 0.6965\n","Epoch [936/1000], Loss: 0.7330\n","Epoch [938/1000], Loss: 0.9207\n","Epoch [940/1000], Loss: 0.9225\n","Epoch [942/1000], Loss: 0.6162\n","Epoch [944/1000], Loss: 0.9284\n","Epoch [946/1000], Loss: 0.7558\n","Epoch [948/1000], Loss: 0.8656\n","Epoch [950/1000], Loss: 0.9264\n","Epoch [952/1000], Loss: 0.7330\n","Epoch [954/1000], Loss: 1.0616\n","Epoch [956/1000], Loss: 1.2349\n","Epoch [958/1000], Loss: 0.7696\n","Epoch [960/1000], Loss: 0.7522\n","Epoch [962/1000], Loss: 0.7485\n","Epoch [964/1000], Loss: 0.8256\n","Epoch [966/1000], Loss: 1.0738\n","Epoch [968/1000], Loss: 0.6914\n","Epoch [970/1000], Loss: 0.6960\n","Epoch [972/1000], Loss: 0.5910\n","Epoch [974/1000], Loss: 0.8456\n","Epoch [976/1000], Loss: 0.6329\n","Epoch [978/1000], Loss: 0.9697\n","Epoch [980/1000], Loss: 0.8963\n","Epoch [982/1000], Loss: 0.6254\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.0224\n","Epoch [4/1000], Loss: 0.9590\n","Epoch [6/1000], Loss: 1.0438\n","Epoch [8/1000], Loss: 1.1647\n","Epoch [10/1000], Loss: 0.8559\n","Epoch [12/1000], Loss: 0.9631\n","Epoch [14/1000], Loss: 1.1337\n","Epoch [16/1000], Loss: 0.9330\n","Epoch [18/1000], Loss: 1.0766\n","Epoch [20/1000], Loss: 1.0652\n","Epoch [22/1000], Loss: 1.0833\n","Epoch [24/1000], Loss: 1.0063\n","Epoch [26/1000], Loss: 0.7789\n","Epoch [28/1000], Loss: 0.9154\n","Epoch [30/1000], Loss: 0.8721\n","Epoch [32/1000], Loss: 0.8273\n","Epoch [34/1000], Loss: 0.9211\n","Epoch [36/1000], Loss: 1.0307\n","Epoch [38/1000], Loss: 1.4002\n","Epoch [40/1000], Loss: 1.0571\n","Epoch [42/1000], Loss: 1.1435\n","Epoch [44/1000], Loss: 1.0364\n","Epoch [46/1000], Loss: 1.0292\n","Epoch [48/1000], Loss: 0.8684\n","Epoch [50/1000], Loss: 0.8599\n","Epoch [52/1000], Loss: 0.8302\n","Epoch [54/1000], Loss: 0.7834\n","Epoch [56/1000], Loss: 1.0925\n","Epoch [58/1000], Loss: 0.8331\n","Epoch [60/1000], Loss: 1.0962\n","Epoch [62/1000], Loss: 0.8331\n","Epoch [64/1000], Loss: 0.9982\n","Epoch [66/1000], Loss: 0.8414\n","Epoch [68/1000], Loss: 0.9569\n","Epoch [70/1000], Loss: 0.8598\n","Epoch [72/1000], Loss: 0.8564\n","Epoch [74/1000], Loss: 0.8965\n","Epoch [76/1000], Loss: 0.8396\n","Epoch [78/1000], Loss: 0.7712\n","Epoch [80/1000], Loss: 0.8048\n","Epoch [82/1000], Loss: 0.9767\n","Epoch [84/1000], Loss: 0.9066\n","Epoch [86/1000], Loss: 1.1118\n","Epoch [88/1000], Loss: 1.1153\n","Epoch [90/1000], Loss: 0.9855\n","Epoch [92/1000], Loss: 0.8783\n","Epoch [94/1000], Loss: 1.0656\n","Epoch [96/1000], Loss: 1.1373\n","Epoch [98/1000], Loss: 1.0467\n","Epoch [100/1000], Loss: 0.8838\n","Epoch [102/1000], Loss: 0.7984\n","Epoch [104/1000], Loss: 0.9672\n","Epoch [106/1000], Loss: 0.8573\n","Epoch [108/1000], Loss: 1.0402\n","Epoch [110/1000], Loss: 1.0276\n","Epoch [112/1000], Loss: 1.0058\n","Epoch [114/1000], Loss: 0.9790\n","Epoch [116/1000], Loss: 0.9327\n","Epoch [118/1000], Loss: 0.7653\n","Epoch [120/1000], Loss: 1.2051\n","Epoch [122/1000], Loss: 0.9929\n","Epoch [124/1000], Loss: 1.2562\n","Epoch [126/1000], Loss: 1.1366\n","Epoch [128/1000], Loss: 1.0940\n","Epoch [130/1000], Loss: 1.1495\n","Epoch [132/1000], Loss: 1.0879\n","Epoch [134/1000], Loss: 1.0906\n","Epoch [136/1000], Loss: 1.0396\n","Epoch [138/1000], Loss: 0.8995\n","Epoch [140/1000], Loss: 0.9870\n","Epoch [142/1000], Loss: 0.8060\n","Epoch [144/1000], Loss: 0.9849\n","Epoch [146/1000], Loss: 0.7921\n","Epoch [148/1000], Loss: 1.1338\n","Epoch [150/1000], Loss: 0.7816\n","Epoch [152/1000], Loss: 1.0052\n","Epoch [154/1000], Loss: 1.1972\n","Epoch [156/1000], Loss: 1.1024\n","Epoch [158/1000], Loss: 0.8676\n","Epoch [160/1000], Loss: 1.0567\n","Epoch [162/1000], Loss: 0.8280\n","Epoch [164/1000], Loss: 1.1003\n","Epoch [166/1000], Loss: 0.9479\n","Epoch [168/1000], Loss: 0.9531\n","Epoch [170/1000], Loss: 1.1417\n","Epoch [172/1000], Loss: 1.0207\n","Epoch [174/1000], Loss: 0.9093\n","Epoch [176/1000], Loss: 0.8466\n","Epoch [178/1000], Loss: 1.2296\n","Epoch [180/1000], Loss: 1.1103\n","Epoch [182/1000], Loss: 1.0750\n","Epoch [184/1000], Loss: 0.8200\n","Epoch [186/1000], Loss: 1.1351\n","Epoch [188/1000], Loss: 1.0883\n","Epoch [190/1000], Loss: 0.7255\n","Epoch [192/1000], Loss: 0.8412\n","Epoch [194/1000], Loss: 1.2134\n","Epoch [196/1000], Loss: 0.9315\n","Epoch [198/1000], Loss: 0.9150\n","Epoch [200/1000], Loss: 0.9754\n","Epoch [202/1000], Loss: 0.9932\n","Epoch [204/1000], Loss: 0.8079\n","Epoch [206/1000], Loss: 1.1311\n","Epoch [208/1000], Loss: 0.9046\n","Epoch [210/1000], Loss: 0.9473\n","Epoch [212/1000], Loss: 1.1051\n","Epoch [214/1000], Loss: 1.0189\n","Epoch [216/1000], Loss: 0.8261\n","Epoch [218/1000], Loss: 1.0865\n","Epoch [220/1000], Loss: 1.0578\n","Epoch [222/1000], Loss: 0.9816\n","Epoch [224/1000], Loss: 1.0006\n","Epoch [226/1000], Loss: 0.9499\n","Epoch [228/1000], Loss: 1.0819\n","Epoch [230/1000], Loss: 0.7953\n","Epoch [232/1000], Loss: 0.6292\n","Epoch [234/1000], Loss: 1.2201\n","Epoch [236/1000], Loss: 0.9975\n","Epoch [238/1000], Loss: 1.0549\n","Epoch [240/1000], Loss: 0.9561\n","Epoch [242/1000], Loss: 0.9790\n","Epoch [244/1000], Loss: 0.7724\n","Epoch [246/1000], Loss: 0.9259\n","Epoch [248/1000], Loss: 1.1435\n","Epoch [250/1000], Loss: 0.8561\n","Epoch [252/1000], Loss: 0.8505\n","Epoch [254/1000], Loss: 0.8942\n","Epoch [256/1000], Loss: 0.7648\n","Epoch [258/1000], Loss: 1.0513\n","Epoch [260/1000], Loss: 1.0328\n","Epoch [262/1000], Loss: 0.8275\n","Epoch [264/1000], Loss: 0.7439\n","Epoch [266/1000], Loss: 1.1396\n","Epoch [268/1000], Loss: 0.7842\n","Epoch [270/1000], Loss: 0.8360\n","Epoch [272/1000], Loss: 0.8759\n","Epoch [274/1000], Loss: 0.8154\n","Epoch [276/1000], Loss: 1.2522\n","Epoch [278/1000], Loss: 1.0485\n","Epoch [280/1000], Loss: 0.6086\n","Epoch [282/1000], Loss: 0.4784\n","Epoch [284/1000], Loss: 1.0612\n","Epoch [286/1000], Loss: 0.9288\n","Epoch [288/1000], Loss: 0.9346\n","Epoch [290/1000], Loss: 0.8893\n","Epoch [292/1000], Loss: 1.0393\n","Epoch [294/1000], Loss: 0.7965\n","Epoch [296/1000], Loss: 1.1024\n","Epoch [298/1000], Loss: 0.9656\n","Epoch [300/1000], Loss: 1.1551\n","Epoch [302/1000], Loss: 1.0018\n","Epoch [304/1000], Loss: 1.0175\n","Epoch [306/1000], Loss: 0.9188\n","Epoch [308/1000], Loss: 0.9347\n","Epoch [310/1000], Loss: 0.8881\n","Epoch [312/1000], Loss: 0.9535\n","Epoch [314/1000], Loss: 1.0964\n","Epoch [316/1000], Loss: 0.9590\n","Epoch [318/1000], Loss: 0.9579\n","Epoch [320/1000], Loss: 0.9369\n","Epoch [322/1000], Loss: 0.8489\n","Epoch [324/1000], Loss: 0.8208\n","Epoch [326/1000], Loss: 0.8422\n","Epoch [328/1000], Loss: 0.6983\n","Epoch [330/1000], Loss: 0.9033\n","Epoch [332/1000], Loss: 1.0592\n","Epoch [334/1000], Loss: 0.8771\n","Epoch [336/1000], Loss: 0.9376\n","Epoch [338/1000], Loss: 1.0941\n","Epoch [340/1000], Loss: 1.1441\n","Epoch [342/1000], Loss: 1.0357\n","Epoch [344/1000], Loss: 1.0241\n","Epoch [346/1000], Loss: 0.8562\n","Epoch [348/1000], Loss: 0.9211\n","Epoch [350/1000], Loss: 0.9773\n","Epoch [352/1000], Loss: 0.9990\n","Epoch [354/1000], Loss: 0.7127\n","Epoch [356/1000], Loss: 1.0622\n","Epoch [358/1000], Loss: 1.0587\n","Epoch [360/1000], Loss: 1.0893\n","Epoch [362/1000], Loss: 0.8079\n","Epoch [364/1000], Loss: 0.9472\n","Epoch [366/1000], Loss: 0.8239\n","Epoch [368/1000], Loss: 0.6821\n","Epoch [370/1000], Loss: 0.9135\n","Epoch [372/1000], Loss: 0.9833\n","Epoch [374/1000], Loss: 0.9094\n","Epoch [376/1000], Loss: 1.0043\n","Epoch [378/1000], Loss: 1.0388\n","Epoch [380/1000], Loss: 0.6637\n","Epoch [382/1000], Loss: 0.9495\n","Epoch [384/1000], Loss: 0.8913\n","Epoch [386/1000], Loss: 0.8902\n","Epoch [388/1000], Loss: 0.7720\n","Epoch [390/1000], Loss: 1.0005\n","Epoch [392/1000], Loss: 0.7685\n","Epoch [394/1000], Loss: 0.8080\n","Epoch [396/1000], Loss: 1.1084\n","Epoch [398/1000], Loss: 1.0045\n","Epoch [400/1000], Loss: 1.1064\n","Epoch [402/1000], Loss: 0.8174\n","Epoch [404/1000], Loss: 0.8083\n","Epoch [406/1000], Loss: 1.0542\n","Epoch [408/1000], Loss: 0.9109\n","Epoch [410/1000], Loss: 1.1585\n","Epoch [412/1000], Loss: 0.8475\n","Epoch [414/1000], Loss: 0.8374\n","Epoch [416/1000], Loss: 0.9760\n","Epoch [418/1000], Loss: 0.9145\n","Epoch [420/1000], Loss: 1.0001\n","Epoch [422/1000], Loss: 0.9500\n","Epoch [424/1000], Loss: 0.7552\n","Epoch [426/1000], Loss: 0.6112\n","Epoch [428/1000], Loss: 1.0463\n","Epoch [430/1000], Loss: 0.8300\n","Epoch [432/1000], Loss: 0.8288\n","Epoch [434/1000], Loss: 0.9193\n","Epoch [436/1000], Loss: 0.8639\n","Epoch [438/1000], Loss: 0.9151\n","Epoch [440/1000], Loss: 1.2813\n","Epoch [442/1000], Loss: 0.8977\n","Epoch [444/1000], Loss: 1.0805\n","Epoch [446/1000], Loss: 0.9522\n","Epoch [448/1000], Loss: 0.9317\n","Epoch [450/1000], Loss: 1.0201\n","Epoch [452/1000], Loss: 1.0622\n","Epoch [454/1000], Loss: 0.7259\n","Epoch [456/1000], Loss: 0.9562\n","Epoch [458/1000], Loss: 0.6099\n","Epoch [460/1000], Loss: 0.9018\n","Epoch [462/1000], Loss: 0.8789\n","Epoch [464/1000], Loss: 0.7413\n","Epoch [466/1000], Loss: 0.6841\n","Epoch [468/1000], Loss: 0.9732\n","Epoch [470/1000], Loss: 0.6573\n","Epoch [472/1000], Loss: 0.9419\n","Epoch [474/1000], Loss: 0.7886\n","Epoch [476/1000], Loss: 0.7764\n","Epoch [478/1000], Loss: 1.1376\n","Epoch [480/1000], Loss: 1.0151\n","Epoch [482/1000], Loss: 1.0395\n","Epoch [484/1000], Loss: 1.0011\n","Epoch [486/1000], Loss: 1.1158\n","Epoch [488/1000], Loss: 0.7317\n","Epoch [490/1000], Loss: 0.9170\n","Epoch [492/1000], Loss: 0.7765\n","Epoch [494/1000], Loss: 0.6884\n","Epoch [496/1000], Loss: 0.7310\n","Epoch [498/1000], Loss: 0.7256\n","Epoch [500/1000], Loss: 1.0440\n","Epoch [502/1000], Loss: 1.1273\n","Epoch [504/1000], Loss: 0.6868\n","Epoch [506/1000], Loss: 0.9015\n","Epoch [508/1000], Loss: 0.8780\n","Epoch [510/1000], Loss: 0.8468\n","Epoch [512/1000], Loss: 0.6836\n","Epoch [514/1000], Loss: 1.2375\n","Epoch [516/1000], Loss: 0.6691\n","Epoch [518/1000], Loss: 0.7344\n","Epoch [520/1000], Loss: 0.6363\n","Epoch [522/1000], Loss: 1.0391\n","Epoch [524/1000], Loss: 0.8607\n","Epoch [526/1000], Loss: 0.6959\n","Epoch [528/1000], Loss: 0.9871\n","Epoch [530/1000], Loss: 0.6086\n","Epoch [532/1000], Loss: 0.4184\n","Epoch [534/1000], Loss: 0.7654\n","Epoch [536/1000], Loss: 0.6504\n","Epoch [538/1000], Loss: 0.9796\n","Epoch [540/1000], Loss: 0.7825\n","Epoch [542/1000], Loss: 0.7853\n","Epoch [544/1000], Loss: 0.8567\n","Epoch [546/1000], Loss: 1.0599\n","Epoch [548/1000], Loss: 1.1395\n","Epoch [550/1000], Loss: 1.0406\n","Epoch [552/1000], Loss: 1.0772\n","Epoch [554/1000], Loss: 0.8906\n","Epoch [556/1000], Loss: 0.8040\n","Epoch [558/1000], Loss: 0.8063\n","Epoch [560/1000], Loss: 1.0280\n","Epoch [562/1000], Loss: 0.9312\n","Epoch [564/1000], Loss: 0.8031\n","Epoch [566/1000], Loss: 0.7434\n","Epoch [568/1000], Loss: 0.8796\n","Epoch [570/1000], Loss: 0.9612\n","Epoch [572/1000], Loss: 0.6588\n","Epoch [574/1000], Loss: 0.8851\n","Epoch [576/1000], Loss: 0.6879\n","Epoch [578/1000], Loss: 0.6704\n","Epoch [580/1000], Loss: 0.8635\n","Epoch [582/1000], Loss: 0.9892\n","Epoch [584/1000], Loss: 0.9797\n","Epoch [586/1000], Loss: 0.6415\n","Epoch [588/1000], Loss: 0.7176\n","Epoch [590/1000], Loss: 0.9280\n","Epoch [592/1000], Loss: 0.9866\n","Epoch [594/1000], Loss: 0.7142\n","Epoch [596/1000], Loss: 0.7922\n","Epoch [598/1000], Loss: 0.6308\n","Epoch [600/1000], Loss: 0.6779\n","Epoch [602/1000], Loss: 0.9752\n","Epoch [604/1000], Loss: 0.7853\n","Epoch [606/1000], Loss: 0.9113\n","Epoch [608/1000], Loss: 0.6654\n","Epoch [610/1000], Loss: 0.5978\n","Epoch [612/1000], Loss: 0.7808\n","Epoch [614/1000], Loss: 0.9073\n","Epoch [616/1000], Loss: 0.9181\n","Epoch [618/1000], Loss: 0.9910\n","Epoch [620/1000], Loss: 0.8190\n","Epoch [622/1000], Loss: 0.8298\n","Epoch [624/1000], Loss: 0.9130\n","Epoch [626/1000], Loss: 1.1059\n","Epoch [628/1000], Loss: 0.7651\n","Epoch [630/1000], Loss: 0.7808\n","Epoch [632/1000], Loss: 0.8709\n","Epoch [634/1000], Loss: 0.9482\n","Epoch [636/1000], Loss: 0.6359\n","Epoch [638/1000], Loss: 0.7530\n","Epoch [640/1000], Loss: 0.8561\n","Epoch [642/1000], Loss: 0.6777\n","Epoch [644/1000], Loss: 0.5822\n","Epoch [646/1000], Loss: 0.7892\n","Epoch [648/1000], Loss: 0.8336\n","Epoch [650/1000], Loss: 0.5729\n","Epoch [652/1000], Loss: 0.7864\n","Epoch [654/1000], Loss: 0.9002\n","Epoch [656/1000], Loss: 0.7905\n","Epoch [658/1000], Loss: 0.7994\n","Epoch [660/1000], Loss: 0.9684\n","Epoch [662/1000], Loss: 0.8252\n","Epoch [664/1000], Loss: 0.7660\n","Epoch [666/1000], Loss: 0.9943\n","Epoch [668/1000], Loss: 0.8910\n","Epoch [670/1000], Loss: 0.7322\n","Epoch [672/1000], Loss: 0.8232\n","Epoch [674/1000], Loss: 1.2108\n","Epoch [676/1000], Loss: 0.7646\n","Epoch [678/1000], Loss: 0.7582\n","Epoch [680/1000], Loss: 0.8512\n","Epoch [682/1000], Loss: 0.8187\n","Epoch [684/1000], Loss: 0.8136\n","Epoch [686/1000], Loss: 1.1400\n","Epoch [688/1000], Loss: 0.8045\n","Epoch [690/1000], Loss: 1.0330\n","Epoch [692/1000], Loss: 1.1198\n","Epoch [694/1000], Loss: 0.8075\n","Epoch [696/1000], Loss: 0.8824\n","Epoch [698/1000], Loss: 0.7739\n","Epoch [700/1000], Loss: 0.6211\n","Epoch [702/1000], Loss: 0.7264\n","Epoch [704/1000], Loss: 0.8291\n","Epoch [706/1000], Loss: 0.5583\n","Epoch [708/1000], Loss: 0.8211\n","Epoch [710/1000], Loss: 0.8505\n","Epoch [712/1000], Loss: 0.8003\n","Epoch [714/1000], Loss: 1.0166\n","Epoch [716/1000], Loss: 0.7658\n","Epoch [718/1000], Loss: 1.0674\n","Epoch [720/1000], Loss: 0.9823\n","Epoch [722/1000], Loss: 0.6144\n","Epoch [724/1000], Loss: 0.7920\n","Epoch [726/1000], Loss: 0.8279\n","Epoch [728/1000], Loss: 0.9979\n","Epoch [730/1000], Loss: 0.9259\n","Epoch [732/1000], Loss: 0.6309\n","Epoch [734/1000], Loss: 0.6418\n","Epoch [736/1000], Loss: 0.6865\n","Epoch [738/1000], Loss: 1.0094\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.0077\n","Epoch [4/1000], Loss: 1.1373\n","Epoch [6/1000], Loss: 0.9404\n","Epoch [8/1000], Loss: 0.7685\n","Epoch [10/1000], Loss: 1.3052\n","Epoch [12/1000], Loss: 1.0099\n","Epoch [14/1000], Loss: 0.9624\n","Epoch [16/1000], Loss: 0.9316\n","Epoch [18/1000], Loss: 1.2959\n","Epoch [20/1000], Loss: 0.9597\n","Epoch [22/1000], Loss: 0.9010\n","Epoch [24/1000], Loss: 1.0688\n","Epoch [26/1000], Loss: 1.0046\n","Epoch [28/1000], Loss: 1.2625\n","Epoch [30/1000], Loss: 0.9922\n","Epoch [32/1000], Loss: 0.8869\n","Epoch [34/1000], Loss: 1.2270\n","Epoch [36/1000], Loss: 1.0020\n","Epoch [38/1000], Loss: 0.8557\n","Epoch [40/1000], Loss: 0.9253\n","Epoch [42/1000], Loss: 1.1012\n","Epoch [44/1000], Loss: 1.0701\n","Epoch [46/1000], Loss: 0.9354\n","Epoch [48/1000], Loss: 1.2509\n","Epoch [50/1000], Loss: 1.0255\n","Epoch [52/1000], Loss: 1.1395\n","Epoch [54/1000], Loss: 1.0505\n","Epoch [56/1000], Loss: 1.0583\n","Epoch [58/1000], Loss: 1.0770\n","Epoch [60/1000], Loss: 1.2726\n","Epoch [62/1000], Loss: 1.0558\n","Epoch [64/1000], Loss: 1.0723\n","Epoch [66/1000], Loss: 0.9340\n","Epoch [68/1000], Loss: 0.9387\n","Epoch [70/1000], Loss: 0.8166\n","Epoch [72/1000], Loss: 0.9195\n","Epoch [74/1000], Loss: 0.5961\n","Epoch [76/1000], Loss: 0.9243\n","Epoch [78/1000], Loss: 1.1642\n","Epoch [80/1000], Loss: 0.9038\n","Epoch [82/1000], Loss: 1.1624\n","Epoch [84/1000], Loss: 0.8544\n","Epoch [86/1000], Loss: 0.9078\n","Epoch [88/1000], Loss: 0.9625\n","Epoch [90/1000], Loss: 0.8417\n","Epoch [92/1000], Loss: 0.8187\n","Epoch [94/1000], Loss: 0.8485\n","Epoch [96/1000], Loss: 0.9709\n","Epoch [98/1000], Loss: 0.9466\n","Epoch [100/1000], Loss: 1.0456\n","Epoch [102/1000], Loss: 0.8924\n","Epoch [104/1000], Loss: 0.8861\n","Epoch [106/1000], Loss: 1.1982\n","Epoch [108/1000], Loss: 1.2407\n","Epoch [110/1000], Loss: 1.0320\n","Epoch [112/1000], Loss: 0.8081\n","Epoch [114/1000], Loss: 1.1011\n","Epoch [116/1000], Loss: 0.9466\n","Epoch [118/1000], Loss: 0.9293\n","Epoch [120/1000], Loss: 0.8517\n","Epoch [122/1000], Loss: 0.8585\n","Epoch [124/1000], Loss: 0.9002\n","Epoch [126/1000], Loss: 1.0862\n","Epoch [128/1000], Loss: 0.8031\n","Epoch [130/1000], Loss: 0.8850\n","Epoch [132/1000], Loss: 1.0386\n","Epoch [134/1000], Loss: 0.9684\n","Epoch [136/1000], Loss: 1.0625\n","Epoch [138/1000], Loss: 1.0559\n","Epoch [140/1000], Loss: 0.8134\n","Epoch [142/1000], Loss: 0.9153\n","Epoch [144/1000], Loss: 1.3061\n","Epoch [146/1000], Loss: 1.0443\n","Epoch [148/1000], Loss: 1.2028\n","Epoch [150/1000], Loss: 1.1710\n","Epoch [152/1000], Loss: 0.9560\n","Epoch [154/1000], Loss: 0.9277\n","Epoch [156/1000], Loss: 1.0405\n","Epoch [158/1000], Loss: 0.9505\n","Epoch [160/1000], Loss: 0.8245\n","Epoch [162/1000], Loss: 0.7877\n","Epoch [164/1000], Loss: 1.1953\n","Epoch [166/1000], Loss: 0.8586\n","Epoch [168/1000], Loss: 0.8600\n","Epoch [170/1000], Loss: 0.7008\n","Epoch [172/1000], Loss: 0.9499\n","Epoch [174/1000], Loss: 0.9556\n","Epoch [176/1000], Loss: 1.1074\n","Epoch [178/1000], Loss: 0.8937\n","Epoch [180/1000], Loss: 0.8616\n","Epoch [182/1000], Loss: 0.7840\n","Epoch [184/1000], Loss: 0.9770\n","Epoch [186/1000], Loss: 1.0946\n","Epoch [188/1000], Loss: 0.6705\n","Epoch [190/1000], Loss: 0.7403\n","Epoch [192/1000], Loss: 0.7292\n","Epoch [194/1000], Loss: 0.9117\n","Epoch [196/1000], Loss: 0.9938\n","Epoch [198/1000], Loss: 0.9113\n","Epoch [200/1000], Loss: 0.9482\n","Epoch [202/1000], Loss: 1.0264\n","Epoch [204/1000], Loss: 0.6948\n","Epoch [206/1000], Loss: 0.8573\n","Epoch [208/1000], Loss: 0.9792\n","Epoch [210/1000], Loss: 0.9565\n","Epoch [212/1000], Loss: 1.1110\n","Epoch [214/1000], Loss: 1.1339\n","Epoch [216/1000], Loss: 0.9672\n","Epoch [218/1000], Loss: 0.6980\n","Epoch [220/1000], Loss: 1.0436\n","Epoch [222/1000], Loss: 0.8819\n","Epoch [224/1000], Loss: 0.8226\n","Epoch [226/1000], Loss: 0.8342\n","Epoch [228/1000], Loss: 1.0705\n","Epoch [230/1000], Loss: 0.9213\n","Epoch [232/1000], Loss: 0.8054\n","Epoch [234/1000], Loss: 1.0407\n","Epoch [236/1000], Loss: 0.9216\n","Epoch [238/1000], Loss: 0.8538\n","Epoch [240/1000], Loss: 1.0925\n","Epoch [242/1000], Loss: 0.9697\n","Epoch [244/1000], Loss: 0.9808\n","Epoch [246/1000], Loss: 1.1965\n","Epoch [248/1000], Loss: 0.7044\n","Epoch [250/1000], Loss: 0.8735\n","Epoch [252/1000], Loss: 0.5102\n","Epoch [254/1000], Loss: 0.9782\n","Epoch [256/1000], Loss: 1.0146\n","Epoch [258/1000], Loss: 0.9326\n","Epoch [260/1000], Loss: 0.6949\n","Epoch [262/1000], Loss: 0.5983\n","Epoch [264/1000], Loss: 0.9369\n","Epoch [266/1000], Loss: 0.9971\n","Epoch [268/1000], Loss: 1.0226\n","Epoch [270/1000], Loss: 1.0191\n","Epoch [272/1000], Loss: 0.7678\n","Epoch [274/1000], Loss: 0.9465\n","Epoch [276/1000], Loss: 1.1115\n","Epoch [278/1000], Loss: 0.7064\n","Epoch [280/1000], Loss: 0.8803\n","Epoch [282/1000], Loss: 0.9883\n","Epoch [284/1000], Loss: 0.9082\n","Epoch [286/1000], Loss: 1.0889\n","Epoch [288/1000], Loss: 1.1717\n","Epoch [290/1000], Loss: 1.0850\n","Epoch [292/1000], Loss: 0.9806\n","Epoch [294/1000], Loss: 0.8717\n","Epoch [296/1000], Loss: 0.7429\n","Epoch [298/1000], Loss: 1.0449\n","Epoch [300/1000], Loss: 0.8792\n","Epoch [302/1000], Loss: 0.7583\n","Epoch [304/1000], Loss: 0.7887\n","Epoch [306/1000], Loss: 1.2479\n","Epoch [308/1000], Loss: 0.9253\n","Epoch [310/1000], Loss: 0.9668\n","Epoch [312/1000], Loss: 0.8417\n","Epoch [314/1000], Loss: 1.0563\n","Epoch [316/1000], Loss: 0.9981\n","Epoch [318/1000], Loss: 1.1309\n","Epoch [320/1000], Loss: 1.1668\n","Epoch [322/1000], Loss: 0.9270\n","Epoch [324/1000], Loss: 0.9242\n","Epoch [326/1000], Loss: 1.1372\n","Epoch [328/1000], Loss: 0.7788\n","Epoch [330/1000], Loss: 0.9280\n","Epoch [332/1000], Loss: 0.8822\n","Epoch [334/1000], Loss: 0.9421\n","Epoch [336/1000], Loss: 0.8400\n","Epoch [338/1000], Loss: 0.7730\n","Epoch [340/1000], Loss: 0.9658\n","Epoch [342/1000], Loss: 1.0248\n","Epoch [344/1000], Loss: 0.9631\n","Epoch [346/1000], Loss: 0.9741\n","Epoch [348/1000], Loss: 0.8428\n","Epoch [350/1000], Loss: 1.0175\n","Epoch [352/1000], Loss: 0.7487\n","Epoch [354/1000], Loss: 1.1084\n","Epoch [356/1000], Loss: 1.0454\n","Epoch [358/1000], Loss: 0.8686\n","Epoch [360/1000], Loss: 1.1022\n","Epoch [362/1000], Loss: 0.9417\n","Epoch [364/1000], Loss: 1.0615\n","Epoch [366/1000], Loss: 1.0474\n","Epoch [368/1000], Loss: 0.9098\n","Epoch [370/1000], Loss: 0.8670\n","Epoch [372/1000], Loss: 0.7864\n","Epoch [374/1000], Loss: 0.8193\n","Epoch [376/1000], Loss: 1.1231\n","Epoch [378/1000], Loss: 1.0273\n","Epoch [380/1000], Loss: 0.7599\n","Epoch [382/1000], Loss: 0.9024\n","Epoch [384/1000], Loss: 1.0493\n","Epoch [386/1000], Loss: 0.7603\n","Epoch [388/1000], Loss: 0.9253\n","Epoch [390/1000], Loss: 0.9885\n","Epoch [392/1000], Loss: 0.8569\n","Epoch [394/1000], Loss: 1.0592\n","Epoch [396/1000], Loss: 0.6055\n","Epoch [398/1000], Loss: 1.0508\n","Epoch [400/1000], Loss: 0.7553\n","Epoch [402/1000], Loss: 1.2119\n","Epoch [404/1000], Loss: 0.8465\n","Epoch [406/1000], Loss: 0.8515\n","Epoch [408/1000], Loss: 0.9322\n","Epoch [410/1000], Loss: 0.8891\n","Epoch [412/1000], Loss: 1.0582\n","Epoch [414/1000], Loss: 0.9752\n","Epoch [416/1000], Loss: 0.8359\n","Epoch [418/1000], Loss: 1.0237\n","Epoch [420/1000], Loss: 1.3297\n","Epoch [422/1000], Loss: 1.1945\n","Epoch [424/1000], Loss: 0.6957\n","Epoch [426/1000], Loss: 0.7499\n","Epoch [428/1000], Loss: 0.7269\n","Epoch [430/1000], Loss: 0.9019\n","Epoch [432/1000], Loss: 0.7704\n","Epoch [434/1000], Loss: 1.1174\n","Epoch [436/1000], Loss: 1.0021\n","Epoch [438/1000], Loss: 0.8765\n","Epoch [440/1000], Loss: 1.0035\n","Epoch [442/1000], Loss: 0.9939\n","Epoch [444/1000], Loss: 0.8044\n","Epoch [446/1000], Loss: 0.6389\n","Epoch [448/1000], Loss: 0.7718\n","Epoch [450/1000], Loss: 1.0847\n","Epoch [452/1000], Loss: 1.0370\n","Epoch [454/1000], Loss: 0.9697\n","Epoch [456/1000], Loss: 0.6216\n","Epoch [458/1000], Loss: 1.0031\n","Epoch [460/1000], Loss: 0.9154\n","Epoch [462/1000], Loss: 1.1076\n","Epoch [464/1000], Loss: 0.7992\n","Epoch [466/1000], Loss: 1.1100\n","Epoch [468/1000], Loss: 0.8965\n","Epoch [470/1000], Loss: 0.8670\n","Epoch [472/1000], Loss: 1.0067\n","Epoch [474/1000], Loss: 0.8003\n","Epoch [476/1000], Loss: 0.7413\n","Epoch [478/1000], Loss: 1.0345\n","Epoch [480/1000], Loss: 0.8429\n","Epoch [482/1000], Loss: 0.8272\n","Epoch [484/1000], Loss: 1.0182\n","Epoch [486/1000], Loss: 0.7499\n","Epoch [488/1000], Loss: 0.7914\n","Epoch [490/1000], Loss: 1.1163\n","Epoch [492/1000], Loss: 0.8416\n","Epoch [494/1000], Loss: 0.7185\n","Epoch [496/1000], Loss: 0.9258\n","Epoch [498/1000], Loss: 0.7187\n","Epoch [500/1000], Loss: 0.7994\n","Epoch [502/1000], Loss: 1.0027\n","Epoch [504/1000], Loss: 0.8264\n","Epoch [506/1000], Loss: 0.7727\n","Epoch [508/1000], Loss: 0.8995\n","Epoch [510/1000], Loss: 1.2014\n","Epoch [512/1000], Loss: 0.6833\n","Epoch [514/1000], Loss: 0.8549\n","Epoch [516/1000], Loss: 0.6567\n","Epoch [518/1000], Loss: 0.8017\n","Epoch [520/1000], Loss: 0.6499\n","Epoch [522/1000], Loss: 1.2969\n","Epoch [524/1000], Loss: 0.6723\n","Epoch [526/1000], Loss: 0.8353\n","Epoch [528/1000], Loss: 0.9164\n","Epoch [530/1000], Loss: 1.1908\n","Epoch [532/1000], Loss: 0.8714\n","Epoch [534/1000], Loss: 0.8127\n","Epoch [536/1000], Loss: 1.1099\n","Epoch [538/1000], Loss: 0.5040\n","Epoch [540/1000], Loss: 1.0497\n","Epoch [542/1000], Loss: 0.9878\n","Epoch [544/1000], Loss: 0.7863\n","Epoch [546/1000], Loss: 0.9122\n","Epoch [548/1000], Loss: 0.9684\n","Epoch [550/1000], Loss: 0.6338\n","Epoch [552/1000], Loss: 0.7644\n","Epoch [554/1000], Loss: 0.9720\n","Epoch [556/1000], Loss: 0.7729\n","Epoch [558/1000], Loss: 0.9469\n","Epoch [560/1000], Loss: 0.9789\n","Epoch [562/1000], Loss: 0.9439\n","Epoch [564/1000], Loss: 0.7800\n","Epoch [566/1000], Loss: 0.8674\n","Epoch [568/1000], Loss: 0.6957\n","Epoch [570/1000], Loss: 0.7191\n","Epoch [572/1000], Loss: 0.8317\n","Epoch [574/1000], Loss: 0.8134\n","Epoch [576/1000], Loss: 0.8374\n","Epoch [578/1000], Loss: 1.0355\n","Epoch [580/1000], Loss: 0.8033\n","Epoch [582/1000], Loss: 0.6426\n","Epoch [584/1000], Loss: 0.9049\n","Epoch [586/1000], Loss: 0.7668\n","Epoch [588/1000], Loss: 0.8059\n","Epoch [590/1000], Loss: 0.9548\n","Epoch [592/1000], Loss: 0.5775\n","Epoch [594/1000], Loss: 0.7121\n","Epoch [596/1000], Loss: 0.9251\n","Epoch [598/1000], Loss: 0.9832\n","Epoch [600/1000], Loss: 0.8509\n","Epoch [602/1000], Loss: 0.5543\n","Epoch [604/1000], Loss: 0.7964\n","Epoch [606/1000], Loss: 0.9186\n","Epoch [608/1000], Loss: 0.7494\n","Epoch [610/1000], Loss: 0.6120\n","Epoch [612/1000], Loss: 0.7315\n","Epoch [614/1000], Loss: 0.8314\n","Epoch [616/1000], Loss: 0.8059\n","Epoch [618/1000], Loss: 0.7034\n","Epoch [620/1000], Loss: 0.7784\n","Epoch [622/1000], Loss: 0.8031\n","Epoch [624/1000], Loss: 0.7141\n","Epoch [626/1000], Loss: 1.2004\n","Epoch [628/1000], Loss: 1.0543\n","Epoch [630/1000], Loss: 0.8903\n","Epoch [632/1000], Loss: 1.2196\n","Epoch [634/1000], Loss: 0.8667\n","Epoch [636/1000], Loss: 1.0274\n","Epoch [638/1000], Loss: 0.7766\n","Epoch [640/1000], Loss: 0.7806\n","Epoch [642/1000], Loss: 0.6411\n","Epoch [644/1000], Loss: 0.6710\n","Epoch [646/1000], Loss: 1.0869\n","Epoch [648/1000], Loss: 0.6962\n","Epoch [650/1000], Loss: 0.6546\n","Epoch [652/1000], Loss: 0.7551\n","Epoch [654/1000], Loss: 0.9551\n","Epoch [656/1000], Loss: 0.9297\n","Epoch [658/1000], Loss: 0.8949\n","Epoch [660/1000], Loss: 0.8394\n","Epoch [662/1000], Loss: 0.6961\n","Epoch [664/1000], Loss: 0.9973\n","Epoch [666/1000], Loss: 0.6259\n","Epoch [668/1000], Loss: 1.1481\n","Epoch [670/1000], Loss: 0.6025\n","Epoch [672/1000], Loss: 0.9318\n","Epoch [674/1000], Loss: 0.7472\n","Epoch [676/1000], Loss: 0.5303\n","Epoch [678/1000], Loss: 1.0793\n","Epoch [680/1000], Loss: 0.8185\n","Epoch [682/1000], Loss: 0.6937\n","Epoch [684/1000], Loss: 0.6042\n","Epoch [686/1000], Loss: 0.8176\n","Epoch [688/1000], Loss: 0.7249\n","Epoch [690/1000], Loss: 0.9283\n","Epoch [692/1000], Loss: 0.8269\n","Epoch [694/1000], Loss: 0.6520\n","Epoch [696/1000], Loss: 0.8368\n","Epoch [698/1000], Loss: 0.6812\n","Epoch [700/1000], Loss: 0.5670\n","Epoch [702/1000], Loss: 0.9148\n","Epoch [704/1000], Loss: 0.7497\n","Epoch [706/1000], Loss: 0.9801\n","Epoch [708/1000], Loss: 0.8113\n","Epoch [710/1000], Loss: 0.9264\n","Epoch [712/1000], Loss: 0.8321\n","Epoch [714/1000], Loss: 0.9773\n","Epoch [716/1000], Loss: 0.8963\n","Epoch [718/1000], Loss: 0.8281\n","Epoch [720/1000], Loss: 0.5881\n","Epoch [722/1000], Loss: 0.8250\n","Epoch [724/1000], Loss: 0.7817\n","Epoch [726/1000], Loss: 0.9051\n","Epoch [728/1000], Loss: 0.8623\n","Epoch [730/1000], Loss: 0.6709\n","Epoch [732/1000], Loss: 0.7033\n","Epoch [734/1000], Loss: 0.7931\n","Epoch [736/1000], Loss: 0.6693\n","Epoch [738/1000], Loss: 0.7214\n","Epoch [740/1000], Loss: 0.7307\n","Epoch [742/1000], Loss: 0.8497\n","Epoch [744/1000], Loss: 0.7516\n","Epoch [746/1000], Loss: 0.7099\n","Epoch [748/1000], Loss: 0.8404\n","Epoch [750/1000], Loss: 0.6849\n","Epoch [752/1000], Loss: 0.7373\n","Epoch [754/1000], Loss: 1.0431\n","Epoch [756/1000], Loss: 0.7535\n","Epoch [758/1000], Loss: 0.8855\n","Epoch [760/1000], Loss: 1.0289\n","Epoch [762/1000], Loss: 0.8604\n","Epoch [764/1000], Loss: 0.5981\n","Epoch [766/1000], Loss: 0.5398\n","Epoch [768/1000], Loss: 0.8117\n","Epoch [770/1000], Loss: 0.8796\n","Epoch [772/1000], Loss: 0.9912\n","Epoch [774/1000], Loss: 0.7100\n","Epoch [776/1000], Loss: 0.6116\n","Epoch [778/1000], Loss: 1.2551\n","Epoch [780/1000], Loss: 0.6977\n","Epoch [782/1000], Loss: 0.6119\n","Epoch [784/1000], Loss: 0.9166\n","Epoch [786/1000], Loss: 1.0065\n","Epoch [788/1000], Loss: 0.7335\n","Epoch [790/1000], Loss: 0.7184\n","Epoch [792/1000], Loss: 0.4895\n","Epoch [794/1000], Loss: 0.5689\n","Epoch [796/1000], Loss: 1.0362\n","Epoch [798/1000], Loss: 1.0856\n","Epoch [800/1000], Loss: 0.6407\n","Epoch [802/1000], Loss: 0.8541\n","Epoch [804/1000], Loss: 0.7882\n","Epoch [806/1000], Loss: 0.6597\n","Epoch [808/1000], Loss: 0.8681\n","Epoch [810/1000], Loss: 0.7259\n","Epoch [812/1000], Loss: 0.8479\n","Epoch [814/1000], Loss: 0.7906\n","Epoch [816/1000], Loss: 0.7140\n","Epoch [818/1000], Loss: 0.8413\n","Epoch [820/1000], Loss: 0.8986\n","Epoch [822/1000], Loss: 0.4707\n","Epoch [824/1000], Loss: 0.9167\n","Epoch [826/1000], Loss: 0.7274\n","Epoch [828/1000], Loss: 0.7626\n","Epoch [830/1000], Loss: 0.8079\n","Epoch [832/1000], Loss: 0.7975\n","Epoch [834/1000], Loss: 0.8068\n","Epoch [836/1000], Loss: 0.6847\n","Epoch [838/1000], Loss: 0.6333\n","Epoch [840/1000], Loss: 0.6456\n","Epoch [842/1000], Loss: 0.8009\n","Epoch [844/1000], Loss: 1.1234\n","Epoch [846/1000], Loss: 0.8041\n","Epoch [848/1000], Loss: 0.8223\n","Epoch [850/1000], Loss: 0.5759\n","Epoch [852/1000], Loss: 0.5822\n","Epoch [854/1000], Loss: 0.6935\n","Epoch [856/1000], Loss: 0.6678\n","Epoch [858/1000], Loss: 0.7453\n","Epoch [860/1000], Loss: 0.9232\n","Epoch [862/1000], Loss: 0.9243\n","Epoch [864/1000], Loss: 1.2114\n","Epoch [866/1000], Loss: 0.7061\n","Epoch [868/1000], Loss: 0.9938\n","Epoch [870/1000], Loss: 0.8778\n","Epoch [872/1000], Loss: 0.9503\n","Epoch [874/1000], Loss: 0.8016\n","Epoch [876/1000], Loss: 0.7578\n","Epoch [878/1000], Loss: 0.5197\n","Epoch [880/1000], Loss: 0.5123\n","Epoch [882/1000], Loss: 0.6466\n","Epoch [884/1000], Loss: 0.7822\n","Epoch [886/1000], Loss: 0.9344\n","Epoch [888/1000], Loss: 0.7987\n","Epoch [890/1000], Loss: 0.7467\n","Epoch [892/1000], Loss: 0.8562\n","Epoch [894/1000], Loss: 0.9398\n","Epoch [896/1000], Loss: 0.7238\n","Epoch [898/1000], Loss: 0.9177\n","Epoch [900/1000], Loss: 0.8649\n","Epoch [902/1000], Loss: 0.8534\n","Epoch [904/1000], Loss: 0.9097\n","Epoch [906/1000], Loss: 0.6575\n","Epoch [908/1000], Loss: 0.9912\n","Epoch [910/1000], Loss: 1.0593\n","Epoch [912/1000], Loss: 0.6528\n","Epoch [914/1000], Loss: 1.2345\n","Epoch [916/1000], Loss: 0.9560\n","Epoch [918/1000], Loss: 0.6082\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 0.8133\n","Epoch [4/1000], Loss: 1.0036\n","Epoch [6/1000], Loss: 1.1130\n","Epoch [8/1000], Loss: 1.1518\n","Epoch [10/1000], Loss: 0.7553\n","Epoch [12/1000], Loss: 1.1125\n","Epoch [14/1000], Loss: 1.1266\n","Epoch [16/1000], Loss: 1.2870\n","Epoch [18/1000], Loss: 1.0416\n","Epoch [20/1000], Loss: 1.1334\n","Epoch [22/1000], Loss: 1.1754\n","Epoch [24/1000], Loss: 1.1330\n","Epoch [26/1000], Loss: 1.1094\n","Epoch [28/1000], Loss: 1.0505\n","Epoch [30/1000], Loss: 1.3332\n","Epoch [32/1000], Loss: 0.7394\n","Epoch [34/1000], Loss: 1.3042\n","Epoch [36/1000], Loss: 0.8420\n","Epoch [38/1000], Loss: 1.0370\n","Epoch [40/1000], Loss: 1.0188\n","Epoch [42/1000], Loss: 0.7801\n","Epoch [44/1000], Loss: 0.9016\n","Epoch [46/1000], Loss: 1.0810\n","Epoch [48/1000], Loss: 1.0282\n","Epoch [50/1000], Loss: 1.2137\n","Epoch [52/1000], Loss: 1.3052\n","Epoch [54/1000], Loss: 0.8434\n","Epoch [56/1000], Loss: 0.9238\n","Epoch [58/1000], Loss: 0.8991\n","Epoch [60/1000], Loss: 1.1821\n","Epoch [62/1000], Loss: 1.1920\n","Epoch [64/1000], Loss: 0.9617\n","Epoch [66/1000], Loss: 1.2272\n","Epoch [68/1000], Loss: 1.0961\n","Epoch [70/1000], Loss: 1.0358\n","Epoch [72/1000], Loss: 1.1274\n","Epoch [74/1000], Loss: 0.8762\n","Epoch [76/1000], Loss: 1.0169\n","Epoch [78/1000], Loss: 0.8923\n","Epoch [80/1000], Loss: 0.8180\n","Epoch [82/1000], Loss: 1.2154\n","Epoch [84/1000], Loss: 0.9181\n","Epoch [86/1000], Loss: 1.0708\n","Epoch [88/1000], Loss: 0.9932\n","Epoch [90/1000], Loss: 0.9762\n","Epoch [92/1000], Loss: 1.0840\n","Epoch [94/1000], Loss: 1.1894\n","Epoch [96/1000], Loss: 1.0539\n","Epoch [98/1000], Loss: 0.8992\n","Epoch [100/1000], Loss: 0.9333\n","Epoch [102/1000], Loss: 0.9614\n","Epoch [104/1000], Loss: 0.8112\n","Epoch [106/1000], Loss: 1.0978\n","Epoch [108/1000], Loss: 0.8867\n","Epoch [110/1000], Loss: 0.9625\n","Epoch [112/1000], Loss: 1.0923\n","Epoch [114/1000], Loss: 1.0852\n","Epoch [116/1000], Loss: 0.9959\n","Epoch [118/1000], Loss: 0.8560\n","Epoch [120/1000], Loss: 1.0230\n","Epoch [122/1000], Loss: 0.9816\n","Epoch [124/1000], Loss: 1.0976\n","Epoch [126/1000], Loss: 1.2357\n","Epoch [128/1000], Loss: 0.9480\n","Epoch [130/1000], Loss: 1.3377\n","Epoch [132/1000], Loss: 0.9729\n","Epoch [134/1000], Loss: 0.9389\n","Epoch [136/1000], Loss: 0.9487\n","Epoch [138/1000], Loss: 0.9638\n","Epoch [140/1000], Loss: 1.0852\n","Epoch [142/1000], Loss: 0.7957\n","Epoch [144/1000], Loss: 1.0722\n","Epoch [146/1000], Loss: 1.1233\n","Epoch [148/1000], Loss: 0.9134\n","Epoch [150/1000], Loss: 0.9297\n","Epoch [152/1000], Loss: 0.9110\n","Epoch [154/1000], Loss: 0.6744\n","Epoch [156/1000], Loss: 1.0348\n","Epoch [158/1000], Loss: 1.0721\n","Epoch [160/1000], Loss: 0.8385\n","Epoch [162/1000], Loss: 1.1248\n","Epoch [164/1000], Loss: 0.9199\n","Epoch [166/1000], Loss: 0.9717\n","Epoch [168/1000], Loss: 1.1688\n","Epoch [170/1000], Loss: 1.0350\n","Epoch [172/1000], Loss: 1.2503\n","Epoch [174/1000], Loss: 1.0601\n","Epoch [176/1000], Loss: 0.9737\n","Epoch [178/1000], Loss: 0.9919\n","Epoch [180/1000], Loss: 0.8618\n","Epoch [182/1000], Loss: 0.9807\n","Epoch [184/1000], Loss: 0.9178\n","Epoch [186/1000], Loss: 0.7684\n","Epoch [188/1000], Loss: 0.9109\n","Epoch [190/1000], Loss: 1.0856\n","Epoch [192/1000], Loss: 0.8882\n","Epoch [194/1000], Loss: 0.9206\n","Epoch [196/1000], Loss: 0.9930\n","Epoch [198/1000], Loss: 0.9114\n","Epoch [200/1000], Loss: 1.2042\n","Epoch [202/1000], Loss: 0.8825\n","Epoch [204/1000], Loss: 0.9604\n","Epoch [206/1000], Loss: 0.7535\n","Epoch [208/1000], Loss: 0.9815\n","Epoch [210/1000], Loss: 0.7886\n","Epoch [212/1000], Loss: 0.9120\n","Epoch [214/1000], Loss: 0.9859\n","Epoch [216/1000], Loss: 0.9605\n","Epoch [218/1000], Loss: 0.9951\n","Epoch [220/1000], Loss: 1.0995\n","Epoch [222/1000], Loss: 0.8879\n","Epoch [224/1000], Loss: 0.8746\n","Epoch [226/1000], Loss: 1.0628\n","Epoch [228/1000], Loss: 0.7528\n","Epoch [230/1000], Loss: 0.5759\n","Epoch [232/1000], Loss: 0.8005\n","Epoch [234/1000], Loss: 0.9396\n","Epoch [236/1000], Loss: 1.0244\n","Epoch [238/1000], Loss: 0.8571\n","Epoch [240/1000], Loss: 0.9351\n","Epoch [242/1000], Loss: 0.8284\n","Epoch [244/1000], Loss: 1.1734\n","Epoch [246/1000], Loss: 0.8876\n","Epoch [248/1000], Loss: 0.8632\n","Epoch [250/1000], Loss: 0.9277\n","Epoch [252/1000], Loss: 0.8759\n","Epoch [254/1000], Loss: 0.9566\n","Epoch [256/1000], Loss: 0.9094\n","Epoch [258/1000], Loss: 1.1933\n","Epoch [260/1000], Loss: 0.7972\n","Epoch [262/1000], Loss: 0.9152\n","Epoch [264/1000], Loss: 0.8314\n","Epoch [266/1000], Loss: 0.8208\n","Epoch [268/1000], Loss: 0.9396\n","Epoch [270/1000], Loss: 0.9364\n","Epoch [272/1000], Loss: 0.7528\n","Epoch [274/1000], Loss: 0.7174\n","Epoch [276/1000], Loss: 0.9855\n","Epoch [278/1000], Loss: 1.0274\n","Epoch [280/1000], Loss: 0.9655\n","Epoch [282/1000], Loss: 0.9556\n","Epoch [284/1000], Loss: 1.1605\n","Epoch [286/1000], Loss: 0.6643\n","Epoch [288/1000], Loss: 0.8471\n","Epoch [290/1000], Loss: 0.8208\n","Epoch [292/1000], Loss: 0.8638\n","Epoch [294/1000], Loss: 0.9652\n","Epoch [296/1000], Loss: 1.1164\n","Epoch [298/1000], Loss: 0.8558\n","Epoch [300/1000], Loss: 1.2680\n","Epoch [302/1000], Loss: 1.0396\n","Epoch [304/1000], Loss: 0.4901\n","Epoch [306/1000], Loss: 0.7955\n","Epoch [308/1000], Loss: 0.9874\n","Epoch [310/1000], Loss: 1.0492\n","Epoch [312/1000], Loss: 0.8276\n","Epoch [314/1000], Loss: 1.0664\n","Epoch [316/1000], Loss: 0.9621\n","Epoch [318/1000], Loss: 1.0660\n","Epoch [320/1000], Loss: 1.0659\n","Epoch [322/1000], Loss: 0.9638\n","Epoch [324/1000], Loss: 0.9306\n","Epoch [326/1000], Loss: 0.9302\n","Epoch [328/1000], Loss: 0.7944\n","Epoch [330/1000], Loss: 0.8020\n","Epoch [332/1000], Loss: 0.9036\n","Epoch [334/1000], Loss: 1.2591\n","Epoch [336/1000], Loss: 0.8496\n","Epoch [338/1000], Loss: 1.0274\n","Epoch [340/1000], Loss: 0.9119\n","Epoch [342/1000], Loss: 0.6612\n","Epoch [344/1000], Loss: 1.1434\n","Epoch [346/1000], Loss: 0.9483\n","Epoch [348/1000], Loss: 0.7304\n","Epoch [350/1000], Loss: 1.0947\n","Epoch [352/1000], Loss: 0.9960\n","Epoch [354/1000], Loss: 0.9098\n","Epoch [356/1000], Loss: 0.9928\n","Epoch [358/1000], Loss: 0.9554\n","Epoch [360/1000], Loss: 0.9672\n","Epoch [362/1000], Loss: 1.2103\n","Epoch [364/1000], Loss: 0.9742\n","Epoch [366/1000], Loss: 0.9972\n","Epoch [368/1000], Loss: 1.1821\n","Epoch [370/1000], Loss: 0.8893\n","Epoch [372/1000], Loss: 0.8163\n","Epoch [374/1000], Loss: 1.0104\n","Epoch [376/1000], Loss: 0.7599\n","Epoch [378/1000], Loss: 0.8380\n","Epoch [380/1000], Loss: 1.0242\n","Epoch [382/1000], Loss: 0.8824\n","Epoch [384/1000], Loss: 0.9896\n","Epoch [386/1000], Loss: 0.9837\n","Epoch [388/1000], Loss: 0.9094\n","Epoch [390/1000], Loss: 0.8431\n","Epoch [392/1000], Loss: 0.8473\n","Epoch [394/1000], Loss: 0.9604\n","Epoch [396/1000], Loss: 0.9655\n","Epoch [398/1000], Loss: 1.0750\n","Epoch [400/1000], Loss: 0.9869\n","Epoch [402/1000], Loss: 0.6719\n","Epoch [404/1000], Loss: 0.7988\n","Epoch [406/1000], Loss: 0.9978\n","Epoch [408/1000], Loss: 0.7960\n","Epoch [410/1000], Loss: 0.8909\n","Epoch [412/1000], Loss: 0.8795\n","Epoch [414/1000], Loss: 0.9861\n","Epoch [416/1000], Loss: 0.8296\n","Epoch [418/1000], Loss: 0.8158\n","Epoch [420/1000], Loss: 0.9676\n","Epoch [422/1000], Loss: 1.1683\n","Epoch [424/1000], Loss: 0.8118\n","Epoch [426/1000], Loss: 1.1761\n","Epoch [428/1000], Loss: 1.3141\n","Epoch [430/1000], Loss: 1.0620\n","Epoch [432/1000], Loss: 0.7677\n","Epoch [434/1000], Loss: 0.8220\n","Epoch [436/1000], Loss: 0.8818\n","Epoch [438/1000], Loss: 0.8524\n","Epoch [440/1000], Loss: 0.8996\n","Epoch [442/1000], Loss: 0.7572\n","Epoch [444/1000], Loss: 0.7589\n","Epoch [446/1000], Loss: 1.0474\n","Epoch [448/1000], Loss: 0.6033\n","Epoch [450/1000], Loss: 0.6798\n","Epoch [452/1000], Loss: 0.7647\n","Epoch [454/1000], Loss: 0.9943\n","Epoch [456/1000], Loss: 0.7994\n","Epoch [458/1000], Loss: 0.8335\n","Epoch [460/1000], Loss: 1.1577\n","Epoch [462/1000], Loss: 0.7893\n","Epoch [464/1000], Loss: 1.0754\n","Epoch [466/1000], Loss: 0.7304\n","Epoch [468/1000], Loss: 0.4479\n","Epoch [470/1000], Loss: 1.0836\n","Epoch [472/1000], Loss: 1.0200\n","Epoch [474/1000], Loss: 0.8056\n","Epoch [476/1000], Loss: 0.7429\n","Epoch [478/1000], Loss: 0.9639\n","Epoch [480/1000], Loss: 0.7362\n","Epoch [482/1000], Loss: 1.1233\n","Epoch [484/1000], Loss: 0.9719\n","Epoch [486/1000], Loss: 0.5842\n","Epoch [488/1000], Loss: 0.9255\n","Epoch [490/1000], Loss: 0.8283\n","Epoch [492/1000], Loss: 1.0535\n","Epoch [494/1000], Loss: 0.9927\n","Epoch [496/1000], Loss: 0.8071\n","Epoch [498/1000], Loss: 0.7034\n","Epoch [500/1000], Loss: 0.8427\n","Epoch [502/1000], Loss: 0.9063\n","Epoch [504/1000], Loss: 0.6127\n","Epoch [506/1000], Loss: 0.8987\n","Epoch [508/1000], Loss: 0.7748\n","Epoch [510/1000], Loss: 0.9997\n","Epoch [512/1000], Loss: 0.7951\n","Epoch [514/1000], Loss: 0.9836\n","Epoch [516/1000], Loss: 0.9655\n","Epoch [518/1000], Loss: 0.6286\n","Epoch [520/1000], Loss: 0.9793\n","Epoch [522/1000], Loss: 0.7777\n","Epoch [524/1000], Loss: 0.8704\n","Epoch [526/1000], Loss: 0.9127\n","Epoch [528/1000], Loss: 0.8265\n","Epoch [530/1000], Loss: 0.7625\n","Epoch [532/1000], Loss: 0.9017\n","Epoch [534/1000], Loss: 0.9049\n","Epoch [536/1000], Loss: 0.9443\n","Epoch [538/1000], Loss: 0.8946\n","Epoch [540/1000], Loss: 0.6266\n","Epoch [542/1000], Loss: 0.9460\n","Epoch [544/1000], Loss: 0.7731\n","Epoch [546/1000], Loss: 0.8536\n","Epoch [548/1000], Loss: 0.7292\n","Epoch [550/1000], Loss: 0.9026\n","Epoch [552/1000], Loss: 0.9627\n","Epoch [554/1000], Loss: 0.7528\n","Epoch [556/1000], Loss: 0.7120\n","Epoch [558/1000], Loss: 0.8455\n","Epoch [560/1000], Loss: 0.8648\n","Epoch [562/1000], Loss: 0.9138\n","Epoch [564/1000], Loss: 0.8461\n","Epoch [566/1000], Loss: 0.7608\n","Epoch [568/1000], Loss: 0.7006\n","Epoch [570/1000], Loss: 0.7908\n","Epoch [572/1000], Loss: 0.7445\n","Epoch [574/1000], Loss: 0.7292\n","Epoch [576/1000], Loss: 0.8696\n","Epoch [578/1000], Loss: 0.6639\n","Epoch [580/1000], Loss: 0.7417\n","Epoch [582/1000], Loss: 0.7452\n","Epoch [584/1000], Loss: 0.7326\n","Epoch [586/1000], Loss: 0.8665\n","Epoch [588/1000], Loss: 0.8416\n","Epoch [590/1000], Loss: 1.0373\n","Epoch [592/1000], Loss: 0.9218\n","Epoch [594/1000], Loss: 0.6897\n","Epoch [596/1000], Loss: 0.9319\n","Epoch [598/1000], Loss: 0.8944\n","Epoch [600/1000], Loss: 0.6550\n","Epoch [602/1000], Loss: 0.6348\n","Epoch [604/1000], Loss: 0.6802\n","Epoch [606/1000], Loss: 0.6033\n","Epoch [608/1000], Loss: 0.8414\n","Epoch [610/1000], Loss: 0.6314\n","Epoch [612/1000], Loss: 0.8028\n","Epoch [614/1000], Loss: 0.7530\n","Epoch [616/1000], Loss: 0.9105\n","Epoch [618/1000], Loss: 0.5960\n","Epoch [620/1000], Loss: 0.8561\n","Epoch [622/1000], Loss: 0.8393\n","Epoch [624/1000], Loss: 0.9171\n","Epoch [626/1000], Loss: 0.9353\n","Epoch [628/1000], Loss: 1.1500\n","Epoch [630/1000], Loss: 0.8784\n","Epoch [632/1000], Loss: 0.5926\n","Epoch [634/1000], Loss: 1.0762\n","Epoch [636/1000], Loss: 0.7213\n","Epoch [638/1000], Loss: 0.8072\n","Epoch [640/1000], Loss: 0.7577\n","Epoch [642/1000], Loss: 0.6148\n","Epoch [644/1000], Loss: 0.9628\n","Epoch [646/1000], Loss: 0.9647\n","Epoch [648/1000], Loss: 0.9297\n","Epoch [650/1000], Loss: 0.7014\n","Epoch [652/1000], Loss: 0.8192\n","Epoch [654/1000], Loss: 0.8908\n","Epoch [656/1000], Loss: 0.7742\n","Epoch [658/1000], Loss: 0.7769\n","Epoch [660/1000], Loss: 0.7459\n","Epoch [662/1000], Loss: 0.8419\n","Epoch [664/1000], Loss: 1.2144\n","Epoch [666/1000], Loss: 0.9510\n","Epoch [668/1000], Loss: 1.2218\n","Epoch [670/1000], Loss: 0.6787\n","Epoch [672/1000], Loss: 0.7998\n","Epoch [674/1000], Loss: 0.6353\n","Epoch [676/1000], Loss: 0.7797\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.0170\n","Epoch [4/1000], Loss: 1.1981\n","Epoch [6/1000], Loss: 0.8903\n","Epoch [8/1000], Loss: 0.8309\n","Epoch [10/1000], Loss: 1.0585\n","Epoch [12/1000], Loss: 1.1187\n","Epoch [14/1000], Loss: 0.9119\n","Epoch [16/1000], Loss: 0.7936\n","Epoch [18/1000], Loss: 0.9848\n","Epoch [20/1000], Loss: 1.1432\n","Epoch [22/1000], Loss: 0.8603\n","Epoch [24/1000], Loss: 0.8948\n","Epoch [26/1000], Loss: 1.1297\n","Epoch [28/1000], Loss: 1.3461\n","Epoch [30/1000], Loss: 1.0138\n","Epoch [32/1000], Loss: 0.8834\n","Epoch [34/1000], Loss: 0.8565\n","Epoch [36/1000], Loss: 1.2221\n","Epoch [38/1000], Loss: 0.9198\n","Epoch [40/1000], Loss: 0.7683\n","Epoch [42/1000], Loss: 1.0569\n","Epoch [44/1000], Loss: 1.1398\n","Epoch [46/1000], Loss: 0.8852\n","Epoch [48/1000], Loss: 0.9752\n","Epoch [50/1000], Loss: 0.9670\n","Epoch [52/1000], Loss: 0.9619\n","Epoch [54/1000], Loss: 0.6948\n","Epoch [56/1000], Loss: 0.9033\n","Epoch [58/1000], Loss: 1.0278\n","Epoch [60/1000], Loss: 1.0584\n","Epoch [62/1000], Loss: 1.2142\n","Epoch [64/1000], Loss: 1.2607\n","Epoch [66/1000], Loss: 1.3514\n","Epoch [68/1000], Loss: 1.0130\n","Epoch [70/1000], Loss: 0.8230\n","Epoch [72/1000], Loss: 1.0253\n","Epoch [74/1000], Loss: 1.0616\n","Epoch [76/1000], Loss: 1.1876\n","Epoch [78/1000], Loss: 0.8123\n","Epoch [80/1000], Loss: 0.9475\n","Epoch [82/1000], Loss: 0.8407\n","Epoch [84/1000], Loss: 1.1778\n","Epoch [86/1000], Loss: 1.4324\n","Epoch [88/1000], Loss: 0.9134\n","Epoch [90/1000], Loss: 1.0139\n","Epoch [92/1000], Loss: 0.8729\n","Epoch [94/1000], Loss: 0.7299\n","Epoch [96/1000], Loss: 0.7617\n","Epoch [98/1000], Loss: 0.9402\n","Epoch [100/1000], Loss: 1.0935\n","Epoch [102/1000], Loss: 1.1471\n","Epoch [104/1000], Loss: 0.7665\n","Epoch [106/1000], Loss: 0.8291\n","Epoch [108/1000], Loss: 0.8247\n","Epoch [110/1000], Loss: 1.2209\n","Epoch [112/1000], Loss: 1.2891\n","Epoch [114/1000], Loss: 0.8913\n","Epoch [116/1000], Loss: 0.9808\n","Epoch [118/1000], Loss: 1.1303\n","Epoch [120/1000], Loss: 1.1840\n","Epoch [122/1000], Loss: 0.9601\n","Epoch [124/1000], Loss: 0.7974\n","Epoch [126/1000], Loss: 0.8976\n","Epoch [128/1000], Loss: 0.9528\n","Epoch [130/1000], Loss: 1.2252\n","Epoch [132/1000], Loss: 0.8273\n","Epoch [134/1000], Loss: 1.0425\n","Epoch [136/1000], Loss: 0.9489\n","Epoch [138/1000], Loss: 1.2085\n","Epoch [140/1000], Loss: 1.1026\n","Epoch [142/1000], Loss: 1.1245\n","Epoch [144/1000], Loss: 0.8528\n","Epoch [146/1000], Loss: 0.7656\n","Epoch [148/1000], Loss: 0.9555\n","Epoch [150/1000], Loss: 1.2564\n","Epoch [152/1000], Loss: 1.1599\n","Epoch [154/1000], Loss: 1.0205\n","Epoch [156/1000], Loss: 0.7612\n","Epoch [158/1000], Loss: 0.9273\n","Epoch [160/1000], Loss: 1.0645\n","Epoch [162/1000], Loss: 0.8770\n","Epoch [164/1000], Loss: 0.7058\n","Epoch [166/1000], Loss: 0.8486\n","Epoch [168/1000], Loss: 1.0001\n","Epoch [170/1000], Loss: 1.2341\n","Epoch [172/1000], Loss: 0.9529\n","Epoch [174/1000], Loss: 0.8784\n","Epoch [176/1000], Loss: 1.2152\n","Epoch [178/1000], Loss: 1.1322\n","Epoch [180/1000], Loss: 0.8307\n","Epoch [182/1000], Loss: 0.9589\n","Epoch [184/1000], Loss: 0.9132\n","Epoch [186/1000], Loss: 1.0406\n","Epoch [188/1000], Loss: 0.7532\n","Epoch [190/1000], Loss: 1.0058\n","Epoch [192/1000], Loss: 0.9974\n","Epoch [194/1000], Loss: 1.0618\n","Epoch [196/1000], Loss: 0.9392\n","Epoch [198/1000], Loss: 0.9623\n","Epoch [200/1000], Loss: 1.0917\n","Epoch [202/1000], Loss: 0.9341\n","Epoch [204/1000], Loss: 1.0198\n","Epoch [206/1000], Loss: 1.0161\n","Epoch [208/1000], Loss: 1.1463\n","Epoch [210/1000], Loss: 0.7153\n","Epoch [212/1000], Loss: 0.9084\n","Epoch [214/1000], Loss: 0.7251\n","Epoch [216/1000], Loss: 0.7933\n","Epoch [218/1000], Loss: 0.7355\n","Epoch [220/1000], Loss: 0.7191\n","Epoch [222/1000], Loss: 0.6400\n","Epoch [224/1000], Loss: 0.7081\n","Epoch [226/1000], Loss: 0.7996\n","Epoch [228/1000], Loss: 0.9179\n","Epoch [230/1000], Loss: 1.0065\n","Epoch [232/1000], Loss: 1.0527\n","Epoch [234/1000], Loss: 1.0029\n","Epoch [236/1000], Loss: 0.9293\n","Epoch [238/1000], Loss: 0.8454\n","Epoch [240/1000], Loss: 0.8326\n","Epoch [242/1000], Loss: 0.7735\n","Epoch [244/1000], Loss: 0.6397\n","Epoch [246/1000], Loss: 0.6508\n","Epoch [248/1000], Loss: 1.0443\n","Epoch [250/1000], Loss: 0.7560\n","Epoch [252/1000], Loss: 0.7913\n","Epoch [254/1000], Loss: 1.0836\n","Epoch [256/1000], Loss: 1.1491\n","Epoch [258/1000], Loss: 0.9497\n","Epoch [260/1000], Loss: 0.7780\n","Epoch [262/1000], Loss: 0.6171\n","Epoch [264/1000], Loss: 0.9403\n","Epoch [266/1000], Loss: 1.1571\n","Epoch [268/1000], Loss: 0.9764\n","Epoch [270/1000], Loss: 1.0654\n","Epoch [272/1000], Loss: 0.6063\n","Epoch [274/1000], Loss: 0.8597\n","Epoch [276/1000], Loss: 0.9001\n","Epoch [278/1000], Loss: 0.8993\n","Epoch [280/1000], Loss: 0.9617\n","Epoch [282/1000], Loss: 1.0296\n","Epoch [284/1000], Loss: 0.8353\n","Epoch [286/1000], Loss: 0.9902\n","Epoch [288/1000], Loss: 1.0126\n","Epoch [290/1000], Loss: 0.7293\n","Epoch [292/1000], Loss: 1.0001\n","Epoch [294/1000], Loss: 0.8696\n","Epoch [296/1000], Loss: 0.9062\n","Epoch [298/1000], Loss: 1.1815\n","Epoch [300/1000], Loss: 0.7382\n","Epoch [302/1000], Loss: 1.0751\n","Epoch [304/1000], Loss: 0.8889\n","Epoch [306/1000], Loss: 0.9959\n","Epoch [308/1000], Loss: 1.0553\n","Epoch [310/1000], Loss: 0.8871\n","Epoch [312/1000], Loss: 0.6639\n","Epoch [314/1000], Loss: 0.8217\n","Epoch [316/1000], Loss: 1.1086\n","Epoch [318/1000], Loss: 0.7305\n","Epoch [320/1000], Loss: 1.0175\n","Epoch [322/1000], Loss: 0.9273\n","Epoch [324/1000], Loss: 1.0674\n","Epoch [326/1000], Loss: 0.7503\n","Epoch [328/1000], Loss: 1.2808\n","Epoch [330/1000], Loss: 1.0621\n","Epoch [332/1000], Loss: 1.0713\n","Epoch [334/1000], Loss: 1.0429\n","Epoch [336/1000], Loss: 0.7636\n","Epoch [338/1000], Loss: 0.8937\n","Epoch [340/1000], Loss: 0.7959\n","Epoch [342/1000], Loss: 1.0285\n","Epoch [344/1000], Loss: 0.8701\n","Epoch [346/1000], Loss: 0.6429\n","Epoch [348/1000], Loss: 1.0281\n","Epoch [350/1000], Loss: 1.0394\n","Epoch [352/1000], Loss: 0.8692\n","Epoch [354/1000], Loss: 0.7052\n","Epoch [356/1000], Loss: 1.0138\n","Epoch [358/1000], Loss: 1.1774\n","Epoch [360/1000], Loss: 0.9584\n","Epoch [362/1000], Loss: 0.6721\n","Epoch [364/1000], Loss: 1.0259\n","Epoch [366/1000], Loss: 0.9907\n","Epoch [368/1000], Loss: 1.1492\n","Epoch [370/1000], Loss: 1.1498\n","Epoch [372/1000], Loss: 0.9609\n","Epoch [374/1000], Loss: 0.8272\n","Epoch [376/1000], Loss: 0.6956\n","Epoch [378/1000], Loss: 0.9381\n","Epoch [380/1000], Loss: 0.9748\n","Epoch [382/1000], Loss: 1.1979\n","Epoch [384/1000], Loss: 0.9071\n","Epoch [386/1000], Loss: 0.9736\n","Epoch [388/1000], Loss: 0.9908\n","Epoch [390/1000], Loss: 0.7873\n","Epoch [392/1000], Loss: 0.9211\n","Epoch [394/1000], Loss: 0.6890\n","Epoch [396/1000], Loss: 0.8647\n","Epoch [398/1000], Loss: 0.9776\n","Epoch [400/1000], Loss: 0.8306\n","Epoch [402/1000], Loss: 0.7177\n","Epoch [404/1000], Loss: 0.8272\n","Epoch [406/1000], Loss: 0.7289\n","Epoch [408/1000], Loss: 0.6414\n","Epoch [410/1000], Loss: 0.8653\n","Epoch [412/1000], Loss: 0.9933\n","Epoch [414/1000], Loss: 0.7815\n","Epoch [416/1000], Loss: 0.9345\n","Epoch [418/1000], Loss: 0.8680\n","Epoch [420/1000], Loss: 0.9269\n","Epoch [422/1000], Loss: 0.9238\n","Epoch [424/1000], Loss: 0.7126\n","Epoch [426/1000], Loss: 0.7792\n","Epoch [428/1000], Loss: 0.9090\n","Epoch [430/1000], Loss: 0.8690\n","Epoch [432/1000], Loss: 0.6889\n","Epoch [434/1000], Loss: 0.8339\n","Epoch [436/1000], Loss: 1.0826\n","Epoch [438/1000], Loss: 0.9364\n","Epoch [440/1000], Loss: 0.8235\n","Epoch [442/1000], Loss: 0.8674\n","Epoch [444/1000], Loss: 0.6812\n","Epoch [446/1000], Loss: 0.6481\n","Epoch [448/1000], Loss: 0.7476\n","Epoch [450/1000], Loss: 0.7307\n","Epoch [452/1000], Loss: 0.8134\n","Epoch [454/1000], Loss: 0.6544\n","Epoch [456/1000], Loss: 0.9196\n","Epoch [458/1000], Loss: 0.7144\n","Epoch [460/1000], Loss: 0.5485\n","Epoch [462/1000], Loss: 0.7406\n","Epoch [464/1000], Loss: 0.7623\n","Epoch [466/1000], Loss: 0.7631\n","Epoch [468/1000], Loss: 1.0973\n","Epoch [470/1000], Loss: 0.8251\n","Epoch [472/1000], Loss: 0.7712\n","Epoch [474/1000], Loss: 1.0124\n","Epoch [476/1000], Loss: 0.7051\n","Epoch [478/1000], Loss: 1.1635\n","Epoch [480/1000], Loss: 0.7811\n","Epoch [482/1000], Loss: 0.9555\n","Epoch [484/1000], Loss: 0.8566\n","Epoch [486/1000], Loss: 0.9941\n","Epoch [488/1000], Loss: 1.0822\n","Epoch [490/1000], Loss: 0.7771\n","Epoch [492/1000], Loss: 0.7322\n","Epoch [494/1000], Loss: 0.8393\n","Epoch [496/1000], Loss: 0.8172\n","Epoch [498/1000], Loss: 0.9689\n","Epoch [500/1000], Loss: 0.5952\n","Epoch [502/1000], Loss: 1.1004\n","Epoch [504/1000], Loss: 0.9461\n","Epoch [506/1000], Loss: 0.7696\n","Epoch [508/1000], Loss: 0.7015\n","Epoch [510/1000], Loss: 1.0412\n","Epoch [512/1000], Loss: 0.6922\n","Epoch [514/1000], Loss: 0.7615\n","Epoch [516/1000], Loss: 0.9911\n","Epoch [518/1000], Loss: 0.5472\n","Epoch [520/1000], Loss: 0.6912\n","Epoch [522/1000], Loss: 0.7468\n","Epoch [524/1000], Loss: 0.9406\n","Epoch [526/1000], Loss: 0.9124\n","Epoch [528/1000], Loss: 1.0563\n","Epoch [530/1000], Loss: 0.8799\n","Epoch [532/1000], Loss: 0.9986\n","Epoch [534/1000], Loss: 0.8849\n","Epoch [536/1000], Loss: 0.7007\n","Epoch [538/1000], Loss: 0.9798\n","Epoch [540/1000], Loss: 0.8769\n","Epoch [542/1000], Loss: 0.8070\n","Epoch [544/1000], Loss: 0.7357\n","Epoch [546/1000], Loss: 0.7458\n","Epoch [548/1000], Loss: 0.8149\n","Epoch [550/1000], Loss: 0.7272\n","Epoch [552/1000], Loss: 0.9519\n","Epoch [554/1000], Loss: 0.6292\n","Epoch [556/1000], Loss: 0.6883\n","Epoch [558/1000], Loss: 0.9072\n","Epoch [560/1000], Loss: 0.8353\n","Epoch [562/1000], Loss: 0.7731\n","Epoch [564/1000], Loss: 1.0024\n","Epoch [566/1000], Loss: 0.7247\n","Epoch [568/1000], Loss: 0.8577\n","Epoch [570/1000], Loss: 0.9031\n","Epoch [572/1000], Loss: 0.8616\n","Epoch [574/1000], Loss: 1.0532\n","Epoch [576/1000], Loss: 0.7539\n","Epoch [578/1000], Loss: 0.9343\n","Epoch [580/1000], Loss: 0.7526\n","Epoch [582/1000], Loss: 0.6992\n","Epoch [584/1000], Loss: 0.7831\n","Epoch [586/1000], Loss: 1.0286\n","Epoch [588/1000], Loss: 1.1045\n","Epoch [590/1000], Loss: 0.8611\n","Epoch [592/1000], Loss: 0.9125\n","Epoch [594/1000], Loss: 0.9233\n","Epoch [596/1000], Loss: 0.9014\n","Epoch [598/1000], Loss: 0.9300\n","Epoch [600/1000], Loss: 0.5216\n","Epoch [602/1000], Loss: 0.6837\n","Epoch [604/1000], Loss: 0.8664\n","Epoch [606/1000], Loss: 0.7919\n","Epoch [608/1000], Loss: 0.8832\n","Epoch [610/1000], Loss: 0.8603\n","Epoch [612/1000], Loss: 0.8101\n","Epoch [614/1000], Loss: 0.4978\n","Epoch [616/1000], Loss: 0.7762\n","Epoch [618/1000], Loss: 0.8783\n","Epoch [620/1000], Loss: 0.7861\n","Epoch [622/1000], Loss: 0.8867\n","Epoch [624/1000], Loss: 0.8985\n","Epoch [626/1000], Loss: 0.8117\n","Epoch [628/1000], Loss: 0.6758\n","Epoch [630/1000], Loss: 0.6534\n","Epoch [632/1000], Loss: 0.9805\n","Epoch [634/1000], Loss: 0.7461\n","Epoch [636/1000], Loss: 0.8526\n","Epoch [638/1000], Loss: 0.7061\n","Epoch [640/1000], Loss: 1.0305\n","Epoch [642/1000], Loss: 0.6418\n","Epoch [644/1000], Loss: 0.6478\n","Epoch [646/1000], Loss: 0.9139\n","Epoch [648/1000], Loss: 0.6277\n","Epoch [650/1000], Loss: 0.8466\n","Epoch [652/1000], Loss: 0.8227\n","Epoch [654/1000], Loss: 1.0721\n","Epoch [656/1000], Loss: 0.8972\n","Epoch [658/1000], Loss: 0.7700\n","Epoch [660/1000], Loss: 0.9869\n","Epoch [662/1000], Loss: 1.0220\n","Epoch [664/1000], Loss: 0.8776\n","Epoch [666/1000], Loss: 0.7406\n","Epoch [668/1000], Loss: 1.0383\n","Epoch [670/1000], Loss: 0.6309\n","Epoch [672/1000], Loss: 0.9377\n","Epoch [674/1000], Loss: 0.8763\n","Epoch [676/1000], Loss: 1.0143\n","Epoch [678/1000], Loss: 0.6958\n","Epoch [680/1000], Loss: 1.0516\n","Epoch [682/1000], Loss: 0.8463\n","Epoch [684/1000], Loss: 0.7259\n","Epoch [686/1000], Loss: 0.8880\n","Epoch [688/1000], Loss: 0.7879\n","Epoch [690/1000], Loss: 0.8144\n","Epoch [692/1000], Loss: 0.8827\n","Epoch [694/1000], Loss: 0.9518\n","Epoch [696/1000], Loss: 0.6294\n","Epoch [698/1000], Loss: 0.8742\n","Epoch [700/1000], Loss: 0.5768\n","Epoch [702/1000], Loss: 1.0857\n","Epoch [704/1000], Loss: 0.9170\n","Epoch [706/1000], Loss: 0.9220\n","Epoch [708/1000], Loss: 1.0756\n","Epoch [710/1000], Loss: 0.8686\n","Epoch [712/1000], Loss: 1.0305\n","Epoch [714/1000], Loss: 0.9989\n","Epoch [716/1000], Loss: 0.7569\n","Epoch [718/1000], Loss: 0.7572\n","Epoch [720/1000], Loss: 0.7304\n","Epoch [722/1000], Loss: 0.8454\n","Epoch [724/1000], Loss: 0.5282\n","Epoch [726/1000], Loss: 0.9641\n","Epoch [728/1000], Loss: 0.5804\n","Epoch [730/1000], Loss: 0.6255\n","Epoch [732/1000], Loss: 0.7650\n","Epoch [734/1000], Loss: 0.7248\n","Epoch [736/1000], Loss: 0.7385\n","Epoch [738/1000], Loss: 0.7759\n","Epoch [740/1000], Loss: 0.7146\n","Epoch [742/1000], Loss: 0.5838\n","Epoch [744/1000], Loss: 0.7962\n","Epoch [746/1000], Loss: 0.9144\n","Epoch [748/1000], Loss: 0.8488\n","Epoch [750/1000], Loss: 0.7465\n","Epoch [752/1000], Loss: 0.7938\n","Epoch [754/1000], Loss: 0.8661\n","Epoch [756/1000], Loss: 0.8204\n","Epoch [758/1000], Loss: 0.7489\n","Epoch [760/1000], Loss: 1.2127\n","Epoch [762/1000], Loss: 1.1676\n","Epoch [764/1000], Loss: 0.6948\n","Epoch [766/1000], Loss: 0.8571\n","Epoch [768/1000], Loss: 0.7095\n","Epoch [770/1000], Loss: 1.0128\n","Epoch [772/1000], Loss: 0.9751\n","Epoch [774/1000], Loss: 0.9419\n","Epoch [776/1000], Loss: 0.9604\n","Epoch [778/1000], Loss: 0.8104\n","Epoch [780/1000], Loss: 0.5716\n","Epoch [782/1000], Loss: 0.7995\n","Epoch [784/1000], Loss: 0.6563\n","Epoch [786/1000], Loss: 0.6264\n","Epoch [788/1000], Loss: 0.6340\n","Epoch [790/1000], Loss: 0.6883\n","Epoch [792/1000], Loss: 0.7592\n","Epoch [794/1000], Loss: 0.8009\n","Epoch [796/1000], Loss: 0.6717\n","Epoch [798/1000], Loss: 0.9930\n","Epoch [800/1000], Loss: 0.9408\n","Epoch [802/1000], Loss: 0.7359\n","Epoch [804/1000], Loss: 0.8177\n","Epoch [806/1000], Loss: 0.9096\n","Epoch [808/1000], Loss: 0.8804\n","Epoch [810/1000], Loss: 1.0788\n","Epoch [812/1000], Loss: 0.7447\n","Epoch [814/1000], Loss: 0.8701\n","Epoch [816/1000], Loss: 0.7285\n","Epoch [818/1000], Loss: 0.7373\n","Epoch [820/1000], Loss: 0.7446\n","Epoch [822/1000], Loss: 0.7914\n","Epoch [824/1000], Loss: 0.8085\n","Epoch [826/1000], Loss: 1.1146\n","Epoch [828/1000], Loss: 0.6413\n","Epoch [830/1000], Loss: 0.5949\n","Epoch [832/1000], Loss: 0.7029\n","Epoch [834/1000], Loss: 0.9840\n","Epoch [836/1000], Loss: 0.5378\n","Epoch [838/1000], Loss: 0.6222\n","Epoch [840/1000], Loss: 0.8773\n","Epoch [842/1000], Loss: 0.7862\n","Epoch [844/1000], Loss: 0.9928\n","Epoch [846/1000], Loss: 0.7705\n","Epoch [848/1000], Loss: 0.6370\n","Epoch [850/1000], Loss: 0.9657\n","Epoch [852/1000], Loss: 0.9846\n","Epoch [854/1000], Loss: 0.8454\n","Epoch [856/1000], Loss: 0.6256\n","Epoch [858/1000], Loss: 0.9410\n","Epoch [860/1000], Loss: 0.6944\n","Epoch [862/1000], Loss: 0.8856\n","Epoch [864/1000], Loss: 0.6167\n","Epoch [866/1000], Loss: 0.7224\n","Epoch [868/1000], Loss: 0.9972\n","Epoch [870/1000], Loss: 1.0151\n","Epoch [872/1000], Loss: 0.7259\n","Epoch [874/1000], Loss: 0.9530\n","Epoch [876/1000], Loss: 0.9298\n","Epoch [878/1000], Loss: 0.6478\n","Epoch [880/1000], Loss: 0.9096\n","Epoch [882/1000], Loss: 0.8051\n","Epoch [884/1000], Loss: 0.7380\n","Epoch [886/1000], Loss: 0.8831\n","Epoch [888/1000], Loss: 0.8196\n","Epoch [890/1000], Loss: 0.6653\n","Epoch [892/1000], Loss: 0.8489\n","Epoch [894/1000], Loss: 1.0776\n","Epoch [896/1000], Loss: 0.9631\n","Epoch [898/1000], Loss: 1.0540\n","Epoch [900/1000], Loss: 0.4572\n","Epoch [902/1000], Loss: 0.8122\n","Epoch [904/1000], Loss: 0.9404\n","Epoch [906/1000], Loss: 0.7661\n","Epoch [908/1000], Loss: 0.7421\n","Epoch [910/1000], Loss: 0.6573\n","Epoch [912/1000], Loss: 0.8581\n","Epoch [914/1000], Loss: 0.9831\n","Epoch [916/1000], Loss: 0.7977\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.1270\n","Epoch [4/1000], Loss: 1.0884\n","Epoch [6/1000], Loss: 0.8813\n","Epoch [8/1000], Loss: 0.8946\n","Epoch [10/1000], Loss: 0.9788\n","Epoch [12/1000], Loss: 1.0009\n","Epoch [14/1000], Loss: 0.9677\n","Epoch [16/1000], Loss: 1.0367\n","Epoch [18/1000], Loss: 1.2744\n","Epoch [20/1000], Loss: 0.9364\n","Epoch [22/1000], Loss: 1.1149\n","Epoch [24/1000], Loss: 0.8624\n","Epoch [26/1000], Loss: 1.0142\n","Epoch [28/1000], Loss: 0.9148\n","Epoch [30/1000], Loss: 0.6594\n","Epoch [32/1000], Loss: 0.9354\n","Epoch [34/1000], Loss: 0.8846\n","Epoch [36/1000], Loss: 1.0177\n","Epoch [38/1000], Loss: 0.8297\n","Epoch [40/1000], Loss: 1.2293\n","Epoch [42/1000], Loss: 1.0970\n","patience exceeded, loading best model\n","Epoch [2/1000], Loss: 1.0717\n","Epoch [4/1000], Loss: 1.0809\n","Epoch [6/1000], Loss: 1.0826\n","Epoch [8/1000], Loss: 1.1131\n","Epoch [10/1000], Loss: 1.3702\n","Epoch [12/1000], Loss: 0.9522\n","Epoch [14/1000], Loss: 0.9210\n","Epoch [16/1000], Loss: 1.0186\n","Epoch [18/1000], Loss: 1.0457\n","Epoch [20/1000], Loss: 0.9286\n","Epoch [22/1000], Loss: 1.0669\n","Epoch [24/1000], Loss: 0.8851\n","Epoch [26/1000], Loss: 0.9048\n","Epoch [28/1000], Loss: 0.8056\n","Epoch [30/1000], Loss: 1.2067\n","Epoch [32/1000], Loss: 1.1901\n","Epoch [34/1000], Loss: 1.0076\n","Epoch [36/1000], Loss: 0.9818\n","Epoch [38/1000], Loss: 0.6572\n","Epoch [40/1000], Loss: 1.1352\n","Epoch [42/1000], Loss: 0.9521\n","Epoch [44/1000], Loss: 0.8591\n","Epoch [46/1000], Loss: 1.1853\n","Epoch [48/1000], Loss: 0.9059\n","Epoch [50/1000], Loss: 1.0524\n","Epoch [52/1000], Loss: 0.8629\n","Epoch [54/1000], Loss: 1.0357\n","Epoch [56/1000], Loss: 1.0860\n","Epoch [58/1000], Loss: 0.8532\n","Epoch [60/1000], Loss: 0.9945\n","Epoch [62/1000], Loss: 0.9508\n","Epoch [64/1000], Loss: 0.8658\n","Epoch [66/1000], Loss: 1.0420\n","Epoch [68/1000], Loss: 0.8399\n","Epoch [70/1000], Loss: 0.9823\n","Epoch [72/1000], Loss: 0.8670\n","Epoch [74/1000], Loss: 1.1356\n","Epoch [76/1000], Loss: 0.9632\n","Epoch [78/1000], Loss: 0.9413\n","Epoch [80/1000], Loss: 1.0532\n","Epoch [82/1000], Loss: 0.9723\n","Epoch [84/1000], Loss: 1.1146\n","Epoch [86/1000], Loss: 0.9780\n","Epoch [88/1000], Loss: 0.9963\n","Epoch [90/1000], Loss: 0.8823\n","Epoch [92/1000], Loss: 0.9820\n","Epoch [94/1000], Loss: 1.0133\n","Epoch [96/1000], Loss: 1.2169\n","Epoch [98/1000], Loss: 1.1305\n","Epoch [100/1000], Loss: 0.9160\n","Epoch [102/1000], Loss: 1.0007\n","Epoch [104/1000], Loss: 1.0532\n","Epoch [106/1000], Loss: 1.0678\n","Epoch [108/1000], Loss: 1.0608\n","Epoch [110/1000], Loss: 0.9411\n","Epoch [112/1000], Loss: 0.9874\n","Epoch [114/1000], Loss: 1.1009\n","Epoch [116/1000], Loss: 1.0426\n","Epoch [118/1000], Loss: 1.0491\n","Epoch [120/1000], Loss: 0.9833\n","Epoch [122/1000], Loss: 0.8932\n","Epoch [124/1000], Loss: 1.0528\n","Epoch [126/1000], Loss: 1.0956\n","Epoch [128/1000], Loss: 1.2048\n","Epoch [130/1000], Loss: 1.2150\n","Epoch [132/1000], Loss: 1.1460\n","Epoch [134/1000], Loss: 1.0471\n","Epoch [136/1000], Loss: 0.9258\n","Epoch [138/1000], Loss: 1.1476\n","Epoch [140/1000], Loss: 0.7250\n","Epoch [142/1000], Loss: 0.8412\n","Epoch [144/1000], Loss: 1.1112\n","Epoch [146/1000], Loss: 1.1533\n","Epoch [148/1000], Loss: 0.9625\n","Epoch [150/1000], Loss: 0.7313\n","Epoch [152/1000], Loss: 1.1047\n","Epoch [154/1000], Loss: 0.8341\n","Epoch [156/1000], Loss: 0.9309\n","Epoch [158/1000], Loss: 0.8789\n","Epoch [160/1000], Loss: 1.4237\n","Epoch [162/1000], Loss: 0.9988\n","Epoch [164/1000], Loss: 0.7383\n","Epoch [166/1000], Loss: 0.8628\n","Epoch [168/1000], Loss: 1.1687\n","Epoch [170/1000], Loss: 1.0658\n","Epoch [172/1000], Loss: 1.1696\n","Epoch [174/1000], Loss: 0.7405\n","Epoch [176/1000], Loss: 0.8391\n","Epoch [178/1000], Loss: 0.9489\n","Epoch [180/1000], Loss: 0.8527\n","Epoch [182/1000], Loss: 0.8637\n","Epoch [184/1000], Loss: 1.0845\n","Epoch [186/1000], Loss: 1.0321\n","Epoch [188/1000], Loss: 1.0193\n","Epoch [190/1000], Loss: 0.7728\n","Epoch [192/1000], Loss: 0.7756\n","Epoch [194/1000], Loss: 1.1510\n","Epoch [196/1000], Loss: 0.9370\n","Epoch [198/1000], Loss: 0.9631\n","Epoch [200/1000], Loss: 0.8873\n","Epoch [202/1000], Loss: 0.9774\n","Epoch [204/1000], Loss: 0.9932\n","Epoch [206/1000], Loss: 0.7540\n","Epoch [208/1000], Loss: 0.8773\n","Epoch [210/1000], Loss: 0.8437\n","Epoch [212/1000], Loss: 1.0451\n","Epoch [214/1000], Loss: 1.0999\n","Epoch [216/1000], Loss: 1.0382\n","Epoch [218/1000], Loss: 1.1642\n","Epoch [220/1000], Loss: 1.0062\n","Epoch [222/1000], Loss: 0.9773\n","Epoch [224/1000], Loss: 0.8712\n","Epoch [226/1000], Loss: 0.9035\n","Epoch [228/1000], Loss: 1.0841\n","Epoch [230/1000], Loss: 0.8446\n","Epoch [232/1000], Loss: 0.9662\n","Epoch [234/1000], Loss: 0.6716\n","Epoch [236/1000], Loss: 0.8134\n","Epoch [238/1000], Loss: 0.7786\n","Epoch [240/1000], Loss: 1.0655\n","Epoch [242/1000], Loss: 0.8012\n","Epoch [244/1000], Loss: 0.8744\n","Epoch [246/1000], Loss: 0.8072\n","Epoch [248/1000], Loss: 0.8723\n","Epoch [250/1000], Loss: 0.7299\n","Epoch [252/1000], Loss: 0.9445\n","Epoch [254/1000], Loss: 0.9876\n","Epoch [256/1000], Loss: 0.8388\n","Epoch [258/1000], Loss: 0.9800\n","Epoch [260/1000], Loss: 1.2480\n","Epoch [262/1000], Loss: 0.8255\n","Epoch [264/1000], Loss: 1.1389\n","Epoch [266/1000], Loss: 0.8712\n","Epoch [268/1000], Loss: 0.8108\n","Epoch [270/1000], Loss: 0.6730\n","Epoch [272/1000], Loss: 0.9448\n","Epoch [274/1000], Loss: 0.7536\n","Epoch [276/1000], Loss: 1.0406\n","Epoch [278/1000], Loss: 0.8434\n","Epoch [280/1000], Loss: 0.8854\n","Epoch [282/1000], Loss: 0.6422\n","Epoch [284/1000], Loss: 0.9121\n","Epoch [286/1000], Loss: 0.9887\n","Epoch [288/1000], Loss: 0.8157\n","Epoch [290/1000], Loss: 0.9230\n","Epoch [292/1000], Loss: 1.0182\n","Epoch [294/1000], Loss: 1.0663\n","Epoch [296/1000], Loss: 0.8147\n","Epoch [298/1000], Loss: 0.8760\n","Epoch [300/1000], Loss: 1.1276\n","Epoch [302/1000], Loss: 0.9860\n","Epoch [304/1000], Loss: 0.9819\n","Epoch [306/1000], Loss: 0.7543\n","Epoch [308/1000], Loss: 0.8046\n","Epoch [310/1000], Loss: 0.9183\n","Epoch [312/1000], Loss: 0.9873\n","Epoch [314/1000], Loss: 0.9912\n","Epoch [316/1000], Loss: 1.1750\n","Epoch [318/1000], Loss: 1.0302\n","Epoch [320/1000], Loss: 0.8338\n","Epoch [322/1000], Loss: 1.0824\n","Epoch [324/1000], Loss: 0.9449\n","Epoch [326/1000], Loss: 0.8845\n","Epoch [328/1000], Loss: 1.1576\n","Epoch [330/1000], Loss: 0.7851\n","Epoch [332/1000], Loss: 0.7848\n","Epoch [334/1000], Loss: 1.0543\n","Epoch [336/1000], Loss: 0.9520\n","Epoch [338/1000], Loss: 0.7987\n","Epoch [340/1000], Loss: 1.0538\n","Epoch [342/1000], Loss: 0.9700\n","Epoch [344/1000], Loss: 1.0838\n","Epoch [346/1000], Loss: 1.0961\n","Epoch [348/1000], Loss: 0.6997\n","Epoch [350/1000], Loss: 1.0538\n","Epoch [352/1000], Loss: 1.1456\n","Epoch [354/1000], Loss: 0.8577\n","Epoch [356/1000], Loss: 1.5327\n","Epoch [358/1000], Loss: 0.8332\n","Epoch [360/1000], Loss: 1.0563\n","Epoch [362/1000], Loss: 0.7640\n","Epoch [364/1000], Loss: 0.8710\n","Epoch [366/1000], Loss: 0.6097\n","Epoch [368/1000], Loss: 0.9514\n","Epoch [370/1000], Loss: 1.1949\n","Epoch [372/1000], Loss: 0.9632\n","Epoch [374/1000], Loss: 0.7622\n","Epoch [376/1000], Loss: 0.9287\n","Epoch [378/1000], Loss: 0.9133\n","Epoch [380/1000], Loss: 1.0389\n","Epoch [382/1000], Loss: 0.7778\n","Epoch [384/1000], Loss: 0.9709\n","Epoch [386/1000], Loss: 0.9011\n","Epoch [388/1000], Loss: 1.0210\n","Epoch [390/1000], Loss: 0.7338\n","Epoch [392/1000], Loss: 0.8683\n","Epoch [394/1000], Loss: 0.8407\n","Epoch [396/1000], Loss: 0.8684\n","Epoch [398/1000], Loss: 0.9368\n","Epoch [400/1000], Loss: 0.6651\n","Epoch [402/1000], Loss: 1.0689\n","Epoch [404/1000], Loss: 0.5497\n","Epoch [406/1000], Loss: 0.8728\n","Epoch [408/1000], Loss: 0.5557\n","Epoch [410/1000], Loss: 0.8426\n","Epoch [412/1000], Loss: 1.0119\n","Epoch [414/1000], Loss: 0.7088\n","Epoch [416/1000], Loss: 0.6276\n","Epoch [418/1000], Loss: 0.8164\n","Epoch [420/1000], Loss: 1.2530\n","Epoch [422/1000], Loss: 1.1123\n","Epoch [424/1000], Loss: 1.1447\n","Epoch [426/1000], Loss: 1.1675\n","Epoch [428/1000], Loss: 1.0350\n","Epoch [430/1000], Loss: 0.7905\n","Epoch [432/1000], Loss: 0.9182\n","Epoch [434/1000], Loss: 0.7031\n","Epoch [436/1000], Loss: 0.8526\n","Epoch [438/1000], Loss: 0.9332\n","Epoch [440/1000], Loss: 1.0708\n","Epoch [442/1000], Loss: 0.9213\n","Epoch [444/1000], Loss: 0.8166\n","Epoch [446/1000], Loss: 0.8015\n","Epoch [448/1000], Loss: 0.9878\n","Epoch [450/1000], Loss: 0.7833\n","Epoch [452/1000], Loss: 1.0531\n","Epoch [454/1000], Loss: 0.6222\n","Epoch [456/1000], Loss: 0.6625\n","Epoch [458/1000], Loss: 0.9977\n","Epoch [460/1000], Loss: 0.8992\n","Epoch [462/1000], Loss: 0.7241\n","Epoch [464/1000], Loss: 1.3170\n","Epoch [466/1000], Loss: 0.9705\n","Epoch [468/1000], Loss: 0.9599\n","Epoch [470/1000], Loss: 1.1231\n","Epoch [472/1000], Loss: 0.7482\n","Epoch [474/1000], Loss: 0.9708\n","Epoch [476/1000], Loss: 0.7604\n","Epoch [478/1000], Loss: 0.9788\n","Epoch [480/1000], Loss: 1.0283\n","Epoch [482/1000], Loss: 0.8302\n","Epoch [484/1000], Loss: 1.0060\n","Epoch [486/1000], Loss: 0.6256\n","Epoch [488/1000], Loss: 1.1697\n","Epoch [490/1000], Loss: 0.6726\n","Epoch [492/1000], Loss: 1.0385\n","Epoch [494/1000], Loss: 0.7140\n","Epoch [496/1000], Loss: 0.9481\n","Epoch [498/1000], Loss: 0.6840\n","Epoch [500/1000], Loss: 0.7855\n","Epoch [502/1000], Loss: 1.0038\n","Epoch [504/1000], Loss: 0.6713\n","Epoch [506/1000], Loss: 0.8442\n","Epoch [508/1000], Loss: 0.7699\n","Epoch [510/1000], Loss: 1.1557\n","Epoch [512/1000], Loss: 0.6858\n","Epoch [514/1000], Loss: 0.8177\n","Epoch [516/1000], Loss: 0.9614\n","Epoch [518/1000], Loss: 0.7173\n","Epoch [520/1000], Loss: 0.8193\n","Epoch [522/1000], Loss: 0.7782\n","Epoch [524/1000], Loss: 0.8243\n","Epoch [526/1000], Loss: 0.6475\n","Epoch [528/1000], Loss: 1.0654\n","Epoch [530/1000], Loss: 0.9259\n","Epoch [532/1000], Loss: 0.6177\n","Epoch [534/1000], Loss: 0.8038\n","Epoch [536/1000], Loss: 0.9035\n","Epoch [538/1000], Loss: 0.7921\n","Epoch [540/1000], Loss: 0.7085\n","Epoch [542/1000], Loss: 0.9287\n","Epoch [544/1000], Loss: 0.8708\n","Epoch [546/1000], Loss: 0.8077\n","Epoch [548/1000], Loss: 0.8397\n","Epoch [550/1000], Loss: 0.5752\n","Epoch [552/1000], Loss: 1.0063\n","Epoch [554/1000], Loss: 0.7366\n","Epoch [556/1000], Loss: 1.0131\n","Epoch [558/1000], Loss: 0.7854\n","Epoch [560/1000], Loss: 0.8244\n","Epoch [562/1000], Loss: 0.7610\n","Epoch [564/1000], Loss: 0.8770\n","Epoch [566/1000], Loss: 0.4588\n","Epoch [568/1000], Loss: 0.6639\n","Epoch [570/1000], Loss: 0.9335\n","Epoch [572/1000], Loss: 0.9039\n","Epoch [574/1000], Loss: 0.8701\n","Epoch [576/1000], Loss: 0.8458\n","Epoch [578/1000], Loss: 1.0935\n","Epoch [580/1000], Loss: 0.6810\n","Epoch [582/1000], Loss: 0.7434\n","Epoch [584/1000], Loss: 0.7571\n","Epoch [586/1000], Loss: 0.8961\n","Epoch [588/1000], Loss: 0.5401\n","Epoch [590/1000], Loss: 0.7496\n","Epoch [592/1000], Loss: 1.0851\n","Epoch [594/1000], Loss: 0.6346\n","Epoch [596/1000], Loss: 0.7740\n","Epoch [598/1000], Loss: 0.7342\n","Epoch [600/1000], Loss: 0.8671\n","Epoch [602/1000], Loss: 0.9480\n","Epoch [604/1000], Loss: 0.6509\n","Epoch [606/1000], Loss: 0.7726\n","Epoch [608/1000], Loss: 0.8060\n","Epoch [610/1000], Loss: 0.6547\n","Epoch [612/1000], Loss: 0.8967\n","Epoch [614/1000], Loss: 0.9508\n","Epoch [616/1000], Loss: 0.9697\n","Epoch [618/1000], Loss: 0.9993\n","Epoch [620/1000], Loss: 0.6497\n","Epoch [622/1000], Loss: 0.8270\n","Epoch [624/1000], Loss: 0.9334\n","Epoch [626/1000], Loss: 1.0019\n","Epoch [628/1000], Loss: 0.5750\n","Epoch [630/1000], Loss: 0.5380\n","Epoch [632/1000], Loss: 0.8532\n","Epoch [634/1000], Loss: 0.9280\n","Epoch [636/1000], Loss: 0.8448\n","Epoch [638/1000], Loss: 0.8572\n","Epoch [640/1000], Loss: 0.8314\n","Epoch [642/1000], Loss: 0.8914\n","Epoch [644/1000], Loss: 0.8086\n","Epoch [646/1000], Loss: 0.6809\n","Epoch [648/1000], Loss: 0.7146\n","Epoch [650/1000], Loss: 0.8222\n","Epoch [652/1000], Loss: 0.5451\n","Epoch [654/1000], Loss: 0.8989\n","Epoch [656/1000], Loss: 0.8186\n","Epoch [658/1000], Loss: 0.7532\n","Epoch [660/1000], Loss: 1.0896\n","Epoch [662/1000], Loss: 0.7477\n","Epoch [664/1000], Loss: 0.8844\n","Epoch [666/1000], Loss: 0.6906\n","Epoch [668/1000], Loss: 0.7591\n","Epoch [670/1000], Loss: 0.7673\n","Epoch [672/1000], Loss: 0.7213\n","Epoch [674/1000], Loss: 0.8483\n","Epoch [676/1000], Loss: 1.0558\n","Epoch [678/1000], Loss: 0.8060\n","Epoch [680/1000], Loss: 0.7302\n","Epoch [682/1000], Loss: 0.8168\n","Epoch [684/1000], Loss: 0.9207\n","Epoch [686/1000], Loss: 1.0605\n","Epoch [688/1000], Loss: 0.8166\n","Epoch [690/1000], Loss: 0.6729\n","Epoch [692/1000], Loss: 0.8330\n","Epoch [694/1000], Loss: 0.7590\n","Epoch [696/1000], Loss: 0.7677\n","Epoch [698/1000], Loss: 0.8205\n","Epoch [700/1000], Loss: 1.1150\n","Epoch [702/1000], Loss: 0.9234\n","Epoch [704/1000], Loss: 0.7376\n","Epoch [706/1000], Loss: 0.9106\n","Epoch [708/1000], Loss: 1.0521\n","Epoch [710/1000], Loss: 0.8964\n","Epoch [712/1000], Loss: 0.9733\n","Epoch [714/1000], Loss: 0.7880\n","Epoch [716/1000], Loss: 0.9832\n","Epoch [718/1000], Loss: 1.0352\n","Epoch [720/1000], Loss: 0.7788\n","Epoch [722/1000], Loss: 0.6554\n","Epoch [724/1000], Loss: 0.5867\n","Epoch [726/1000], Loss: 0.8779\n","Epoch [728/1000], Loss: 0.7402\n","Epoch [730/1000], Loss: 0.8870\n","Epoch [732/1000], Loss: 0.7749\n","Epoch [734/1000], Loss: 0.6645\n","Epoch [736/1000], Loss: 0.8056\n","Epoch [738/1000], Loss: 1.0392\n","Epoch [740/1000], Loss: 1.0772\n","Epoch [742/1000], Loss: 0.9738\n","Epoch [744/1000], Loss: 0.4895\n","Epoch [746/1000], Loss: 1.1194\n","Epoch [748/1000], Loss: 0.6259\n","Epoch [750/1000], Loss: 0.7368\n","Epoch [752/1000], Loss: 0.9483\n","Epoch [754/1000], Loss: 0.9612\n","Epoch [756/1000], Loss: 0.8361\n","Epoch [758/1000], Loss: 0.9554\n","Epoch [760/1000], Loss: 0.8699\n","Epoch [762/1000], Loss: 0.6161\n","Epoch [764/1000], Loss: 0.7063\n","Epoch [766/1000], Loss: 0.9879\n","Epoch [768/1000], Loss: 0.7017\n","Epoch [770/1000], Loss: 0.7917\n","Epoch [772/1000], Loss: 0.8208\n","Epoch [774/1000], Loss: 0.9844\n","Epoch [776/1000], Loss: 0.8775\n","Epoch [778/1000], Loss: 0.5848\n","Epoch [780/1000], Loss: 0.8038\n","Epoch [782/1000], Loss: 0.6467\n","Epoch [784/1000], Loss: 0.7256\n","Epoch [786/1000], Loss: 0.6750\n","Epoch [788/1000], Loss: 0.6266\n","Epoch [790/1000], Loss: 0.8305\n","Epoch [792/1000], Loss: 0.5987\n","Epoch [794/1000], Loss: 0.7398\n","Epoch [796/1000], Loss: 0.6594\n","Epoch [798/1000], Loss: 0.7566\n","Epoch [800/1000], Loss: 0.9537\n","Epoch [802/1000], Loss: 0.9329\n","Epoch [804/1000], Loss: 0.9021\n","Epoch [806/1000], Loss: 0.9750\n","Epoch [808/1000], Loss: 0.7194\n","Epoch [810/1000], Loss: 0.6342\n","Epoch [812/1000], Loss: 0.8914\n","Epoch [814/1000], Loss: 0.8832\n","Epoch [816/1000], Loss: 1.1414\n","Epoch [818/1000], Loss: 0.8626\n","Epoch [820/1000], Loss: 0.8676\n","Epoch [822/1000], Loss: 0.7048\n","Epoch [824/1000], Loss: 0.7058\n","Epoch [826/1000], Loss: 0.9677\n","Epoch [828/1000], Loss: 0.9031\n","Epoch [830/1000], Loss: 0.6831\n","Epoch [832/1000], Loss: 0.7966\n","Epoch [834/1000], Loss: 0.8802\n","Epoch [836/1000], Loss: 0.8944\n","Epoch [838/1000], Loss: 1.0646\n","Epoch [840/1000], Loss: 0.8146\n","Epoch [842/1000], Loss: 0.7321\n","Epoch [844/1000], Loss: 0.8360\n","Epoch [846/1000], Loss: 0.6380\n","Epoch [848/1000], Loss: 0.8801\n","Epoch [850/1000], Loss: 0.7471\n","Epoch [852/1000], Loss: 0.7760\n","Epoch [854/1000], Loss: 0.9810\n","Epoch [856/1000], Loss: 0.7540\n","Epoch [858/1000], Loss: 0.7900\n","Epoch [860/1000], Loss: 0.6426\n","Epoch [862/1000], Loss: 0.8265\n","Epoch [864/1000], Loss: 0.6873\n","Epoch [866/1000], Loss: 0.9862\n","Epoch [868/1000], Loss: 0.8682\n","Epoch [870/1000], Loss: 0.7456\n","Epoch [872/1000], Loss: 1.2280\n","Epoch [874/1000], Loss: 0.8086\n","Epoch [876/1000], Loss: 0.9510\n","Epoch [878/1000], Loss: 0.7099\n","Epoch [880/1000], Loss: 0.7727\n","Epoch [882/1000], Loss: 0.7363\n","Epoch [884/1000], Loss: 0.7592\n","Epoch [886/1000], Loss: 0.8231\n","Epoch [888/1000], Loss: 0.9793\n","Epoch [890/1000], Loss: 0.7160\n","Epoch [892/1000], Loss: 0.7663\n","Epoch [894/1000], Loss: 0.6027\n","Epoch [896/1000], Loss: 0.6482\n","Epoch [898/1000], Loss: 0.7918\n","Epoch [900/1000], Loss: 1.0951\n","Epoch [902/1000], Loss: 0.8879\n","Epoch [904/1000], Loss: 1.0287\n","Epoch [906/1000], Loss: 0.6016\n","Epoch [908/1000], Loss: 0.9186\n","Epoch [910/1000], Loss: 0.8697\n","Epoch [912/1000], Loss: 0.5555\n","Epoch [914/1000], Loss: 0.9222\n","Epoch [916/1000], Loss: 0.6157\n","Epoch [918/1000], Loss: 1.3644\n","Epoch [920/1000], Loss: 0.9079\n","Epoch [922/1000], Loss: 0.9245\n","Epoch [924/1000], Loss: 0.9603\n","Epoch [926/1000], Loss: 0.7035\n","Epoch [928/1000], Loss: 0.5871\n","Epoch [930/1000], Loss: 0.7026\n","Epoch [932/1000], Loss: 1.0445\n","Epoch [934/1000], Loss: 1.0512\n","Epoch [936/1000], Loss: 0.8652\n","Epoch [938/1000], Loss: 0.9878\n","Epoch [940/1000], Loss: 0.7163\n","Epoch [942/1000], Loss: 0.6896\n","Epoch [944/1000], Loss: 0.6742\n","Epoch [946/1000], Loss: 0.6787\n","Epoch [948/1000], Loss: 0.5336\n","Epoch [950/1000], Loss: 0.7574\n","Epoch [952/1000], Loss: 0.9431\n","Epoch [954/1000], Loss: 0.9006\n","Epoch [956/1000], Loss: 0.8026\n","Epoch [958/1000], Loss: 1.0589\n","Epoch [960/1000], Loss: 1.4080\n","Epoch [962/1000], Loss: 0.9365\n","Epoch [964/1000], Loss: 1.0914\n","Epoch [966/1000], Loss: 0.7210\n","Epoch [968/1000], Loss: 0.6940\n","Epoch [970/1000], Loss: 0.8606\n","Epoch [972/1000], Loss: 0.8823\n","Epoch [974/1000], Loss: 0.7683\n","Epoch [976/1000], Loss: 0.9968\n","Epoch [978/1000], Loss: 0.7408\n","Epoch [980/1000], Loss: 0.6835\n","Epoch [982/1000], Loss: 0.8112\n","Epoch [984/1000], Loss: 0.9518\n","Epoch [986/1000], Loss: 0.8590\n","Epoch [988/1000], Loss: 0.5056\n","Epoch [990/1000], Loss: 0.9958\n","Epoch [992/1000], Loss: 0.8749\n","Epoch [994/1000], Loss: 0.8004\n","Epoch [996/1000], Loss: 0.9788\n","Epoch [998/1000], Loss: 0.5725\n","Epoch [1000/1000], Loss: 0.8744\n","Average accuracy: 0.90239686\n","Average top_k_average_accuracies 1.1699415\n","KNN accuracy: 1.1121247\n"]}],"source":["best_accuracies = []\n","accuracies = []\n","top_k_average_accuracies = []\n","knn_accuracies = []\n","PATH = os.path.join(folder_name, f'checkpoints/regression_{dataset_name}.h5')\n","cfg.PATH = PATH\n","k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n","\n","\n","for train_index, test_index in k_fold.split(Xs):\n","  # Get training and testing data\n","  X_train, X_test = Xs[train_index], Xs[test_index]\n","  y_train, y_test = ys[train_index], ys[test_index]\n","\n","  knn = KNeighborsRegressor(n_neighbors=cfg.top_k)\n","  knn.fit(X_train, y_train)\n","  knn_accuracies.append(mean_squared_error(knn.predict(X_test), y_test))\n","\n","  best_accuracy, accuracy, top_k_average_accuracy, model= train_reg(X_train, y_train, X_test, y_test, cfg)\n","  best_accuracies.append(best_accuracy)\n","  accuracies.append(accuracy)\n","  top_k_average_accuracies.append(top_k_average_accuracy)\n","  # break\n","\n","print(\"Average accuracy:\", np.mean([acc.detach().numpy() for acc in accuracies]))\n","print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n","print(\"KNN accuracy:\", np.mean(knn_accuracies))"]},{"cell_type":"code","source":["print(\"Average accuracy:\", np.mean([acc.detach().numpy() for acc in accuracies]))\n","print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n","print(\"KNN accuracy:\", np.mean(knn_accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9m_Yhd0SOmiV","executionInfo":{"status":"ok","timestamp":1718860395427,"user_tz":-480,"elapsed":45,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"7472e2b8-4cba-47e7-aa4f-19bf71fc6540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy: 0.90239686\n","Average top_k_average_accuracies 1.1699415\n","KNN accuracy: 1.1121247\n"]}]},{"cell_type":"markdown","metadata":{"id":"aQMTPXLwaBVq"},"source":["# Results Interpretation"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"sf23mP1UaIvx","executionInfo":{"status":"ok","timestamp":1718885401562,"user_tz":-480,"elapsed":688,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def print_model_features(input_model):\n","  for n, p in model.named_parameters():\n","    print(n)\n","    print(p.data)"]},{"cell_type":"code","execution_count":121,"metadata":{"id":"MRO3tUPEbJUs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718886632621,"user_tz":-480,"elapsed":710,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"22dcc31b-0e7d-4c19-864c-6ed157dd5b0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["fa_layer.f1weight\n","tensor([1.0001, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0001, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0001,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0001, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0001, 1.0000, 1.0001, 1.0000, 1.0001, 1.0001, 1.0000, 1.0001,\n","        1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0001,\n","        1.0001, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001],\n","       device='cuda:0')\n","ca_layer.fa_weight\n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","       device='cuda:0')\n","ca_layer.bias\n","tensor([0.8944, 0.8944, 0.8944,  ..., 0.8944, 0.8944, 0.8944], device='cuda:0')\n","class_layer.ca_weight\n","tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        ...,\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]], device='cuda:0')\n","class_layer.bias\n","tensor([0.9891, 0.9908, 0.9990, 1.0171], device='cuda:0')\n"]}],"source":["print_model_features(model)"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"PkxAqSoldjML","executionInfo":{"status":"ok","timestamp":1718886636027,"user_tz":-480,"elapsed":1,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# for regression only. for classification is different\n","#feature_activations, case_activations, predicted_number\n","model.eval()\n","feature_activations, case_activations, output, predicted_class = model(X_test.to(device))"]},{"cell_type":"code","execution_count":123,"metadata":{"id":"PtwGRyMjeXtp","executionInfo":{"status":"ok","timestamp":1718886636669,"user_tz":-480,"elapsed":643,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b6355aa-1182-4ee5-8d8a-83246764bab2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n","       device='cuda:0')"]},"metadata":{},"execution_count":123}],"source":["predicted_class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvY5_WGkeaxg","executionInfo":{"status":"aborted","timestamp":1718885244287,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eb7wyntlXft_","executionInfo":{"status":"aborted","timestamp":1718885244287,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# prompt: accuracy comparing predicted_class and y_test\n","\n","accuracy = accuracy_score(y_test.numpy(), predicted_class.cpu().numpy())\n","print(\"Accuracy:\", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjaUAY7Bkjyr","executionInfo":{"status":"aborted","timestamp":1718885244287,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["#inspecting the case activations\n","top_case_indices = torch.topk(case_activations, 5, dim=1)[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfi-PKhokmev","executionInfo":{"status":"aborted","timestamp":1718885244287,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["X_test[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMxrFCywYIaQ","executionInfo":{"status":"aborted","timestamp":1718885244287,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_test[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73NQbeJ_kpO_","executionInfo":{"status":"aborted","timestamp":1718885244287,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["X_train[top_case_indices[0][0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9_6zvszYKPw","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_train[top_case_indices[0][0]]"]},{"cell_type":"markdown","metadata":{"id":"xMePSTR1lXbb"},"source":["By comparing the following two blocks' outputs, you can see we are retrieving a good neighbor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0izuKF6okrsc","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["#sum abs of X_test[0] and the top activated case\n","sum(abs(X_test[0] - X_train[top_case_indices[0][0]]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faYClHOVktuW","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# prompt: average sum abs of X_test[0] and X_train data\n","print(np.mean([sum(abs(X_test[0] - X_train[i])) for i in range(len(X_train))]))"]},{"cell_type":"markdown","metadata":{"id":"zkYKvaP-lz_0"},"source":["TODO:: A better way is to show the distribution of ``X_test[0] - X_train[i]``"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrwLY3gXmCLd","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_train[top_case_indices[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"es5Kv85CmHMt","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["knn.predict(X_test)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxOcKMzHmJkS","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["indices = knn.kneighbors(X_test)[1][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4vuKW0rmNVi","executionInfo":{"status":"aborted","timestamp":1718885244288,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_train[indices]"]},{"cell_type":"markdown","source":["# Sanity Check\n"],"metadata":{"id":"uZsxNJZDQ8RQ"}},{"cell_type":"markdown","source":["## Classification Neural Network"],"metadata":{"id":"ZhHAd1y-Q_C2"}},{"cell_type":"code","source":["# Hyperparameters\n","input_size = X_train.shape[1]\n","hidden_size = 1024\n","num_classes = torch.unique(ys).shape[0]\n","learning_rate = 1e-5\n","batch_size = 16\n","epochs = 2000"],"metadata":{"id":"pXu1CDqXRR_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define the neural network architecture for classification\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.nn = nn.Sequential(\n","            nn.Linear(input_size, hidden_size ),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size , hidden_size // 2),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 2, hidden_size // 4),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 4, num_classes)\n","            )\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.fill_(0)\n","\n","    def forward(self, x):\n","        return self.nn(x)\n"],"metadata":{"id":"CBu0Gj9vRKYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_accuracies = []\n","for train_index, test_index in k_fold.split(Xs):\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n","  patience_counter = 0\n","  best_model = None\n","  best_accuracy = None\n","  # Initialize the model, loss function, and optimizer\n","  model = NeuralNet(input_size, hidden_size, num_classes)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # Training loop\n","  for epoch in range(epochs):\n","    epoch_msg = True\n","    training_total_acc = 0.0\n","    training_total_loss = 0.0\n","    num_of_batches = len(train_loader)\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      # Forward pass\n","      outputs = model(X_train_batch)\n","      loss = criterion(outputs, y_train_batch)\n","\n","      # Backward and optimize\n","      _, predicted = torch.max(outputs, 1)\n","      training_total_acc += torch.sum(predicted == y_train_batch).item()\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      training_total_loss += loss.item()\n","      # if (i + 1) % 5 == 0\n","    if epoch == 0 or (epoch + 1) % 100 == 0:\n","      print(f\"Epoch: {epoch + 1}, Training Loss: {training_total_loss/num_of_batches:.2f} Acc: {training_total_acc/num_of_batches:.2f}\")\n","    # Testing the model\n","    model.eval()\n","    with torch.no_grad():\n","      outputs = model(X_test)\n","      loss = criterion(outputs, y_test)\n","      _, predicted = torch.max(outputs, 1)\n","      accuracy = torch.sum(predicted == y_test).item() / len(y_test)\n","      print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n","      if best_accuracy is None or accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_model = model\n","        patience_counter = 0\n","      else:\n","        patience_counter += 1\n","      if epoch_msg and (epoch + 1) % 100 == 0:\n","        epoch_msg = False\n","        print(f'Epoch [{epoch + 1}/{epoch}], Test Loss: {loss.item()}')\n","    if patience_counter >= cfg.patience:\n","      print(\"Best acc achieved: \", best_accuracy)\n","      break\n","  best_accuracies.append(best_accuracy)\n","  # break\n","print(\"Average accuracy:\", np.mean(best_accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcwG0kIDRnfg","executionInfo":{"status":"ok","timestamp":1718846087211,"user_tz":-480,"elapsed":78358,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"db76ac84-ebe5-4049-9094-f38928f622e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 1.91 Acc: 4.08\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Best acc achieved:  0.43478260869565216\n","Epoch: 1, Training Loss: 1.55 Acc: 4.00\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 39.13%\n","Best acc achieved:  0.4782608695652174\n","Epoch: 1, Training Loss: 2.02 Acc: 3.77\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 56.52%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 56.52%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 54.35%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Best acc achieved:  0.5652173913043478\n","Epoch: 1, Training Loss: 2.56 Acc: 3.69\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Best acc achieved:  0.41304347826086957\n","Epoch: 1, Training Loss: 2.00 Acc: 4.00\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Best acc achieved:  0.391304347826087\n","Epoch: 1, Training Loss: 1.90 Acc: 4.08\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Epoch: 100, Training Loss: 0.94 Acc: 11.38\n","Accuracy on the test set: 52.17%\n","Epoch [100/99], Test Loss: 2.013981342315674\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 52.17%\n","Accuracy on the test set: 41.30%\n","Best acc achieved:  0.5217391304347826\n","Epoch: 1, Training Loss: 2.09 Acc: 4.08\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 17.39%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Epoch: 100, Training Loss: 0.91 Acc: 11.46\n","Accuracy on the test set: 45.65%\n","Epoch [100/99], Test Loss: 2.1118876934051514\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Best acc achieved:  0.5\n","Epoch: 1, Training Loss: 1.75 Acc: 3.65\n","Accuracy on the test set: 17.39%\n","Accuracy on the test set: 13.04%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 15.22%\n","Accuracy on the test set: 15.22%\n","Accuracy on the test set: 15.22%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 10.87%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 17.39%\n","Accuracy on the test set: 17.39%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Best acc achieved:  0.3695652173913043\n","Epoch: 1, Training Loss: 2.35 Acc: 4.23\n","Accuracy on the test set: 17.39%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 21.74%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 17.39%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 23.91%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Best acc achieved:  0.391304347826087\n","Epoch: 1, Training Loss: 2.63 Acc: 4.27\n","Accuracy on the test set: 19.57%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 26.09%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 30.43%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 34.78%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 32.61%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 36.96%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 28.26%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 39.13%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Epoch: 100, Training Loss: 0.97 Acc: 11.46\n","Accuracy on the test set: 43.48%\n","Epoch [100/99], Test Loss: 2.0970592498779297\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 45.65%\n","Accuracy on the test set: 47.83%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 43.48%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Accuracy on the test set: 41.30%\n","Best acc achieved:  0.5\n","Average accuracy: 0.45652173913043476\n"]}]},{"cell_type":"markdown","source":["## Regression Neural Network"],"metadata":{"id":"wF89uiJWRIcf"}},{"cell_type":"code","source":["# Hyperparameters\n","input_size = X_train.shape[1]\n","hidden_size = 100\n","# num_classes = torch.unique(ys).shape[0]\n","learning_rate = 1e-5\n","batch_size = 16\n","epochs = 2000"],"metadata":{"id":"SUqIgNipWaOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: a standard neural network with 3 fully connected layers for regression\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RegressionNet(nn.Module):\n","    def __init__(self, input_size):\n","        super(RegressionNet, self).__init__()\n","        self.nn = nn.Sequential(\n","            nn.Linear(input_size, hidden_size ),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size , hidden_size // 2),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 2, hidden_size // 4),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 4, 1)\n","            )\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.fill_(0)\n","\n","    def forward(self, x):\n","        return self.nn(x).squeeze()"],"metadata":{"id":"BLy5JoRbWSI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_accuracies = []\n","for train_index, test_index in k_fold.split(Xs):\n","  # Get training and testing data\n","  X_train, X_test = Xs[train_index], Xs[test_index]\n","  y_train, y_test = ys[train_index], ys[test_index]\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n","  patience_counter = 0\n","  best_model = None\n","  best_accuracy = None\n","  model = RegressionNet(Xs.shape[1])\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  for epoch in range(epochs):\n","    epoch_msg = True\n","    training_total_loss = 0.0\n","    num_of_batches = len(train_loader)\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      # Forward pass\n","      outputs = model(X_train_batch)\n","      loss = criterion(outputs, y_train_batch)\n","      training_total_loss += loss.item()\n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","    if epoch == 0 or (epoch + 1) % 3 == 0:\n","      print(f'Epoch: {epoch + 1}, Training Loss: {training_total_loss/num_of_batches:.2f}')\n","\n","    # Testing the model\n","    model.eval()\n","    with torch.no_grad():\n","      outputs = model(X_test)\n","      loss = criterion(outputs, y_test)\n","      if best_accuracy is None or loss.item() < best_accuracy:\n","        best_accuracy = loss.item()\n","        best_model = model\n","        patience_counter = 0\n","      else:\n","        patience_counter += 1\n","      if epoch_msg and (epoch + 1) % 100 == 0:\n","        epoch_msg = False\n","        print(f'Epoch [{epoch + 1}/{epochs}], Test Loss: {loss.item()}')\n","        # print(f'Loss on the test set: {loss.item()}')\n","    if patience_counter >= cfg.patience:\n","      print(\"Best loss achieved: \", best_accuracy)\n","      break\n","  best_accuracies.append(best_accuracy)\n","print(\"Average accuracy:\", np.mean(best_accuracies))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55RGrtO-XZuG","executionInfo":{"status":"ok","timestamp":1718851554959,"user_tz":-480,"elapsed":1205799,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"5006503c-b908-431e-dcfe-7eaeb5b18eb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 1.17\n","Epoch: 3, Training Loss: 1.03\n","Epoch: 6, Training Loss: 1.01\n","Epoch: 9, Training Loss: 1.00\n","Epoch: 12, Training Loss: 0.99\n","Epoch: 15, Training Loss: 0.98\n","Epoch: 18, Training Loss: 0.98\n","Epoch: 21, Training Loss: 0.97\n","Epoch: 24, Training Loss: 0.97\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.95\n","Epoch: 33, Training Loss: 0.96\n","Epoch: 36, Training Loss: 0.95\n","Epoch: 39, Training Loss: 0.95\n","Epoch: 42, Training Loss: 0.94\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.93\n","Epoch: 51, Training Loss: 0.93\n","Epoch: 54, Training Loss: 0.92\n","Epoch: 57, Training Loss: 0.92\n","Epoch: 60, Training Loss: 0.92\n","Epoch: 63, Training Loss: 0.92\n","Epoch: 66, Training Loss: 0.91\n","Epoch: 69, Training Loss: 0.91\n","Epoch: 72, Training Loss: 0.90\n","Epoch: 75, Training Loss: 0.90\n","Epoch: 78, Training Loss: 0.89\n","Epoch: 81, Training Loss: 0.89\n","Epoch: 84, Training Loss: 0.88\n","Epoch: 87, Training Loss: 0.88\n","Epoch: 90, Training Loss: 0.88\n","Epoch: 93, Training Loss: 0.87\n","Epoch: 96, Training Loss: 0.87\n","Epoch: 99, Training Loss: 0.87\n","Epoch [100/2000], Test Loss: 0.9691863656044006\n","Epoch: 102, Training Loss: 0.86\n","Epoch: 105, Training Loss: 0.86\n","Epoch: 108, Training Loss: 0.85\n","Epoch: 111, Training Loss: 0.85\n","Epoch: 114, Training Loss: 0.85\n","Epoch: 117, Training Loss: 0.85\n","Epoch: 120, Training Loss: 0.84\n","Epoch: 123, Training Loss: 0.84\n","Epoch: 126, Training Loss: 0.83\n","Epoch: 129, Training Loss: 0.83\n","Epoch: 132, Training Loss: 0.82\n","Epoch: 135, Training Loss: 0.82\n","Epoch: 138, Training Loss: 0.83\n","Epoch: 141, Training Loss: 0.82\n","Epoch: 144, Training Loss: 0.82\n","Epoch: 147, Training Loss: 0.82\n","Epoch: 150, Training Loss: 0.81\n","Epoch: 153, Training Loss: 0.81\n","Epoch: 156, Training Loss: 0.81\n","Epoch: 159, Training Loss: 0.81\n","Epoch: 162, Training Loss: 0.80\n","Epoch: 165, Training Loss: 0.80\n","Epoch: 168, Training Loss: 0.81\n","Epoch: 171, Training Loss: 0.80\n","Epoch: 174, Training Loss: 0.80\n","Epoch: 177, Training Loss: 0.80\n","Epoch: 180, Training Loss: 0.79\n","Epoch: 183, Training Loss: 0.80\n","Epoch: 186, Training Loss: 0.79\n","Epoch: 189, Training Loss: 0.79\n","Epoch: 192, Training Loss: 0.80\n","Epoch: 195, Training Loss: 0.79\n","Epoch: 198, Training Loss: 0.78\n","Epoch [200/2000], Test Loss: 0.948259711265564\n","Epoch: 201, Training Loss: 0.79\n","Epoch: 204, Training Loss: 0.78\n","Epoch: 207, Training Loss: 0.78\n","Epoch: 210, Training Loss: 0.78\n","Epoch: 213, Training Loss: 0.78\n","Epoch: 216, Training Loss: 0.77\n","Epoch: 219, Training Loss: 0.78\n","Epoch: 222, Training Loss: 0.77\n","Epoch: 225, Training Loss: 0.77\n","Epoch: 228, Training Loss: 0.77\n","Epoch: 231, Training Loss: 0.77\n","Epoch: 234, Training Loss: 0.77\n","Epoch: 237, Training Loss: 0.77\n","Epoch: 240, Training Loss: 0.77\n","Epoch: 243, Training Loss: 0.76\n","Epoch: 246, Training Loss: 0.77\n","Epoch: 249, Training Loss: 0.76\n","Epoch: 252, Training Loss: 0.76\n","Epoch: 255, Training Loss: 0.77\n","Epoch: 258, Training Loss: 0.76\n","Epoch: 261, Training Loss: 0.77\n","Epoch: 264, Training Loss: 0.76\n","Epoch: 267, Training Loss: 0.76\n","Epoch: 270, Training Loss: 0.76\n","Epoch: 273, Training Loss: 0.75\n","Epoch: 276, Training Loss: 0.76\n","Epoch: 279, Training Loss: 0.76\n","Epoch: 282, Training Loss: 0.75\n","Epoch: 285, Training Loss: 0.75\n","Epoch: 288, Training Loss: 0.75\n","Epoch: 291, Training Loss: 0.75\n","Epoch: 294, Training Loss: 0.75\n","Epoch: 297, Training Loss: 0.75\n","Epoch: 300, Training Loss: 0.75\n","Epoch [300/2000], Test Loss: 0.9395970106124878\n","Epoch: 303, Training Loss: 0.75\n","Epoch: 306, Training Loss: 0.75\n","Epoch: 309, Training Loss: 0.74\n","Epoch: 312, Training Loss: 0.75\n","Epoch: 315, Training Loss: 0.74\n","Epoch: 318, Training Loss: 0.74\n","Epoch: 321, Training Loss: 0.74\n","Epoch: 324, Training Loss: 0.74\n","Epoch: 327, Training Loss: 0.74\n","Epoch: 330, Training Loss: 0.74\n","Epoch: 333, Training Loss: 0.74\n","Epoch: 336, Training Loss: 0.74\n","Epoch: 339, Training Loss: 0.73\n","Epoch: 342, Training Loss: 0.73\n","Epoch: 345, Training Loss: 0.74\n","Epoch: 348, Training Loss: 0.74\n","Epoch: 351, Training Loss: 0.73\n","Epoch: 354, Training Loss: 0.73\n","Epoch: 357, Training Loss: 0.73\n","Epoch: 360, Training Loss: 0.74\n","Epoch: 363, Training Loss: 0.73\n","Epoch: 366, Training Loss: 0.73\n","Epoch: 369, Training Loss: 0.73\n","Epoch: 372, Training Loss: 0.73\n","Epoch: 375, Training Loss: 0.73\n","Epoch: 378, Training Loss: 0.72\n","Epoch: 381, Training Loss: 0.73\n","Epoch: 384, Training Loss: 0.72\n","Epoch: 387, Training Loss: 0.73\n","Epoch: 390, Training Loss: 0.72\n","Epoch: 393, Training Loss: 0.72\n","Epoch: 396, Training Loss: 0.72\n","Epoch: 399, Training Loss: 0.72\n","Epoch [400/2000], Test Loss: 0.928053081035614\n","Epoch: 402, Training Loss: 0.72\n","Epoch: 405, Training Loss: 0.72\n","Epoch: 408, Training Loss: 0.72\n","Epoch: 411, Training Loss: 0.72\n","Epoch: 414, Training Loss: 0.72\n","Epoch: 417, Training Loss: 0.72\n","Epoch: 420, Training Loss: 0.71\n","Epoch: 423, Training Loss: 0.71\n","Epoch: 426, Training Loss: 0.71\n","Epoch: 429, Training Loss: 0.72\n","Epoch: 432, Training Loss: 0.71\n","Epoch: 435, Training Loss: 0.71\n","Epoch: 438, Training Loss: 0.71\n","Epoch: 441, Training Loss: 0.71\n","Epoch: 444, Training Loss: 0.71\n","Epoch: 447, Training Loss: 0.70\n","Epoch: 450, Training Loss: 0.71\n","Epoch: 453, Training Loss: 0.70\n","Epoch: 456, Training Loss: 0.70\n","Epoch: 459, Training Loss: 0.70\n","Epoch: 462, Training Loss: 0.70\n","Epoch: 465, Training Loss: 0.70\n","Epoch: 468, Training Loss: 0.70\n","Epoch: 471, Training Loss: 0.71\n","Epoch: 474, Training Loss: 0.70\n","Epoch: 477, Training Loss: 0.70\n","Epoch: 480, Training Loss: 0.70\n","Epoch: 483, Training Loss: 0.70\n","Epoch: 486, Training Loss: 0.69\n","Epoch: 489, Training Loss: 0.70\n","Epoch: 492, Training Loss: 0.70\n","Epoch: 495, Training Loss: 0.69\n","Epoch: 498, Training Loss: 0.69\n","Epoch [500/2000], Test Loss: 0.9236001372337341\n","Epoch: 501, Training Loss: 0.70\n","Epoch: 504, Training Loss: 0.69\n","Epoch: 507, Training Loss: 0.70\n","Epoch: 510, Training Loss: 0.69\n","Epoch: 513, Training Loss: 0.69\n","Epoch: 516, Training Loss: 0.69\n","Epoch: 519, Training Loss: 0.68\n","Epoch: 522, Training Loss: 0.69\n","Epoch: 525, Training Loss: 0.69\n","Epoch: 528, Training Loss: 0.69\n","Epoch: 531, Training Loss: 0.68\n","Epoch: 534, Training Loss: 0.68\n","Epoch: 537, Training Loss: 0.68\n","Epoch: 540, Training Loss: 0.68\n","Epoch: 543, Training Loss: 0.68\n","Epoch: 546, Training Loss: 0.68\n","Epoch: 549, Training Loss: 0.68\n","Epoch: 552, Training Loss: 0.68\n","Epoch: 555, Training Loss: 0.68\n","Epoch: 558, Training Loss: 0.68\n","Epoch: 561, Training Loss: 0.67\n","Epoch: 564, Training Loss: 0.67\n","Best loss achieved:  0.9214946627616882\n","Epoch: 1, Training Loss: 1.27\n","Epoch: 3, Training Loss: 1.09\n","Epoch: 6, Training Loss: 1.02\n","Epoch: 9, Training Loss: 1.00\n","Epoch: 12, Training Loss: 0.99\n","Epoch: 15, Training Loss: 0.99\n","Epoch: 18, Training Loss: 0.98\n","Epoch: 21, Training Loss: 0.97\n","Epoch: 24, Training Loss: 0.97\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.96\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.95\n","Epoch: 39, Training Loss: 0.94\n","Epoch: 42, Training Loss: 0.93\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.92\n","Epoch: 51, Training Loss: 0.92\n","Epoch: 54, Training Loss: 0.91\n","Epoch: 57, Training Loss: 0.90\n","Epoch: 60, Training Loss: 0.90\n","Epoch: 63, Training Loss: 0.89\n","Epoch: 66, Training Loss: 0.89\n","Epoch: 69, Training Loss: 0.88\n","Epoch: 72, Training Loss: 0.88\n","Epoch: 75, Training Loss: 0.87\n","Epoch: 78, Training Loss: 0.86\n","Epoch: 81, Training Loss: 0.86\n","Epoch: 84, Training Loss: 0.85\n","Epoch: 87, Training Loss: 0.85\n","Epoch: 90, Training Loss: 0.85\n","Epoch: 93, Training Loss: 0.84\n","Epoch: 96, Training Loss: 0.84\n","Epoch: 99, Training Loss: 0.83\n","Epoch [100/2000], Test Loss: 0.854515790939331\n","Epoch: 102, Training Loss: 0.83\n","Epoch: 105, Training Loss: 0.83\n","Epoch: 108, Training Loss: 0.83\n","Epoch: 111, Training Loss: 0.83\n","Epoch: 114, Training Loss: 0.82\n","Epoch: 117, Training Loss: 0.82\n","Epoch: 120, Training Loss: 0.81\n","Epoch: 123, Training Loss: 0.81\n","Epoch: 126, Training Loss: 0.81\n","Epoch: 129, Training Loss: 0.81\n","Epoch: 132, Training Loss: 0.80\n","Epoch: 135, Training Loss: 0.81\n","Epoch: 138, Training Loss: 0.80\n","Epoch: 141, Training Loss: 0.80\n","Epoch: 144, Training Loss: 0.80\n","Epoch: 147, Training Loss: 0.80\n","Epoch: 150, Training Loss: 0.80\n","Epoch: 153, Training Loss: 0.80\n","Epoch: 156, Training Loss: 0.80\n","Epoch: 159, Training Loss: 0.79\n","Epoch: 162, Training Loss: 0.79\n","Epoch: 165, Training Loss: 0.79\n","Epoch: 168, Training Loss: 0.78\n","Epoch: 171, Training Loss: 0.79\n","Epoch: 174, Training Loss: 0.79\n","Epoch: 177, Training Loss: 0.79\n","Epoch: 180, Training Loss: 0.78\n","Epoch: 183, Training Loss: 0.78\n","Epoch: 186, Training Loss: 0.78\n","Epoch: 189, Training Loss: 0.78\n","Epoch: 192, Training Loss: 0.78\n","Epoch: 195, Training Loss: 0.78\n","Epoch: 198, Training Loss: 0.77\n","Epoch [200/2000], Test Loss: 0.8353716731071472\n","Epoch: 201, Training Loss: 0.77\n","Epoch: 204, Training Loss: 0.78\n","Epoch: 207, Training Loss: 0.78\n","Epoch: 210, Training Loss: 0.77\n","Epoch: 213, Training Loss: 0.77\n","Epoch: 216, Training Loss: 0.77\n","Epoch: 219, Training Loss: 0.77\n","Epoch: 222, Training Loss: 0.76\n","Epoch: 225, Training Loss: 0.76\n","Epoch: 228, Training Loss: 0.76\n","Epoch: 231, Training Loss: 0.76\n","Epoch: 234, Training Loss: 0.76\n","Epoch: 237, Training Loss: 0.76\n","Epoch: 240, Training Loss: 0.76\n","Epoch: 243, Training Loss: 0.76\n","Epoch: 246, Training Loss: 0.76\n","Epoch: 249, Training Loss: 0.76\n","Epoch: 252, Training Loss: 0.75\n","Epoch: 255, Training Loss: 0.76\n","Epoch: 258, Training Loss: 0.75\n","Epoch: 261, Training Loss: 0.75\n","Epoch: 264, Training Loss: 0.75\n","Epoch: 267, Training Loss: 0.75\n","Epoch: 270, Training Loss: 0.75\n","Epoch: 273, Training Loss: 0.75\n","Epoch: 276, Training Loss: 0.74\n","Epoch: 279, Training Loss: 0.75\n","Epoch: 282, Training Loss: 0.75\n","Epoch: 285, Training Loss: 0.74\n","Best loss achieved:  0.8327994346618652\n","Epoch: 1, Training Loss: 3.43\n","Epoch: 3, Training Loss: 2.56\n","Epoch: 6, Training Loss: 1.90\n","Epoch: 9, Training Loss: 1.52\n","Epoch: 12, Training Loss: 1.19\n","Epoch: 15, Training Loss: 1.03\n","Epoch: 18, Training Loss: 1.00\n","Epoch: 21, Training Loss: 0.99\n","Epoch: 24, Training Loss: 0.98\n","Epoch: 27, Training Loss: 0.97\n","Epoch: 30, Training Loss: 0.98\n","Epoch: 33, Training Loss: 0.97\n","Epoch: 36, Training Loss: 0.97\n","Epoch: 39, Training Loss: 0.96\n","Epoch: 42, Training Loss: 0.96\n","Epoch: 45, Training Loss: 0.95\n","Epoch: 48, Training Loss: 0.95\n","Epoch: 51, Training Loss: 0.94\n","Epoch: 54, Training Loss: 0.93\n","Epoch: 57, Training Loss: 0.93\n","Epoch: 60, Training Loss: 0.93\n","Epoch: 63, Training Loss: 0.92\n","Epoch: 66, Training Loss: 0.91\n","Epoch: 69, Training Loss: 0.91\n","Epoch: 72, Training Loss: 0.91\n","Epoch: 75, Training Loss: 0.90\n","Epoch: 78, Training Loss: 0.90\n","Epoch: 81, Training Loss: 0.89\n","Epoch: 84, Training Loss: 0.89\n","Epoch: 87, Training Loss: 0.89\n","Epoch: 90, Training Loss: 0.88\n","Epoch: 93, Training Loss: 0.88\n","Epoch: 96, Training Loss: 0.87\n","Epoch: 99, Training Loss: 0.87\n","Epoch [100/2000], Test Loss: 0.8876606822013855\n","Epoch: 102, Training Loss: 0.87\n","Epoch: 105, Training Loss: 0.86\n","Epoch: 108, Training Loss: 0.86\n","Epoch: 111, Training Loss: 0.86\n","Epoch: 114, Training Loss: 0.85\n","Epoch: 117, Training Loss: 0.85\n","Epoch: 120, Training Loss: 0.84\n","Epoch: 123, Training Loss: 0.84\n","Epoch: 126, Training Loss: 0.84\n","Epoch: 129, Training Loss: 0.84\n","Epoch: 132, Training Loss: 0.84\n","Epoch: 135, Training Loss: 0.83\n","Epoch: 138, Training Loss: 0.83\n","Epoch: 141, Training Loss: 0.83\n","Epoch: 144, Training Loss: 0.83\n","Epoch: 147, Training Loss: 0.82\n","Epoch: 150, Training Loss: 0.82\n","Epoch: 153, Training Loss: 0.82\n","Epoch: 156, Training Loss: 0.81\n","Epoch: 159, Training Loss: 0.81\n","Epoch: 162, Training Loss: 0.81\n","Epoch: 165, Training Loss: 0.81\n","Epoch: 168, Training Loss: 0.80\n","Epoch: 171, Training Loss: 0.80\n","Epoch: 174, Training Loss: 0.80\n","Epoch: 177, Training Loss: 0.80\n","Epoch: 180, Training Loss: 0.80\n","Epoch: 183, Training Loss: 0.80\n","Epoch: 186, Training Loss: 0.79\n","Epoch: 189, Training Loss: 0.79\n","Epoch: 192, Training Loss: 0.79\n","Epoch: 195, Training Loss: 0.79\n","Epoch: 198, Training Loss: 0.79\n","Epoch [200/2000], Test Loss: 0.8760926723480225\n","Epoch: 201, Training Loss: 0.79\n","Epoch: 204, Training Loss: 0.79\n","Epoch: 207, Training Loss: 0.79\n","Epoch: 210, Training Loss: 0.78\n","Epoch: 213, Training Loss: 0.78\n","Epoch: 216, Training Loss: 0.78\n","Epoch: 219, Training Loss: 0.78\n","Epoch: 222, Training Loss: 0.77\n","Epoch: 225, Training Loss: 0.78\n","Epoch: 228, Training Loss: 0.77\n","Epoch: 231, Training Loss: 0.77\n","Epoch: 234, Training Loss: 0.77\n","Epoch: 237, Training Loss: 0.77\n","Epoch: 240, Training Loss: 0.77\n","Epoch: 243, Training Loss: 0.76\n","Best loss achieved:  0.8738996386528015\n","Epoch: 1, Training Loss: 1.01\n","Epoch: 3, Training Loss: 1.00\n","Epoch: 6, Training Loss: 0.99\n","Epoch: 9, Training Loss: 0.99\n","Epoch: 12, Training Loss: 0.98\n","Epoch: 15, Training Loss: 0.98\n","Epoch: 18, Training Loss: 0.97\n","Epoch: 21, Training Loss: 0.97\n","Epoch: 24, Training Loss: 0.96\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.96\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.95\n","Epoch: 39, Training Loss: 0.94\n","Epoch: 42, Training Loss: 0.94\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.93\n","Epoch: 51, Training Loss: 0.92\n","Epoch: 54, Training Loss: 0.92\n","Epoch: 57, Training Loss: 0.91\n","Epoch: 60, Training Loss: 0.91\n","Epoch: 63, Training Loss: 0.90\n","Epoch: 66, Training Loss: 0.90\n","Epoch: 69, Training Loss: 0.89\n","Epoch: 72, Training Loss: 0.89\n","Epoch: 75, Training Loss: 0.88\n","Epoch: 78, Training Loss: 0.88\n","Epoch: 81, Training Loss: 0.87\n","Epoch: 84, Training Loss: 0.87\n","Epoch: 87, Training Loss: 0.86\n","Epoch: 90, Training Loss: 0.86\n","Epoch: 93, Training Loss: 0.86\n","Epoch: 96, Training Loss: 0.85\n","Epoch: 99, Training Loss: 0.85\n","Epoch [100/2000], Test Loss: 0.9076815247535706\n","Epoch: 102, Training Loss: 0.84\n","Epoch: 105, Training Loss: 0.84\n","Epoch: 108, Training Loss: 0.84\n","Epoch: 111, Training Loss: 0.83\n","Epoch: 114, Training Loss: 0.83\n","Epoch: 117, Training Loss: 0.83\n","Epoch: 120, Training Loss: 0.82\n","Epoch: 123, Training Loss: 0.82\n","Epoch: 126, Training Loss: 0.82\n","Epoch: 129, Training Loss: 0.81\n","Epoch: 132, Training Loss: 0.82\n","Epoch: 135, Training Loss: 0.82\n","Epoch: 138, Training Loss: 0.81\n","Epoch: 141, Training Loss: 0.81\n","Epoch: 144, Training Loss: 0.81\n","Epoch: 147, Training Loss: 0.80\n","Epoch: 150, Training Loss: 0.80\n","Epoch: 153, Training Loss: 0.80\n","Epoch: 156, Training Loss: 0.80\n","Epoch: 159, Training Loss: 0.79\n","Epoch: 162, Training Loss: 0.80\n","Epoch: 165, Training Loss: 0.79\n","Epoch: 168, Training Loss: 0.79\n","Epoch: 171, Training Loss: 0.79\n","Epoch: 174, Training Loss: 0.79\n","Epoch: 177, Training Loss: 0.78\n","Epoch: 180, Training Loss: 0.78\n","Epoch: 183, Training Loss: 0.78\n","Epoch: 186, Training Loss: 0.78\n","Epoch: 189, Training Loss: 0.78\n","Epoch: 192, Training Loss: 0.78\n","Epoch: 195, Training Loss: 0.78\n","Epoch: 198, Training Loss: 0.77\n","Epoch [200/2000], Test Loss: 0.8757145404815674\n","Epoch: 201, Training Loss: 0.78\n","Epoch: 204, Training Loss: 0.78\n","Epoch: 207, Training Loss: 0.78\n","Epoch: 210, Training Loss: 0.77\n","Epoch: 213, Training Loss: 0.77\n","Epoch: 216, Training Loss: 0.77\n","Epoch: 219, Training Loss: 0.77\n","Epoch: 222, Training Loss: 0.77\n","Epoch: 225, Training Loss: 0.77\n","Epoch: 228, Training Loss: 0.77\n","Epoch: 231, Training Loss: 0.77\n","Epoch: 234, Training Loss: 0.76\n","Epoch: 237, Training Loss: 0.77\n","Epoch: 240, Training Loss: 0.76\n","Epoch: 243, Training Loss: 0.76\n","Epoch: 246, Training Loss: 0.76\n","Epoch: 249, Training Loss: 0.76\n","Epoch: 252, Training Loss: 0.76\n","Epoch: 255, Training Loss: 0.76\n","Epoch: 258, Training Loss: 0.76\n","Epoch: 261, Training Loss: 0.76\n","Epoch: 264, Training Loss: 0.76\n","Epoch: 267, Training Loss: 0.75\n","Epoch: 270, Training Loss: 0.75\n","Epoch: 273, Training Loss: 0.75\n","Epoch: 276, Training Loss: 0.75\n","Epoch: 279, Training Loss: 0.75\n","Epoch: 282, Training Loss: 0.75\n","Epoch: 285, Training Loss: 0.75\n","Epoch: 288, Training Loss: 0.75\n","Epoch: 291, Training Loss: 0.75\n","Epoch: 294, Training Loss: 0.74\n","Epoch: 297, Training Loss: 0.74\n","Epoch: 300, Training Loss: 0.74\n","Epoch [300/2000], Test Loss: 0.8723980188369751\n","Epoch: 303, Training Loss: 0.74\n","Epoch: 306, Training Loss: 0.74\n","Epoch: 309, Training Loss: 0.74\n","Epoch: 312, Training Loss: 0.74\n","Epoch: 315, Training Loss: 0.74\n","Epoch: 318, Training Loss: 0.74\n","Epoch: 321, Training Loss: 0.74\n","Epoch: 324, Training Loss: 0.74\n","Epoch: 327, Training Loss: 0.74\n","Epoch: 330, Training Loss: 0.74\n","Epoch: 333, Training Loss: 0.73\n","Epoch: 336, Training Loss: 0.74\n","Epoch: 339, Training Loss: 0.74\n","Epoch: 342, Training Loss: 0.73\n","Epoch: 345, Training Loss: 0.73\n","Epoch: 348, Training Loss: 0.73\n","Epoch: 351, Training Loss: 0.73\n","Epoch: 354, Training Loss: 0.73\n","Epoch: 357, Training Loss: 0.73\n","Epoch: 360, Training Loss: 0.73\n","Epoch: 363, Training Loss: 0.72\n","Epoch: 366, Training Loss: 0.72\n","Epoch: 369, Training Loss: 0.73\n","Epoch: 372, Training Loss: 0.72\n","Epoch: 375, Training Loss: 0.72\n","Epoch: 378, Training Loss: 0.72\n","Epoch: 381, Training Loss: 0.72\n","Epoch: 384, Training Loss: 0.72\n","Epoch: 387, Training Loss: 0.72\n","Best loss achieved:  0.8635353446006775\n","Epoch: 1, Training Loss: 2.10\n","Epoch: 3, Training Loss: 1.59\n","Epoch: 6, Training Loss: 1.23\n","Epoch: 9, Training Loss: 1.10\n","Epoch: 12, Training Loss: 1.01\n","Epoch: 15, Training Loss: 0.98\n","Epoch: 18, Training Loss: 0.98\n","Epoch: 21, Training Loss: 0.98\n","Epoch: 24, Training Loss: 0.97\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.96\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.95\n","Epoch: 39, Training Loss: 0.95\n","Epoch: 42, Training Loss: 0.94\n","Epoch: 45, Training Loss: 0.94\n","Epoch: 48, Training Loss: 0.93\n","Epoch: 51, Training Loss: 0.93\n","Epoch: 54, Training Loss: 0.93\n","Epoch: 57, Training Loss: 0.92\n","Epoch: 60, Training Loss: 0.92\n","Epoch: 63, Training Loss: 0.92\n","Epoch: 66, Training Loss: 0.91\n","Epoch: 69, Training Loss: 0.91\n","Epoch: 72, Training Loss: 0.91\n","Epoch: 75, Training Loss: 0.90\n","Epoch: 78, Training Loss: 0.91\n","Epoch: 81, Training Loss: 0.90\n","Epoch: 84, Training Loss: 0.89\n","Epoch: 87, Training Loss: 0.89\n","Epoch: 90, Training Loss: 0.89\n","Epoch: 93, Training Loss: 0.89\n","Epoch: 96, Training Loss: 0.88\n","Epoch: 99, Training Loss: 0.88\n","Epoch [100/2000], Test Loss: 0.9311836957931519\n","Epoch: 102, Training Loss: 0.88\n","Epoch: 105, Training Loss: 0.88\n","Epoch: 108, Training Loss: 0.87\n","Epoch: 111, Training Loss: 0.87\n","Epoch: 114, Training Loss: 0.86\n","Epoch: 117, Training Loss: 0.86\n","Epoch: 120, Training Loss: 0.86\n","Epoch: 123, Training Loss: 0.86\n","Epoch: 126, Training Loss: 0.86\n","Epoch: 129, Training Loss: 0.85\n","Epoch: 132, Training Loss: 0.85\n","Epoch: 135, Training Loss: 0.85\n","Epoch: 138, Training Loss: 0.85\n","Epoch: 141, Training Loss: 0.85\n","Epoch: 144, Training Loss: 0.84\n","Epoch: 147, Training Loss: 0.84\n","Epoch: 150, Training Loss: 0.84\n","Epoch: 153, Training Loss: 0.84\n","Epoch: 156, Training Loss: 0.84\n","Epoch: 159, Training Loss: 0.83\n","Epoch: 162, Training Loss: 0.83\n","Epoch: 165, Training Loss: 0.83\n","Epoch: 168, Training Loss: 0.83\n","Epoch: 171, Training Loss: 0.82\n","Epoch: 174, Training Loss: 0.82\n","Epoch: 177, Training Loss: 0.82\n","Epoch: 180, Training Loss: 0.82\n","Epoch: 183, Training Loss: 0.82\n","Epoch: 186, Training Loss: 0.82\n","Epoch: 189, Training Loss: 0.82\n","Epoch: 192, Training Loss: 0.81\n","Epoch: 195, Training Loss: 0.81\n","Epoch: 198, Training Loss: 0.81\n","Epoch [200/2000], Test Loss: 0.8708078861236572\n","Epoch: 201, Training Loss: 0.81\n","Epoch: 204, Training Loss: 0.81\n","Epoch: 207, Training Loss: 0.80\n","Epoch: 210, Training Loss: 0.80\n","Epoch: 213, Training Loss: 0.80\n","Epoch: 216, Training Loss: 0.80\n","Epoch: 219, Training Loss: 0.80\n","Epoch: 222, Training Loss: 0.80\n","Epoch: 225, Training Loss: 0.80\n","Epoch: 228, Training Loss: 0.79\n","Epoch: 231, Training Loss: 0.79\n","Epoch: 234, Training Loss: 0.79\n","Epoch: 237, Training Loss: 0.80\n","Epoch: 240, Training Loss: 0.79\n","Epoch: 243, Training Loss: 0.79\n","Epoch: 246, Training Loss: 0.79\n","Epoch: 249, Training Loss: 0.79\n","Epoch: 252, Training Loss: 0.78\n","Epoch: 255, Training Loss: 0.78\n","Epoch: 258, Training Loss: 0.78\n","Epoch: 261, Training Loss: 0.78\n","Epoch: 264, Training Loss: 0.78\n","Epoch: 267, Training Loss: 0.78\n","Epoch: 270, Training Loss: 0.78\n","Epoch: 273, Training Loss: 0.77\n","Epoch: 276, Training Loss: 0.78\n","Epoch: 279, Training Loss: 0.78\n","Epoch: 282, Training Loss: 0.77\n","Epoch: 285, Training Loss: 0.77\n","Epoch: 288, Training Loss: 0.77\n","Epoch: 291, Training Loss: 0.77\n","Epoch: 294, Training Loss: 0.77\n","Epoch: 297, Training Loss: 0.77\n","Epoch: 300, Training Loss: 0.76\n","Epoch [300/2000], Test Loss: 0.8420373797416687\n","Epoch: 303, Training Loss: 0.77\n","Epoch: 306, Training Loss: 0.77\n","Epoch: 309, Training Loss: 0.76\n","Epoch: 312, Training Loss: 0.77\n","Epoch: 315, Training Loss: 0.76\n","Epoch: 318, Training Loss: 0.76\n","Epoch: 321, Training Loss: 0.76\n","Epoch: 324, Training Loss: 0.76\n","Epoch: 327, Training Loss: 0.76\n","Epoch: 330, Training Loss: 0.75\n","Epoch: 333, Training Loss: 0.76\n","Epoch: 336, Training Loss: 0.75\n","Epoch: 339, Training Loss: 0.75\n","Epoch: 342, Training Loss: 0.75\n","Epoch: 345, Training Loss: 0.75\n","Epoch: 348, Training Loss: 0.75\n","Epoch: 351, Training Loss: 0.75\n","Epoch: 354, Training Loss: 0.75\n","Epoch: 357, Training Loss: 0.75\n","Epoch: 360, Training Loss: 0.74\n","Epoch: 363, Training Loss: 0.75\n","Epoch: 366, Training Loss: 0.74\n","Epoch: 369, Training Loss: 0.74\n","Epoch: 372, Training Loss: 0.74\n","Epoch: 375, Training Loss: 0.74\n","Epoch: 378, Training Loss: 0.74\n","Epoch: 381, Training Loss: 0.74\n","Epoch: 384, Training Loss: 0.74\n","Epoch: 387, Training Loss: 0.74\n","Epoch: 390, Training Loss: 0.74\n","Epoch: 393, Training Loss: 0.74\n","Epoch: 396, Training Loss: 0.74\n","Epoch: 399, Training Loss: 0.73\n","Epoch [400/2000], Test Loss: 0.8291559219360352\n","Epoch: 402, Training Loss: 0.73\n","Epoch: 405, Training Loss: 0.73\n","Epoch: 408, Training Loss: 0.73\n","Epoch: 411, Training Loss: 0.73\n","Epoch: 414, Training Loss: 0.73\n","Epoch: 417, Training Loss: 0.73\n","Epoch: 420, Training Loss: 0.73\n","Epoch: 423, Training Loss: 0.73\n","Epoch: 426, Training Loss: 0.72\n","Epoch: 429, Training Loss: 0.73\n","Epoch: 432, Training Loss: 0.72\n","Epoch: 435, Training Loss: 0.72\n","Epoch: 438, Training Loss: 0.72\n","Epoch: 441, Training Loss: 0.72\n","Epoch: 444, Training Loss: 0.72\n","Epoch: 447, Training Loss: 0.72\n","Epoch: 450, Training Loss: 0.72\n","Epoch: 453, Training Loss: 0.72\n","Epoch: 456, Training Loss: 0.72\n","Epoch: 459, Training Loss: 0.72\n","Epoch: 462, Training Loss: 0.71\n","Epoch: 465, Training Loss: 0.71\n","Epoch: 468, Training Loss: 0.71\n","Epoch: 471, Training Loss: 0.71\n","Epoch: 474, Training Loss: 0.71\n","Epoch: 477, Training Loss: 0.71\n","Epoch: 480, Training Loss: 0.71\n","Epoch: 483, Training Loss: 0.71\n","Epoch: 486, Training Loss: 0.71\n","Epoch: 489, Training Loss: 0.70\n","Epoch: 492, Training Loss: 0.70\n","Epoch: 495, Training Loss: 0.71\n","Epoch: 498, Training Loss: 0.70\n","Epoch [500/2000], Test Loss: 0.8264631032943726\n","Epoch: 501, Training Loss: 0.70\n","Epoch: 504, Training Loss: 0.70\n","Epoch: 507, Training Loss: 0.70\n","Epoch: 510, Training Loss: 0.70\n","Epoch: 513, Training Loss: 0.70\n","Epoch: 516, Training Loss: 0.70\n","Epoch: 519, Training Loss: 0.70\n","Epoch: 522, Training Loss: 0.71\n","Epoch: 525, Training Loss: 0.70\n","Epoch: 528, Training Loss: 0.70\n","Epoch: 531, Training Loss: 0.69\n","Epoch: 534, Training Loss: 0.69\n","Epoch: 537, Training Loss: 0.69\n","Epoch: 540, Training Loss: 0.70\n","Epoch: 543, Training Loss: 0.69\n","Epoch: 546, Training Loss: 0.69\n","Epoch: 549, Training Loss: 0.69\n","Epoch: 552, Training Loss: 0.69\n","Epoch: 555, Training Loss: 0.69\n","Epoch: 558, Training Loss: 0.68\n","Epoch: 561, Training Loss: 0.68\n","Epoch: 564, Training Loss: 0.69\n","Best loss achieved:  0.8221989274024963\n","Epoch: 1, Training Loss: 1.29\n","Epoch: 3, Training Loss: 1.14\n","Epoch: 6, Training Loss: 1.06\n","Epoch: 9, Training Loss: 1.04\n","Epoch: 12, Training Loss: 1.02\n","Epoch: 15, Training Loss: 1.00\n","Epoch: 18, Training Loss: 1.00\n","Epoch: 21, Training Loss: 0.98\n","Epoch: 24, Training Loss: 0.97\n","Epoch: 27, Training Loss: 0.97\n","Epoch: 30, Training Loss: 0.96\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.95\n","Epoch: 39, Training Loss: 0.94\n","Epoch: 42, Training Loss: 0.94\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.92\n","Epoch: 51, Training Loss: 0.92\n","Epoch: 54, Training Loss: 0.92\n","Epoch: 57, Training Loss: 0.91\n","Epoch: 60, Training Loss: 0.91\n","Epoch: 63, Training Loss: 0.91\n","Epoch: 66, Training Loss: 0.90\n","Epoch: 69, Training Loss: 0.89\n","Epoch: 72, Training Loss: 0.89\n","Epoch: 75, Training Loss: 0.89\n","Epoch: 78, Training Loss: 0.88\n","Epoch: 81, Training Loss: 0.88\n","Epoch: 84, Training Loss: 0.87\n","Epoch: 87, Training Loss: 0.87\n","Epoch: 90, Training Loss: 0.86\n","Epoch: 93, Training Loss: 0.86\n","Epoch: 96, Training Loss: 0.86\n","Epoch: 99, Training Loss: 0.85\n","Epoch [100/2000], Test Loss: 0.8811151385307312\n","Epoch: 102, Training Loss: 0.85\n","Epoch: 105, Training Loss: 0.84\n","Epoch: 108, Training Loss: 0.84\n","Epoch: 111, Training Loss: 0.84\n","Epoch: 114, Training Loss: 0.84\n","Epoch: 117, Training Loss: 0.83\n","Epoch: 120, Training Loss: 0.83\n","Epoch: 123, Training Loss: 0.83\n","Epoch: 126, Training Loss: 0.83\n","Epoch: 129, Training Loss: 0.82\n","Epoch: 132, Training Loss: 0.82\n","Epoch: 135, Training Loss: 0.82\n","Epoch: 138, Training Loss: 0.82\n","Epoch: 141, Training Loss: 0.82\n","Epoch: 144, Training Loss: 0.81\n","Epoch: 147, Training Loss: 0.81\n","Epoch: 150, Training Loss: 0.81\n","Epoch: 153, Training Loss: 0.81\n","Epoch: 156, Training Loss: 0.81\n","Epoch: 159, Training Loss: 0.81\n","Epoch: 162, Training Loss: 0.80\n","Epoch: 165, Training Loss: 0.81\n","Epoch: 168, Training Loss: 0.80\n","Epoch: 171, Training Loss: 0.80\n","Epoch: 174, Training Loss: 0.80\n","Epoch: 177, Training Loss: 0.80\n","Epoch: 180, Training Loss: 0.79\n","Epoch: 183, Training Loss: 0.79\n","Epoch: 186, Training Loss: 0.79\n","Epoch: 189, Training Loss: 0.79\n","Epoch: 192, Training Loss: 0.79\n","Epoch: 195, Training Loss: 0.79\n","Epoch: 198, Training Loss: 0.79\n","Epoch [200/2000], Test Loss: 0.8324065804481506\n","Epoch: 201, Training Loss: 0.78\n","Epoch: 204, Training Loss: 0.78\n","Epoch: 207, Training Loss: 0.78\n","Epoch: 210, Training Loss: 0.78\n","Epoch: 213, Training Loss: 0.78\n","Epoch: 216, Training Loss: 0.78\n","Epoch: 219, Training Loss: 0.78\n","Epoch: 222, Training Loss: 0.78\n","Epoch: 225, Training Loss: 0.78\n","Epoch: 228, Training Loss: 0.78\n","Epoch: 231, Training Loss: 0.77\n","Epoch: 234, Training Loss: 0.77\n","Epoch: 237, Training Loss: 0.77\n","Epoch: 240, Training Loss: 0.77\n","Epoch: 243, Training Loss: 0.77\n","Epoch: 246, Training Loss: 0.77\n","Epoch: 249, Training Loss: 0.77\n","Epoch: 252, Training Loss: 0.77\n","Epoch: 255, Training Loss: 0.76\n","Epoch: 258, Training Loss: 0.77\n","Epoch: 261, Training Loss: 0.77\n","Epoch: 264, Training Loss: 0.76\n","Epoch: 267, Training Loss: 0.76\n","Epoch: 270, Training Loss: 0.76\n","Epoch: 273, Training Loss: 0.76\n","Epoch: 276, Training Loss: 0.76\n","Epoch: 279, Training Loss: 0.76\n","Epoch: 282, Training Loss: 0.76\n","Epoch: 285, Training Loss: 0.75\n","Epoch: 288, Training Loss: 0.75\n","Epoch: 291, Training Loss: 0.76\n","Epoch: 294, Training Loss: 0.76\n","Epoch: 297, Training Loss: 0.75\n","Epoch: 300, Training Loss: 0.75\n","Epoch [300/2000], Test Loss: 0.8237566351890564\n","Epoch: 303, Training Loss: 0.75\n","Epoch: 306, Training Loss: 0.75\n","Epoch: 309, Training Loss: 0.75\n","Epoch: 312, Training Loss: 0.74\n","Epoch: 315, Training Loss: 0.75\n","Epoch: 318, Training Loss: 0.75\n","Epoch: 321, Training Loss: 0.75\n","Epoch: 324, Training Loss: 0.74\n","Epoch: 327, Training Loss: 0.74\n","Epoch: 330, Training Loss: 0.74\n","Epoch: 333, Training Loss: 0.74\n","Epoch: 336, Training Loss: 0.74\n","Epoch: 339, Training Loss: 0.74\n","Epoch: 342, Training Loss: 0.74\n","Epoch: 345, Training Loss: 0.73\n","Epoch: 348, Training Loss: 0.74\n","Epoch: 351, Training Loss: 0.73\n","Epoch: 354, Training Loss: 0.74\n","Epoch: 357, Training Loss: 0.74\n","Epoch: 360, Training Loss: 0.73\n","Epoch: 363, Training Loss: 0.73\n","Epoch: 366, Training Loss: 0.73\n","Epoch: 369, Training Loss: 0.73\n","Epoch: 372, Training Loss: 0.73\n","Epoch: 375, Training Loss: 0.73\n","Epoch: 378, Training Loss: 0.73\n","Epoch: 381, Training Loss: 0.72\n","Epoch: 384, Training Loss: 0.73\n","Epoch: 387, Training Loss: 0.73\n","Epoch: 390, Training Loss: 0.72\n","Best loss achieved:  0.815897524356842\n","Epoch: 1, Training Loss: 1.12\n","Epoch: 3, Training Loss: 1.02\n","Epoch: 6, Training Loss: 1.00\n","Epoch: 9, Training Loss: 0.99\n","Epoch: 12, Training Loss: 0.98\n","Epoch: 15, Training Loss: 0.97\n","Epoch: 18, Training Loss: 0.97\n","Epoch: 21, Training Loss: 0.96\n","Epoch: 24, Training Loss: 0.96\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.95\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.94\n","Epoch: 39, Training Loss: 0.94\n","Epoch: 42, Training Loss: 0.93\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.92\n","Epoch: 51, Training Loss: 0.92\n","Epoch: 54, Training Loss: 0.92\n","Epoch: 57, Training Loss: 0.91\n","Epoch: 60, Training Loss: 0.91\n","Epoch: 63, Training Loss: 0.90\n","Epoch: 66, Training Loss: 0.90\n","Epoch: 69, Training Loss: 0.90\n","Epoch: 72, Training Loss: 0.89\n","Epoch: 75, Training Loss: 0.88\n","Epoch: 78, Training Loss: 0.88\n","Epoch: 81, Training Loss: 0.88\n","Epoch: 84, Training Loss: 0.88\n","Epoch: 87, Training Loss: 0.88\n","Epoch: 90, Training Loss: 0.87\n","Epoch: 93, Training Loss: 0.87\n","Epoch: 96, Training Loss: 0.86\n","Epoch: 99, Training Loss: 0.86\n","Epoch [100/2000], Test Loss: 0.9178293943405151\n","Epoch: 102, Training Loss: 0.85\n","Epoch: 105, Training Loss: 0.85\n","Epoch: 108, Training Loss: 0.85\n","Epoch: 111, Training Loss: 0.85\n","Epoch: 114, Training Loss: 0.84\n","Epoch: 117, Training Loss: 0.84\n","Epoch: 120, Training Loss: 0.83\n","Epoch: 123, Training Loss: 0.83\n","Epoch: 126, Training Loss: 0.83\n","Epoch: 129, Training Loss: 0.83\n","Epoch: 132, Training Loss: 0.83\n","Epoch: 135, Training Loss: 0.83\n","Epoch: 138, Training Loss: 0.82\n","Epoch: 141, Training Loss: 0.82\n","Epoch: 144, Training Loss: 0.81\n","Epoch: 147, Training Loss: 0.81\n","Epoch: 150, Training Loss: 0.81\n","Epoch: 153, Training Loss: 0.81\n","Epoch: 156, Training Loss: 0.81\n","Epoch: 159, Training Loss: 0.81\n","Epoch: 162, Training Loss: 0.80\n","Epoch: 165, Training Loss: 0.80\n","Epoch: 168, Training Loss: 0.80\n","Epoch: 171, Training Loss: 0.80\n","Epoch: 174, Training Loss: 0.80\n","Epoch: 177, Training Loss: 0.79\n","Epoch: 180, Training Loss: 0.80\n","Epoch: 183, Training Loss: 0.79\n","Epoch: 186, Training Loss: 0.79\n","Epoch: 189, Training Loss: 0.79\n","Epoch: 192, Training Loss: 0.79\n","Epoch: 195, Training Loss: 0.79\n","Epoch: 198, Training Loss: 0.79\n","Epoch [200/2000], Test Loss: 0.8640544414520264\n","Epoch: 201, Training Loss: 0.79\n","Epoch: 204, Training Loss: 0.78\n","Epoch: 207, Training Loss: 0.78\n","Epoch: 210, Training Loss: 0.78\n","Epoch: 213, Training Loss: 0.78\n","Epoch: 216, Training Loss: 0.78\n","Epoch: 219, Training Loss: 0.78\n","Epoch: 222, Training Loss: 0.78\n","Epoch: 225, Training Loss: 0.77\n","Epoch: 228, Training Loss: 0.77\n","Epoch: 231, Training Loss: 0.77\n","Epoch: 234, Training Loss: 0.77\n","Epoch: 237, Training Loss: 0.77\n","Epoch: 240, Training Loss: 0.77\n","Epoch: 243, Training Loss: 0.76\n","Epoch: 246, Training Loss: 0.76\n","Epoch: 249, Training Loss: 0.76\n","Epoch: 252, Training Loss: 0.76\n","Epoch: 255, Training Loss: 0.76\n","Epoch: 258, Training Loss: 0.76\n","Epoch: 261, Training Loss: 0.76\n","Epoch: 264, Training Loss: 0.76\n","Epoch: 267, Training Loss: 0.76\n","Epoch: 270, Training Loss: 0.76\n","Epoch: 273, Training Loss: 0.75\n","Epoch: 276, Training Loss: 0.75\n","Epoch: 279, Training Loss: 0.75\n","Epoch: 282, Training Loss: 0.75\n","Epoch: 285, Training Loss: 0.75\n","Epoch: 288, Training Loss: 0.75\n","Epoch: 291, Training Loss: 0.75\n","Epoch: 294, Training Loss: 0.75\n","Epoch: 297, Training Loss: 0.75\n","Epoch: 300, Training Loss: 0.74\n","Epoch [300/2000], Test Loss: 0.8451523780822754\n","Epoch: 303, Training Loss: 0.74\n","Epoch: 306, Training Loss: 0.75\n","Epoch: 309, Training Loss: 0.74\n","Epoch: 312, Training Loss: 0.74\n","Epoch: 315, Training Loss: 0.74\n","Epoch: 318, Training Loss: 0.74\n","Epoch: 321, Training Loss: 0.74\n","Epoch: 324, Training Loss: 0.74\n","Epoch: 327, Training Loss: 0.73\n","Epoch: 330, Training Loss: 0.74\n","Epoch: 333, Training Loss: 0.74\n","Epoch: 336, Training Loss: 0.74\n","Epoch: 339, Training Loss: 0.74\n","Epoch: 342, Training Loss: 0.73\n","Epoch: 345, Training Loss: 0.73\n","Epoch: 348, Training Loss: 0.73\n","Epoch: 351, Training Loss: 0.73\n","Epoch: 354, Training Loss: 0.73\n","Epoch: 357, Training Loss: 0.73\n","Epoch: 360, Training Loss: 0.72\n","Epoch: 363, Training Loss: 0.72\n","Epoch: 366, Training Loss: 0.72\n","Epoch: 369, Training Loss: 0.72\n","Epoch: 372, Training Loss: 0.72\n","Epoch: 375, Training Loss: 0.73\n","Epoch: 378, Training Loss: 0.72\n","Epoch: 381, Training Loss: 0.72\n","Epoch: 384, Training Loss: 0.72\n","Epoch: 387, Training Loss: 0.71\n","Epoch: 390, Training Loss: 0.72\n","Best loss achieved:  0.8422195911407471\n","Epoch: 1, Training Loss: 1.04\n","Epoch: 3, Training Loss: 1.02\n","Epoch: 6, Training Loss: 1.00\n","Epoch: 9, Training Loss: 1.00\n","Epoch: 12, Training Loss: 0.99\n","Epoch: 15, Training Loss: 0.98\n","Epoch: 18, Training Loss: 0.97\n","Epoch: 21, Training Loss: 0.97\n","Epoch: 24, Training Loss: 0.97\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.95\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.94\n","Epoch: 39, Training Loss: 0.94\n","Epoch: 42, Training Loss: 0.93\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.93\n","Epoch: 51, Training Loss: 0.93\n","Epoch: 54, Training Loss: 0.92\n","Epoch: 57, Training Loss: 0.91\n","Epoch: 60, Training Loss: 0.91\n","Epoch: 63, Training Loss: 0.91\n","Epoch: 66, Training Loss: 0.90\n","Epoch: 69, Training Loss: 0.90\n","Epoch: 72, Training Loss: 0.90\n","Epoch: 75, Training Loss: 0.89\n","Epoch: 78, Training Loss: 0.89\n","Epoch: 81, Training Loss: 0.89\n","Epoch: 84, Training Loss: 0.88\n","Epoch: 87, Training Loss: 0.88\n","Epoch: 90, Training Loss: 0.87\n","Epoch: 93, Training Loss: 0.87\n","Epoch: 96, Training Loss: 0.87\n","Epoch: 99, Training Loss: 0.86\n","Epoch [100/2000], Test Loss: 0.9161144495010376\n","Epoch: 102, Training Loss: 0.86\n","Epoch: 105, Training Loss: 0.86\n","Epoch: 108, Training Loss: 0.86\n","Epoch: 111, Training Loss: 0.85\n","Epoch: 114, Training Loss: 0.85\n","Epoch: 117, Training Loss: 0.84\n","Epoch: 120, Training Loss: 0.85\n","Epoch: 123, Training Loss: 0.84\n","Epoch: 126, Training Loss: 0.84\n","Epoch: 129, Training Loss: 0.84\n","Epoch: 132, Training Loss: 0.83\n","Epoch: 135, Training Loss: 0.83\n","Epoch: 138, Training Loss: 0.83\n","Epoch: 141, Training Loss: 0.83\n","Epoch: 144, Training Loss: 0.82\n","Epoch: 147, Training Loss: 0.82\n","Epoch: 150, Training Loss: 0.82\n","Epoch: 153, Training Loss: 0.82\n","Epoch: 156, Training Loss: 0.82\n","Epoch: 159, Training Loss: 0.82\n","Epoch: 162, Training Loss: 0.81\n","Epoch: 165, Training Loss: 0.81\n","Epoch: 168, Training Loss: 0.81\n","Epoch: 171, Training Loss: 0.81\n","Epoch: 174, Training Loss: 0.80\n","Epoch: 177, Training Loss: 0.80\n","Epoch: 180, Training Loss: 0.81\n","Epoch: 183, Training Loss: 0.80\n","Epoch: 186, Training Loss: 0.80\n","Epoch: 189, Training Loss: 0.80\n","Epoch: 192, Training Loss: 0.79\n","Epoch: 195, Training Loss: 0.80\n","Epoch: 198, Training Loss: 0.79\n","Epoch [200/2000], Test Loss: 0.8801589012145996\n","Epoch: 201, Training Loss: 0.79\n","Epoch: 204, Training Loss: 0.79\n","Epoch: 207, Training Loss: 0.79\n","Epoch: 210, Training Loss: 0.79\n","Epoch: 213, Training Loss: 0.79\n","Epoch: 216, Training Loss: 0.79\n","Epoch: 219, Training Loss: 0.79\n","Epoch: 222, Training Loss: 0.78\n","Epoch: 225, Training Loss: 0.78\n","Epoch: 228, Training Loss: 0.78\n","Epoch: 231, Training Loss: 0.78\n","Epoch: 234, Training Loss: 0.78\n","Epoch: 237, Training Loss: 0.78\n","Epoch: 240, Training Loss: 0.77\n","Epoch: 243, Training Loss: 0.78\n","Epoch: 246, Training Loss: 0.78\n","Epoch: 249, Training Loss: 0.77\n","Epoch: 252, Training Loss: 0.77\n","Epoch: 255, Training Loss: 0.77\n","Epoch: 258, Training Loss: 0.77\n","Epoch: 261, Training Loss: 0.77\n","Epoch: 264, Training Loss: 0.77\n","Epoch: 267, Training Loss: 0.77\n","Epoch: 270, Training Loss: 0.77\n","Epoch: 273, Training Loss: 0.77\n","Epoch: 276, Training Loss: 0.77\n","Epoch: 279, Training Loss: 0.76\n","Epoch: 282, Training Loss: 0.76\n","Epoch: 285, Training Loss: 0.76\n","Epoch: 288, Training Loss: 0.76\n","Epoch: 291, Training Loss: 0.76\n","Epoch: 294, Training Loss: 0.76\n","Epoch: 297, Training Loss: 0.76\n","Epoch: 300, Training Loss: 0.76\n","Epoch [300/2000], Test Loss: 0.8774996399879456\n","Epoch: 303, Training Loss: 0.76\n","Epoch: 306, Training Loss: 0.76\n","Epoch: 309, Training Loss: 0.76\n","Epoch: 312, Training Loss: 0.76\n","Epoch: 315, Training Loss: 0.76\n","Epoch: 318, Training Loss: 0.75\n","Epoch: 321, Training Loss: 0.75\n","Epoch: 324, Training Loss: 0.75\n","Epoch: 327, Training Loss: 0.75\n","Epoch: 330, Training Loss: 0.76\n","Best loss achieved:  0.875551164150238\n","Epoch: 1, Training Loss: 1.21\n","Epoch: 3, Training Loss: 1.07\n","Epoch: 6, Training Loss: 1.04\n","Epoch: 9, Training Loss: 1.01\n","Epoch: 12, Training Loss: 0.99\n","Epoch: 15, Training Loss: 0.99\n","Epoch: 18, Training Loss: 0.97\n","Epoch: 21, Training Loss: 0.97\n","Epoch: 24, Training Loss: 0.96\n","Epoch: 27, Training Loss: 0.95\n","Epoch: 30, Training Loss: 0.94\n","Epoch: 33, Training Loss: 0.94\n","Epoch: 36, Training Loss: 0.93\n","Epoch: 39, Training Loss: 0.93\n","Epoch: 42, Training Loss: 0.92\n","Epoch: 45, Training Loss: 0.91\n","Epoch: 48, Training Loss: 0.91\n","Epoch: 51, Training Loss: 0.90\n","Epoch: 54, Training Loss: 0.90\n","Epoch: 57, Training Loss: 0.89\n","Epoch: 60, Training Loss: 0.89\n","Epoch: 63, Training Loss: 0.88\n","Epoch: 66, Training Loss: 0.88\n","Epoch: 69, Training Loss: 0.88\n","Epoch: 72, Training Loss: 0.87\n","Epoch: 75, Training Loss: 0.87\n","Epoch: 78, Training Loss: 0.87\n","Epoch: 81, Training Loss: 0.86\n","Epoch: 84, Training Loss: 0.86\n","Epoch: 87, Training Loss: 0.85\n","Epoch: 90, Training Loss: 0.85\n","Epoch: 93, Training Loss: 0.85\n","Epoch: 96, Training Loss: 0.84\n","Epoch: 99, Training Loss: 0.84\n","Epoch [100/2000], Test Loss: 0.8168354630470276\n","Epoch: 102, Training Loss: 0.84\n","Epoch: 105, Training Loss: 0.84\n","Epoch: 108, Training Loss: 0.83\n","Epoch: 111, Training Loss: 0.83\n","Epoch: 114, Training Loss: 0.82\n","Epoch: 117, Training Loss: 0.82\n","Epoch: 120, Training Loss: 0.83\n","Epoch: 123, Training Loss: 0.82\n","Epoch: 126, Training Loss: 0.82\n","Epoch: 129, Training Loss: 0.82\n","Epoch: 132, Training Loss: 0.82\n","Epoch: 135, Training Loss: 0.82\n","Epoch: 138, Training Loss: 0.81\n","Epoch: 141, Training Loss: 0.81\n","Epoch: 144, Training Loss: 0.81\n","Epoch: 147, Training Loss: 0.81\n","Epoch: 150, Training Loss: 0.80\n","Epoch: 153, Training Loss: 0.80\n","Epoch: 156, Training Loss: 0.80\n","Epoch: 159, Training Loss: 0.80\n","Epoch: 162, Training Loss: 0.80\n","Epoch: 165, Training Loss: 0.80\n","Epoch: 168, Training Loss: 0.80\n","Epoch: 171, Training Loss: 0.79\n","Epoch: 174, Training Loss: 0.79\n","Epoch: 177, Training Loss: 0.79\n","Epoch: 180, Training Loss: 0.79\n","Epoch: 183, Training Loss: 0.79\n","Epoch: 186, Training Loss: 0.79\n","Epoch: 189, Training Loss: 0.79\n","Epoch: 192, Training Loss: 0.79\n","Epoch: 195, Training Loss: 0.78\n","Epoch: 198, Training Loss: 0.78\n","Epoch [200/2000], Test Loss: 0.7995910048484802\n","Epoch: 201, Training Loss: 0.78\n","Epoch: 204, Training Loss: 0.78\n","Best loss achieved:  0.7946975231170654\n","Epoch: 1, Training Loss: 2.04\n","Epoch: 3, Training Loss: 1.41\n","Epoch: 6, Training Loss: 1.10\n","Epoch: 9, Training Loss: 1.03\n","Epoch: 12, Training Loss: 1.01\n","Epoch: 15, Training Loss: 0.99\n","Epoch: 18, Training Loss: 0.98\n","Epoch: 21, Training Loss: 0.97\n","Epoch: 24, Training Loss: 0.97\n","Epoch: 27, Training Loss: 0.96\n","Epoch: 30, Training Loss: 0.95\n","Epoch: 33, Training Loss: 0.95\n","Epoch: 36, Training Loss: 0.94\n","Epoch: 39, Training Loss: 0.94\n","Epoch: 42, Training Loss: 0.93\n","Epoch: 45, Training Loss: 0.93\n","Epoch: 48, Training Loss: 0.92\n","Epoch: 51, Training Loss: 0.92\n","Epoch: 54, Training Loss: 0.91\n","Epoch: 57, Training Loss: 0.91\n","Epoch: 60, Training Loss: 0.90\n","Epoch: 63, Training Loss: 0.90\n","Epoch: 66, Training Loss: 0.89\n","Epoch: 69, Training Loss: 0.89\n","Epoch: 72, Training Loss: 0.88\n","Epoch: 75, Training Loss: 0.88\n","Epoch: 78, Training Loss: 0.88\n","Epoch: 81, Training Loss: 0.88\n","Epoch: 84, Training Loss: 0.87\n","Epoch: 87, Training Loss: 0.87\n","Epoch: 90, Training Loss: 0.86\n","Epoch: 93, Training Loss: 0.86\n","Epoch: 96, Training Loss: 0.85\n","Epoch: 99, Training Loss: 0.86\n","Epoch [100/2000], Test Loss: 0.9155357480049133\n","Epoch: 102, Training Loss: 0.85\n","Epoch: 105, Training Loss: 0.85\n","Epoch: 108, Training Loss: 0.84\n","Epoch: 111, Training Loss: 0.84\n","Epoch: 114, Training Loss: 0.84\n","Epoch: 117, Training Loss: 0.84\n","Epoch: 120, Training Loss: 0.84\n","Epoch: 123, Training Loss: 0.83\n","Epoch: 126, Training Loss: 0.83\n","Epoch: 129, Training Loss: 0.83\n","Epoch: 132, Training Loss: 0.82\n","Epoch: 135, Training Loss: 0.82\n","Epoch: 138, Training Loss: 0.82\n","Epoch: 141, Training Loss: 0.82\n","Epoch: 144, Training Loss: 0.82\n","Epoch: 147, Training Loss: 0.82\n","Epoch: 150, Training Loss: 0.81\n","Epoch: 153, Training Loss: 0.82\n","Epoch: 156, Training Loss: 0.81\n","Epoch: 159, Training Loss: 0.81\n","Epoch: 162, Training Loss: 0.80\n","Epoch: 165, Training Loss: 0.80\n","Epoch: 168, Training Loss: 0.80\n","Epoch: 171, Training Loss: 0.80\n","Epoch: 174, Training Loss: 0.80\n","Epoch: 177, Training Loss: 0.80\n","Epoch: 180, Training Loss: 0.79\n","Epoch: 183, Training Loss: 0.79\n","Epoch: 186, Training Loss: 0.80\n","Epoch: 189, Training Loss: 0.79\n","Epoch: 192, Training Loss: 0.79\n","Epoch: 195, Training Loss: 0.79\n","Epoch: 198, Training Loss: 0.78\n","Epoch [200/2000], Test Loss: 0.8757675886154175\n","Epoch: 201, Training Loss: 0.79\n","Epoch: 204, Training Loss: 0.78\n","Epoch: 207, Training Loss: 0.78\n","Epoch: 210, Training Loss: 0.78\n","Epoch: 213, Training Loss: 0.78\n","Epoch: 216, Training Loss: 0.78\n","Epoch: 219, Training Loss: 0.77\n","Epoch: 222, Training Loss: 0.78\n","Epoch: 225, Training Loss: 0.77\n","Epoch: 228, Training Loss: 0.77\n","Epoch: 231, Training Loss: 0.77\n","Epoch: 234, Training Loss: 0.77\n","Epoch: 237, Training Loss: 0.77\n","Epoch: 240, Training Loss: 0.76\n","Epoch: 243, Training Loss: 0.76\n","Epoch: 246, Training Loss: 0.77\n","Epoch: 249, Training Loss: 0.76\n","Epoch: 252, Training Loss: 0.76\n","Epoch: 255, Training Loss: 0.76\n","Epoch: 258, Training Loss: 0.76\n","Epoch: 261, Training Loss: 0.76\n","Epoch: 264, Training Loss: 0.76\n","Epoch: 267, Training Loss: 0.76\n","Epoch: 270, Training Loss: 0.75\n","Epoch: 273, Training Loss: 0.75\n","Epoch: 276, Training Loss: 0.75\n","Epoch: 279, Training Loss: 0.75\n","Epoch: 282, Training Loss: 0.75\n","Epoch: 285, Training Loss: 0.75\n","Epoch: 288, Training Loss: 0.75\n","Epoch: 291, Training Loss: 0.74\n","Epoch: 294, Training Loss: 0.74\n","Best loss achieved:  0.8729220032691956\n","Average accuracy: 0.8515215814113617\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"L4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}