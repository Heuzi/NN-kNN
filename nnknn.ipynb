{"cells":[{"cell_type":"markdown","metadata":{"id":"rxoHWNZEmG4g"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"CsfLxAVaPOTf"},"source":["On google colab, you have to restart runtime after running the following line"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6370,"status":"ok","timestamp":1719034303535,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"T3Y5CLhymG4l","outputId":"8b93248e-734d-479b-a1c7-2da340a6b1ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n"]}],"source":["!pip install omegaconf"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31515,"status":"ok","timestamp":1719034335047,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"BxzI25zfmG4n","outputId":"5f1e45db-e9bc-464a-d5a3-c9cf59b2633f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")\n","#\"/content/drive/My Drive/NN-kNN/\"\n","folder_name = \"/content/drive/Othercomputers/My MacBook Pro/GitHub/NN-kNN/\"\n","import sys\n","sys.path.insert(0,folder_name)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"q-0ffsjpbDLE","executionInfo":{"status":"ok","timestamp":1719034335047,"user_tz":-480,"elapsed":5,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["##This is added because my Rdata uses Cdata for the covid data set.\n","##Rdata use Cdata function to load the data set, then convert it to regression problem\n","import os\n","import sys\n","sys.path.append('/content/drive/Othercomputers/My MacBook Pro/GitHub/NN-kNN/dataset')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"joYfU4jLmG4o","executionInfo":{"status":"ok","timestamp":1719034335047,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# folder_name = os.getcwd()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"oDGrACwVmG4o","executionInfo":{"status":"ok","timestamp":1719034346470,"user_tz":-480,"elapsed":11427,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","from tqdm import tqdm\n","from omegaconf import DictConfig, OmegaConf\n","\n","from dataset import cls_small_data as Cdata\n","import model.cls_model as Cmodel\n","from dataset import cls_medium_data\n","\n","from dataset import reg_data as Rdata\n","import model.reg_model as Rmodel"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"bzTxZZJnmG4p","executionInfo":{"status":"ok","timestamp":1719034346970,"user_tz":-480,"elapsed":512,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["conf_file = OmegaConf.load(os.path.join(folder_name, 'config.yaml'))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yT3W4iqSmG4p","executionInfo":{"status":"ok","timestamp":1719034346970,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcsdY45IjhSV","executionInfo":{"status":"ok","timestamp":1719034346970,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"4fa1bf4d-02b6-4296-92e5-4339d74e77af"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"oaKz8Ns3mG4q"},"source":["# NCA and LMNN setup"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5186,"status":"ok","timestamp":1719034352154,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"cWQ8I6icoU8X","outputId":"f716c9d2-7ef2-4b8a-d3f5-92ae0f4fa2b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting metric-learn\n","  Downloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.25.2)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (3.5.0)\n","Installing collected packages: metric-learn\n","Successfully installed metric-learn-0.7.0\n"]}],"source":["pip install metric-learn"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"zmjbXrjQmG4q","executionInfo":{"status":"ok","timestamp":1719034352154,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["import metric_learn\n","from metric_learn import LMNN,NCA"]},{"cell_type":"markdown","metadata":{"id":"46TjfSz-mG4q"},"source":["# Data Sets"]},{"cell_type":"markdown","metadata":{"id":"ZkDrsvjWmG4r"},"source":["Supported small dataset for classification:  \n","'zebra',\n","'zebra_special',\n","'bal',\n","'digits',\n","'iris',\n","'wine',\n","'breast_cancer',\n","\n","for regression:\n","'califonia_housing',\n","'abalone',\n","'diabets',\n","'body_fat',\n","'ziweifaces'\n","\n","\n","Newly added data sets for mental health (psychology):\n","\n","Classification:\n","'psych_depression_physical_symptons',\n","'covid_anxious',\n","'covid_depressed'\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":17673,"status":"ok","timestamp":1719036319831,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"lxq-FY3FmG4r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4dc048df-55f8-4bcd-9cde-9be41c2bf525"},"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the dataset: Index(['SU_ID', 'P_PANEL', 'NATIONAL_WEIGHT', 'REGION_WEIGHT',\n","       'NATIONAL_WEIGHT_POP', 'REGION_WEIGHT_POP', 'NAT_WGT_COMB_POP',\n","       'REG_WGT_COMB_POP', 'P_GEO', 'SOC1',\n","       ...\n","       'REGION9', 'P_DENSE', 'MODE', 'LANGUAGE', 'MAIL50', 'RACE1_BANNER',\n","       'RACE2_BANNER', 'INC_BANNER', 'AGE_BANNER', 'HH_BANNER'],\n","      dtype='object', length=177)\n","   SOC1  SOC2A  SOC2B  SOC3A  SOC3B  SOC4A  SOC4B  PHYS8  PHYS1A  PHYS1B  \\\n","0   3.0    4.0    4.0    1.0    1.0    2.0    2.0    3.0     1.0     1.0   \n","1   3.0    3.0    3.0    1.0    3.0    2.0    2.0    2.0     2.0     2.0   \n","2   3.0    3.0    3.0    1.0    1.0    2.0    1.0    2.0     2.0     1.0   \n","3   3.0    2.0    2.0    1.0    1.0    2.0    1.0    3.0     2.0     2.0   \n","4   3.0    1.0    1.0    1.0    1.0    2.0    2.0    3.0     1.0     1.0   \n","5   2.0    3.0    3.0    1.0    1.0    1.0    1.0    2.0     2.0     2.0   \n","6   2.0    4.0    3.0    1.0    1.0    2.0    2.0    NaN     1.0     2.0   \n","7   3.0    3.0    3.0    1.0    2.0    2.0    2.0    3.0     2.0     2.0   \n","8   1.0    4.0    5.0    3.0    3.0    2.0    2.0    1.0     2.0     2.0   \n","9   3.0    4.0    3.0    3.0    4.0    1.0    2.0    2.0     2.0     2.0   \n","\n","   PHYS1C  PHYS1D  PHYS1E  PHYS1F  PHYS1G  PHYS1H  PHYS1I  PHYS1J  PHYS1K  \\\n","0     2.0     1.0     1.0     2.0     2.0     1.0     1.0     1.0     1.0   \n","1     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","2     2.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0     2.0   \n","3     2.0     2.0     2.0     2.0     1.0     2.0     1.0     1.0     2.0   \n","4     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0   \n","5     2.0     2.0     1.0     2.0     NaN     2.0     2.0     NaN     2.0   \n","6     1.0     2.0     1.0     2.0     2.0     2.0     2.0     1.0     2.0   \n","7     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","8     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","9     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","\n","   PHYS1L  PHYS1M  PHYS1N  PHYS1O  PHYS1P  PHYS1Q  SOC5E  PHYS2_1  PHYS2_2  \\\n","0     1.0     2.0     1.0     1.0     2.0     1.0    2.0        0        1   \n","1     2.0     2.0     1.0     2.0     2.0     2.0    1.0        0        1   \n","2     2.0     2.0     2.0     2.0     2.0     2.0    1.0        0        1   \n","3     2.0     1.0     2.0     2.0     2.0     1.0    1.0        0        1   \n","4     1.0     2.0     1.0     2.0     2.0     2.0    1.0        1        1   \n","5     1.0     NaN     2.0     2.0     2.0     2.0    1.0        0        1   \n","6     2.0     2.0     1.0     1.0     2.0     2.0    3.0        1        1   \n","7     2.0     2.0     2.0     2.0     2.0     2.0    1.0        0        1   \n","8     2.0     2.0     2.0     2.0     2.0     2.0    1.0        0        1   \n","9     2.0     2.0     2.0     2.0     2.0     2.0    1.0        0        1   \n","\n","   PHYS2_3  PHYS2_4  PHYS2_5  PHYS2_6  PHYS2_7  PHYS2_8  PHYS2_9  PHYS2_10  \\\n","0        0        0        0        0        0        1        1         0   \n","1        0        0        0        0        0        1        0         0   \n","2        0        1        0        0        1        1        1         0   \n","3        0        1        0        1        0        1        1         0   \n","4        0        1        0        0        0        1        0         0   \n","5        0        1        0        1        0        1        1         0   \n","6        0        0        0        1        0        1        0         0   \n","7        0        0        0        0        0        1        1         0   \n","8        0        1        0        0        1        0        0         0   \n","9        0        1        1        0        0        1        1         1   \n","\n","   PHYS2_11  PHYS2_12  PHYS2_13  PHYS2_14  PHYS2_15  PHYS2_16  PHYS2_17  \\\n","0         1         0         1         0         1         1         1   \n","1         1         1         1         1         1         1         1   \n","2         1         0         1         0         0         1         1   \n","3         1         0         1         0         1         1         1   \n","4         1         0         1         0         0         1         1   \n","5         1         1         1         0         1         1         1   \n","6         1         1         1         1         1         1         1   \n","7         1         1         1         0         0         1         0   \n","8         0         0         0         0         0         0         1   \n","9         1         0         1         0         1         1         1   \n","\n","   PHYS2_18  PHYS2_19  PHYS2_DK  PHYS2_SKP  PHYS2_REF  PHYS10A  PHYS10B  \\\n","0         1         0         0          0          0      3.0      3.0   \n","1         0         1         0          0          0      2.0      3.0   \n","2         0         1         0          0          0      1.0      2.0   \n","3         0         1         0          0          0      5.0      5.0   \n","4         0         1         0          0          0      4.0      4.0   \n","5         0         0         0          0          0      5.0      4.0   \n","6         1         1         0          0          0      1.0      1.0   \n","7         0         1         0          0          0      4.0      4.0   \n","8         0         1         0          0          0      1.0      1.0   \n","9         0         0         0          0          0      3.0      3.0   \n","\n","   PHYS10C  PHYS10D  PHYS10E  ECON8A  ECON8B  ECON8C  ECON8D  ECON8E  ECON8F  \\\n","0      4.0      1.0      1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","1      3.0      1.0      1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n","2      1.0      1.0      1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","3      4.0      2.0      2.0     2.0     2.0     1.0     2.0     1.0     2.0   \n","4      4.0      2.0      1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","5      3.0      1.0      1.0     1.0     2.0     1.0     2.0     1.0     1.0   \n","6      1.0      1.0      1.0     1.0     1.0     2.0     1.0     2.0     1.0   \n","7      1.0      1.0      1.0     2.0     1.0     2.0     1.0     2.0     2.0   \n","8      1.0      1.0      1.0     2.0     2.0     2.0     2.0     1.0     2.0   \n","9      4.0      3.0      2.0     1.0     2.0     1.0     1.0     2.0     2.0   \n","\n","   ECON8G  ECON8H  ECON8I  ECON8J  ECON8K  ECON8L  ECON8M  ECON8N  ECON8O  \\\n","0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","1     1.0     2.0     2.0     1.0     1.0     2.0     2.0     2.0     1.0   \n","2     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","3     2.0     2.0     1.0     1.0     2.0     2.0     1.0     1.0     2.0   \n","4     2.0     1.0     1.0     2.0     1.0     1.0     2.0     2.0     2.0   \n","5     2.0     1.0     2.0     1.0     1.0     2.0     2.0     1.0     1.0   \n","6     1.0     2.0     1.0     2.0     1.0     1.0     2.0     2.0     2.0   \n","7     1.0     1.0     2.0     1.0     2.0     2.0     2.0     1.0     2.0   \n","8     1.0     1.0     1.0     1.0     2.0     2.0     2.0     2.0     2.0   \n","9     1.0     1.0     1.0     2.0     2.0     2.0     2.0     2.0     1.0   \n","\n","   ECON8P  ECON8Q  ECON8R  ECON8S  ECON7_1  ECON7_2  ECON7_3  ECON7_4  \\\n","0     2.0     2.0     2.0     2.0        0        1        1        0   \n","1     2.0     2.0     1.0     1.0        1        0        0        0   \n","2     2.0     2.0     2.0     2.0        1        0        0        0   \n","3     2.0     2.0     1.0     1.0        1        0        1        0   \n","4     2.0     2.0     2.0     1.0        1        0        1        0   \n","5     2.0     2.0     1.0     1.0        1        0        1        0   \n","6     1.0     2.0     1.0     1.0        0        0        0        0   \n","7     1.0     1.0     1.0     2.0        1        1        0        0   \n","8     1.0     1.0     1.0     1.0        0        0        0        0   \n","9     2.0     2.0     1.0     1.0        1        0        1        0   \n","\n","   ECON7_5  ECON7_6  ECON7_7  ECON7_8  ECON7_DK  ECON7_SKP  ECON7_REF  ECON1  \\\n","0        0        0        0        0         0          0          0    3.0   \n","1        0        0        0        0         0          0          0    3.0   \n","2        0        0        0        0         0          0          0    2.0   \n","3        0        0        0        0         0          0          0    3.0   \n","4        0        0        0        0         0          0          0    3.0   \n","5        0        0        0        0         0          0          0    1.0   \n","6        0        0        0        1         0          0          0    3.0   \n","7        0        0        0        0         0          0          0    1.0   \n","8        0        0        0        1         0          0          0    1.0   \n","9        0        0        0        0         0          0          0    1.0   \n","\n","   ECON2  ECON4  ECON3  ECON4A  ECON4B  ECON6A  ECON6B  ECON6C  ECON6D  \\\n","0    0.0    7.0   40.0     1.0     1.0     4.0     4.0     4.0     4.0   \n","1    0.0    2.0    0.0     5.0     5.0     4.0     4.0     4.0     1.0   \n","2    0.0    0.0   40.0     1.0     1.0     4.0     4.0     4.0     4.0   \n","3    0.0    2.0   16.0     3.0     3.0     4.0     4.0     4.0     1.0   \n","4    0.0    2.0   25.0     2.0     1.0     4.0     4.0     4.0     1.0   \n","5   60.0    0.0   50.0     1.0     1.0     4.0     4.0     4.0     4.0   \n","6    0.0    2.0    0.0     5.0     5.0     4.0     3.0     4.0     1.0   \n","7   40.0    0.0   40.0     1.0     1.0     4.0     4.0     4.0     4.0   \n","8   10.0    0.0   20.0     1.0     2.0     4.0     4.0     4.0     4.0   \n","9   35.0    0.0   55.0     1.0     2.0     2.0     4.0     4.0     4.0   \n","\n","   ECON6E  ECON6F  ECON6G  ECON6H  ECON6I  ECON6J  ECON6K  ECON6L  ECON5A_A  \\\n","0     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0       2.0   \n","1     4.0     1.0     4.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","2     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","3     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","4     4.0     1.0     4.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","5     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","6     1.0     1.0     4.0     4.0     4.0     4.0     4.0     4.0       1.0   \n","7     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","8     4.0     4.0     4.0     4.0     4.0     4.0     4.0     4.0       1.0   \n","9     4.0     4.0     1.0     4.0     4.0     4.0     4.0     4.0       3.0   \n","\n","   ECON5A_B  PHYS7_1  PHYS7_2  PHYS7_3  PHYS7_4  PHYS7_DK  PHYS7_SKP  \\\n","0       3.0        0        1        0        0         0          0   \n","1       3.0        0        0        0        1         0          0   \n","2       3.0        0        0        0        1         0          0   \n","3       3.0        0        0        0        1         0          0   \n","4       3.0        0        0        0        1         0          0   \n","5       3.0        0        0        0        1         0          0   \n","6       2.0        0        0        0        1         0          0   \n","7       3.0        0        0        0        1         0          0   \n","8       1.0        0        0        0        1         0          0   \n","9       3.0        0        0        0        1         0          0   \n","\n","   PHYS7_REF  PHYS11  PHYS9A  PHYS9B  PHYS9C  PHYS9D  PHYS9E  PHYS9F  PHYS9G  \\\n","0          0     1.0     2.0     2.0     2.0     1.0     2.0     2.0     2.0   \n","1          0     1.0     2.0     1.0     2.0     2.0     1.0     2.0     2.0   \n","2          0     2.0     2.0     1.0     2.0     2.0     1.0     2.0     2.0   \n","3          0     2.0     2.0     2.0     2.0     2.0     1.0     2.0     2.0   \n","4          0     1.0     2.0     1.0     2.0     2.0     1.0     2.0     2.0   \n","5          0     1.0     1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","6          0     1.0     2.0     2.0     2.0     1.0     2.0     2.0     2.0   \n","7          0     2.0     1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","8          0     2.0     1.0     2.0     2.0     2.0     1.0     1.0     1.0   \n","9          0     1.0     1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","\n","   PHYS9H  PHYS3A  PHYS3B  PHYS3C  PHYS3D  PHYS3E  PHYS3F  PHYS3G  PHYS3H  \\\n","0     1.0     2.0     2.0     2.0     1.0     2.0     1.0     2.0     1.0   \n","1     2.0     2.0     1.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","2     1.0     2.0     2.0     2.0     2.0     2.0     2.0     1.0     2.0   \n","3     2.0     2.0     1.0     2.0     2.0     2.0     2.0     2.0     1.0   \n","4     2.0     2.0     1.0     2.0     1.0     1.0     1.0     1.0     2.0   \n","5     2.0     2.0     2.0     2.0     NaN     2.0     NaN     NaN     2.0   \n","6     2.0     1.0     1.0     2.0     1.0     2.0     2.0     1.0     1.0   \n","7     2.0     1.0     1.0     1.0     2.0     2.0     2.0     2.0     2.0   \n","8     1.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0     2.0   \n","9     2.0     2.0     2.0     2.0     2.0     2.0     NaN     1.0     2.0   \n","\n","   PHYS3I  PHYS3J  PHYS3K  PHYS3L  PHYS3M  PHYS4  PHYS5  PHYS6  AGE4  AGE7  \\\n","0     2.0     2.0     2.0     2.0     1.0    1.0    2.0    2.0   2.0   3.0   \n","1     2.0     2.0     1.0     1.0     2.0    2.0    2.0    2.0   4.0   6.0   \n","2     2.0     2.0     2.0     2.0     1.0    2.0    2.0    2.0   4.0   6.0   \n","3     2.0     2.0     2.0     2.0     1.0    2.0    2.0    2.0   4.0   6.0   \n","4     2.0     2.0     2.0     2.0     1.0    2.0    2.0    2.0   4.0   6.0   \n","5     2.0     2.0     2.0     2.0     2.0    2.0    2.0    2.0   2.0   2.0   \n","6     2.0     2.0     2.0     1.0     1.0    2.0    2.0    2.0   3.0   4.0   \n","7     2.0     2.0     2.0     2.0     1.0    2.0    2.0    2.0   4.0   5.0   \n","8     2.0     2.0     2.0     2.0     2.0    2.0    2.0    2.0   1.0   1.0   \n","9     2.0     2.0     2.0     2.0     2.0    2.0    2.0    2.0   2.0   2.0   \n","\n","   GENDER  RACETH  RACE_R2 HHINCOME  EDUCATION  EDUC4  HHSIZE1  HH01S  HH25S  \\\n","0     2.0     1.0      1.0        3        3.0    3.0      4.0      0      1   \n","1     1.0     1.0      1.0        9        5.0    4.0      2.0      0      0   \n","2     1.0     NaN      1.0        9        5.0    4.0      2.0      0      0   \n","3     1.0     2.0      2.0        7        5.0    4.0      2.0      0      0   \n","4     1.0     1.0      1.0        7        2.0    2.0      2.0      0      0   \n","5     1.0     1.0      1.0        7        6.0    4.0      2.0      0      0   \n","6     2.0     3.0      2.0        2        5.0    4.0      4.0      0      0   \n","7     1.0     1.0      1.0        9        5.0    4.0      2.0      0      0   \n","8     2.0     2.0      2.0        1        2.0    2.0      1.0      0      0   \n","9     1.0     NaN      1.0        8        5.0    4.0      3.0      0      0   \n","\n","   HH612S  HH1317S  HH18OVS  REGION4  REGION9  P_DENSE  MODE  LANGUAGE  \\\n","0       0        2        1        1        2      3.0     2         1   \n","1       0        0        2        4        8      3.0     2         1   \n","2       0        0        2        4        9      3.0     2         1   \n","3       0        0        2        1        2      3.0     2         1   \n","4       0        0        2        2        3      3.0     2         1   \n","5       0        0        2        1        2      3.0     2         1   \n","6       0        0        4        3        5      3.0     2         2   \n","7       0        0        2        2        3      3.0     2         1   \n","8       0        0        1        2        3      3.0     2         1   \n","9       1        0        2        4        8      2.0     2         1   \n","\n","   RACE1_BANNER  INC_BANNER  AGE_BANNER  HH_BANNER  \n","0           1.0         1.0         2.0        4.0  \n","1           1.0         4.0         3.0        2.0  \n","2           NaN         4.0         3.0        2.0  \n","3           2.0         3.0         3.0        2.0  \n","4           1.0         3.0         3.0        2.0  \n","5           1.0         3.0         2.0        2.0  \n","6           3.0         1.0         2.0        5.0  \n","7           1.0         4.0         2.0        2.0  \n","8           2.0         1.0         2.0        1.0  \n","9           NaN         3.0         2.0        3.0  \n","[ 3.  4.  4.  1.  1.  2.  2.  3.  1.  1.  2.  1.  1.  2.  2.  1.  1.  1.\n","  1.  1.  2.  1.  1.  2.  1.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  1.\n","  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  3.  3.  4.  1.  1.  2.  2.\n","  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  0.\n","  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  7. 40.  1.  1.  4.  4.\n","  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  2.  3.  0.  1.  0.  0.  0.  0.\n","  0.  1.  2.  2.  2.  1.  2.  2.  2.  1.  2.  2.  2.  1.  2.  1.  2.  1.\n","  2.  2.  2.  2.  1.  1.  2.  2.  2.  3.  2.  1.  1.  3.  3.  3.  4.  0.\n","  1.  0.  2.  1.  1.  2.  3.  2.  1.  1.  1.  2.  4.]\n"]}],"source":["dataset_name = 'covid_physical'\n","cfg = conf_file['dataset'][dataset_name]\n","#TODO need to add other covid data sets here.\n","if dataset_name in ['covid_anxious','covid_depressed','covid_physical','covid_lonely','covid_hopeless',\n","                    'psych_depression_physical_symptons',\n","                    'zebra','zebra_special','bal','digits','iris','wine','breast_cancer']:\n","    criterion = torch.nn.CrossEntropyLoss()\n","    Xs, ys = Cdata.Cls_small_data(dataset_name)\n","elif dataset_name in []:\n","    criterion = torch.nn.CrossEntropyLoss()\n","    Xs, ys = cls_medium_data.Cls_medium_data(dataset_name)\n","else:\n","    criterion = torch.nn.MSELoss()\n","    Xs, ys = Rdata.Reg_data(dataset_name)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1719036319831,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"aOIbeEfEmG4s","outputId":"2a4284a0-397e-4a7b-ea76-48591c8f445b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'dataset.cls_small_data' from '/content/drive/Othercomputers/My MacBook Pro/GitHub/NN-kNN/dataset/cls_small_data.py'>"]},"metadata":{},"execution_count":42}],"source":["# This section is used to reload the imported module.\n","# For example, if you made any changes in the model.cls_model, you should run importlib.reload(Cmodel) as long as you set import model.cls_model as Cmodel.\n","import importlib\n","importlib.reload(Rdata)\n","importlib.reload(Cdata)"]},{"cell_type":"code","source":["#for reloading config file, in case you modified it for experimenting\n","conf_file = OmegaConf.load(os.path.join(folder_name, 'config.yaml'))\n","cfg = conf_file['dataset'][dataset_name]"],"metadata":{"id":"cNPWBqvve8v6","executionInfo":{"status":"ok","timestamp":1719036319832,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["cfg"],"metadata":{"id":"t9fvmf2QztcF","executionInfo":{"status":"ok","timestamp":1719036319832,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"b2373094-3270-4d38-92ad-29e8521ff332","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ca_weight_sharing': True, 'top_case_enabled': False, 'training_epochs': 1000, 'learning_rate': 0.01, 'batch_size': 16, 'top_k': 5, 'class_weight_sharing': True, 'patience': 40, 'discount': 40}"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"EOEnGkKnmG4s"},"source":["# Classification with NNKNN"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719036321276,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"fisqdp27w9fg","outputId":"23eb9233-1b3e-4b10-c005-51ca7d18409d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique values: [0 1 2 3]\n","Counts: [115 115 115 115]\n","Xs.size(): torch.Size([460, 157])\n"]}],"source":["# prompt: get the unique y values and their counts\n","\n","unique_values, counts = np.unique(ys, return_counts=True)\n","print(f\"Unique values: {unique_values}\")\n","print(f\"Counts: {counts}\")\n","print(f\"Xs.size(): {Xs.size()}\")\n"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"xOraZstjmG4t","executionInfo":{"status":"ok","timestamp":1719036321276,"user_tz":-480,"elapsed":3,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def train_cls(X_train,y_train, X_test, y_test, cfg:DictConfig):\n","  X_train = X_train.to(device)\n","  y_train = y_train.to(device)\n","  X_test = X_test.to(device)\n","\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=cfg.batch_size, shuffle=True)\n","\n","  # Train model\n","  # X_train_trimmed = X_train[:, :X_train.shape[1]//10]\n","  # y_train_trimmed = y_train[:X_train.shape[1]//10]\n","  model = Cmodel.NN_k_NN(X_train,\n","              y_train,\n","              cfg.ca_weight_sharing,\n","              cfg.top_case_enabled,\n","              cfg.top_k,\n","              cfg.discount,\n","              device=device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate) #, weight_decay=1e-5)\n","\n","  patience_counter = 0\n","  for epoch in range(cfg.training_epochs):\n","    epoch_msg = True\n","\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      _, _, output, predicted_class = model(X_train_batch)\n","      loss = criterion(output, y_train_batch)\n","\n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      if epoch_msg and (epoch + 1) % 2 == 0:\n","        print(f'Epoch [{epoch + 1}/{cfg.training_epochs}], Loss: {loss.item():.4f}')\n","\n","        epoch_msg = False\n","      # print(\"evaluating\")\n","    model.eval()\n","    with torch.no_grad():\n","      _, _, output, predicted_class = model(X_test)\n","\n","      # Calculate accuracy\n","      accuracy_temp = accuracy_score(y_test.numpy(), predicted_class.cpu().numpy())\n","    if epoch == 0:\n","      best_accuracy = accuracy_temp\n","      torch.save(model.state_dict(), cfg.PATH)\n","\n","    elif accuracy_temp > best_accuracy:\n","      #memorize best model\n","      torch.save(model.state_dict(), cfg.PATH)\n","      best_accuracy = accuracy_temp\n","      patience_counter = 0\n","\n","    elif patience_counter > cfg.patience:\n","      model.eval()\n","      print(\"patience exceeded, loading best model\")\n","      break\n","    else:\n","      patience_counter += 1\n","\n","  return best_accuracy, model"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"DMKsNhL3ItkK","executionInfo":{"status":"ok","timestamp":1719036321276,"user_tz":-480,"elapsed":3,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def load_model_cls(X_train,y_train,cfg):\n","  # Define the model architecture\n","  model = Cmodel.NN_k_NN(\n","      X_train,\n","      y_train,\n","      cfg.ca_weight_sharing,\n","      cfg.top_case_enabled,\n","      cfg.top_k,\n","      cfg.discount,\n","      device=device\n","  )\n","  # Load the state dictionary\n","  model.load_state_dict(torch.load(cfg.path))\n","  model.to(device)\n","  model.eval()\n","  return model"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5291187,"status":"ok","timestamp":1719041612461,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"qPIxfcO-mG4t","outputId":"23a879ea-5fb9-4d02-cea0-6863d8aad2f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["lmnn_acc:  0.41304347826086957\n","knn_acc:  0.30434782608695654\n","Epoch [2/1000], Loss: 1.8669\n","Epoch [4/1000], Loss: 1.8907\n","Epoch [6/1000], Loss: 1.6683\n","Epoch [8/1000], Loss: 1.6024\n","Epoch [10/1000], Loss: 1.4150\n","Epoch [12/1000], Loss: 1.4198\n","Epoch [14/1000], Loss: 1.3935\n","Epoch [16/1000], Loss: 1.2275\n","Epoch [18/1000], Loss: 1.1313\n","Epoch [20/1000], Loss: 1.2395\n","Epoch [22/1000], Loss: 1.0801\n","Epoch [24/1000], Loss: 1.2571\n","Epoch [26/1000], Loss: 1.1764\n","Epoch [28/1000], Loss: 1.0724\n","Epoch [30/1000], Loss: 1.3278\n","Epoch [32/1000], Loss: 1.5138\n","Epoch [34/1000], Loss: 1.0134\n","Epoch [36/1000], Loss: 1.1752\n","Epoch [38/1000], Loss: 1.1704\n","Epoch [40/1000], Loss: 1.1890\n","Epoch [42/1000], Loss: 1.0524\n","Epoch [44/1000], Loss: 1.0644\n","Epoch [46/1000], Loss: 0.9777\n","Epoch [48/1000], Loss: 1.1051\n","patience exceeded, loading best model\n","nnknn acc:  0.43478260869565216\n","lmnn_acc:  0.2608695652173913\n","knn_acc:  0.2826086956521739\n","Epoch [2/1000], Loss: 1.9440\n","Epoch [4/1000], Loss: 2.2699\n","Epoch [6/1000], Loss: 1.8037\n","Epoch [8/1000], Loss: 1.6467\n","Epoch [10/1000], Loss: 1.5047\n","Epoch [12/1000], Loss: 1.3077\n","Epoch [14/1000], Loss: 1.4252\n","Epoch [16/1000], Loss: 1.5182\n","Epoch [18/1000], Loss: 1.2284\n","Epoch [20/1000], Loss: 1.2655\n","Epoch [22/1000], Loss: 1.2597\n","Epoch [24/1000], Loss: 1.2266\n","Epoch [26/1000], Loss: 1.3422\n","Epoch [28/1000], Loss: 1.1437\n","Epoch [30/1000], Loss: 1.1826\n","Epoch [32/1000], Loss: 1.1134\n","Epoch [34/1000], Loss: 1.1412\n","Epoch [36/1000], Loss: 1.3297\n","Epoch [38/1000], Loss: 1.2394\n","Epoch [40/1000], Loss: 1.1789\n","Epoch [42/1000], Loss: 0.9240\n","Epoch [44/1000], Loss: 1.1861\n","Epoch [46/1000], Loss: 0.9316\n","Epoch [48/1000], Loss: 1.2214\n","Epoch [50/1000], Loss: 0.9833\n","Epoch [52/1000], Loss: 1.0364\n","Epoch [54/1000], Loss: 0.9520\n","Epoch [56/1000], Loss: 1.2101\n","Epoch [58/1000], Loss: 0.9738\n","Epoch [60/1000], Loss: 0.9943\n","Epoch [62/1000], Loss: 1.1254\n","Epoch [64/1000], Loss: 0.9541\n","Epoch [66/1000], Loss: 0.9687\n","Epoch [68/1000], Loss: 0.9204\n","Epoch [70/1000], Loss: 0.9733\n","patience exceeded, loading best model\n","nnknn acc:  0.32608695652173914\n","lmnn_acc:  0.2391304347826087\n","knn_acc:  0.30434782608695654\n","Epoch [2/1000], Loss: 1.8327\n","Epoch [4/1000], Loss: 1.6879\n","Epoch [6/1000], Loss: 1.6606\n","Epoch [8/1000], Loss: 1.3125\n","Epoch [10/1000], Loss: 1.4048\n","Epoch [12/1000], Loss: 1.3646\n","Epoch [14/1000], Loss: 1.3496\n","Epoch [16/1000], Loss: 1.3078\n","Epoch [18/1000], Loss: 1.2272\n","Epoch [20/1000], Loss: 1.2484\n","Epoch [22/1000], Loss: 1.1967\n","Epoch [24/1000], Loss: 1.2791\n","Epoch [26/1000], Loss: 0.9839\n","Epoch [28/1000], Loss: 1.0663\n","Epoch [30/1000], Loss: 1.0295\n","Epoch [32/1000], Loss: 1.1264\n","Epoch [34/1000], Loss: 1.4370\n","Epoch [36/1000], Loss: 0.9475\n","Epoch [38/1000], Loss: 0.9200\n","Epoch [40/1000], Loss: 1.1560\n","Epoch [42/1000], Loss: 1.2998\n","Epoch [44/1000], Loss: 1.0053\n","Epoch [46/1000], Loss: 1.2245\n","Epoch [48/1000], Loss: 1.1978\n","Epoch [50/1000], Loss: 1.2123\n","Epoch [52/1000], Loss: 1.0571\n","Epoch [54/1000], Loss: 1.2789\n","Epoch [56/1000], Loss: 1.1393\n","Epoch [58/1000], Loss: 1.0565\n","Epoch [60/1000], Loss: 0.9165\n","Epoch [62/1000], Loss: 1.2193\n","Epoch [64/1000], Loss: 1.1619\n","Epoch [66/1000], Loss: 0.9341\n","Epoch [68/1000], Loss: 1.2642\n","Epoch [70/1000], Loss: 1.1294\n","Epoch [72/1000], Loss: 1.3200\n","Epoch [74/1000], Loss: 1.0656\n","patience exceeded, loading best model\n","nnknn acc:  0.45652173913043476\n","lmnn_acc:  0.2608695652173913\n","knn_acc:  0.21739130434782608\n","Epoch [2/1000], Loss: 1.9129\n","Epoch [4/1000], Loss: 1.8498\n","Epoch [6/1000], Loss: 1.6058\n","Epoch [8/1000], Loss: 1.7388\n","Epoch [10/1000], Loss: 1.5589\n","Epoch [12/1000], Loss: 1.5826\n","Epoch [14/1000], Loss: 1.2684\n","Epoch [16/1000], Loss: 1.3427\n","Epoch [18/1000], Loss: 1.1481\n","Epoch [20/1000], Loss: 1.4481\n","Epoch [22/1000], Loss: 1.3009\n","Epoch [24/1000], Loss: 1.2400\n","Epoch [26/1000], Loss: 1.3635\n","Epoch [28/1000], Loss: 1.2205\n","Epoch [30/1000], Loss: 0.9422\n","Epoch [32/1000], Loss: 1.0690\n","Epoch [34/1000], Loss: 1.1695\n","Epoch [36/1000], Loss: 1.2213\n","Epoch [38/1000], Loss: 0.9938\n","Epoch [40/1000], Loss: 1.4286\n","Epoch [42/1000], Loss: 1.0850\n","Epoch [44/1000], Loss: 0.9968\n","Epoch [46/1000], Loss: 0.9583\n","Epoch [48/1000], Loss: 1.0616\n","Epoch [50/1000], Loss: 1.1584\n","Epoch [52/1000], Loss: 0.8551\n","patience exceeded, loading best model\n","nnknn acc:  0.43478260869565216\n","lmnn_acc:  0.3695652173913043\n","knn_acc:  0.34782608695652173\n","Epoch [2/1000], Loss: 1.9773\n","Epoch [4/1000], Loss: 2.0370\n","Epoch [6/1000], Loss: 1.7641\n","Epoch [8/1000], Loss: 1.5861\n","Epoch [10/1000], Loss: 1.2448\n","Epoch [12/1000], Loss: 1.6312\n","Epoch [14/1000], Loss: 1.3493\n","Epoch [16/1000], Loss: 1.3675\n","Epoch [18/1000], Loss: 1.3500\n","Epoch [20/1000], Loss: 1.2023\n","Epoch [22/1000], Loss: 1.3374\n","Epoch [24/1000], Loss: 1.3221\n","Epoch [26/1000], Loss: 1.1942\n","Epoch [28/1000], Loss: 1.1753\n","Epoch [30/1000], Loss: 1.1542\n","Epoch [32/1000], Loss: 1.1143\n","Epoch [34/1000], Loss: 1.1858\n","Epoch [36/1000], Loss: 1.0714\n","Epoch [38/1000], Loss: 1.2760\n","Epoch [40/1000], Loss: 1.2850\n","Epoch [42/1000], Loss: 1.1636\n","Epoch [44/1000], Loss: 0.9795\n","Epoch [46/1000], Loss: 1.4637\n","Epoch [48/1000], Loss: 1.1788\n","patience exceeded, loading best model\n","nnknn acc:  0.5\n","lmnn_acc:  0.30434782608695654\n","knn_acc:  0.2391304347826087\n","Epoch [2/1000], Loss: 2.0084\n","Epoch [4/1000], Loss: 1.5738\n","Epoch [6/1000], Loss: 2.2654\n","Epoch [8/1000], Loss: 1.8639\n","Epoch [10/1000], Loss: 1.7808\n","Epoch [12/1000], Loss: 1.3801\n","Epoch [14/1000], Loss: 1.1630\n","Epoch [16/1000], Loss: 1.3540\n","Epoch [18/1000], Loss: 1.1550\n","Epoch [20/1000], Loss: 1.3265\n","Epoch [22/1000], Loss: 1.0552\n","Epoch [24/1000], Loss: 1.2360\n","Epoch [26/1000], Loss: 1.3970\n","Epoch [28/1000], Loss: 1.1392\n","Epoch [30/1000], Loss: 1.1991\n","Epoch [32/1000], Loss: 1.5490\n","Epoch [34/1000], Loss: 1.0640\n","Epoch [36/1000], Loss: 1.3202\n","Epoch [38/1000], Loss: 1.2612\n","Epoch [40/1000], Loss: 1.2096\n","Epoch [42/1000], Loss: 1.0530\n","Epoch [44/1000], Loss: 1.3856\n","Epoch [46/1000], Loss: 1.0179\n","Epoch [48/1000], Loss: 1.0007\n","Epoch [50/1000], Loss: 0.8266\n","Epoch [52/1000], Loss: 1.2090\n","Epoch [54/1000], Loss: 0.9298\n","Epoch [56/1000], Loss: 1.2073\n","Epoch [58/1000], Loss: 1.2607\n","Epoch [60/1000], Loss: 1.1690\n","patience exceeded, loading best model\n","nnknn acc:  0.34782608695652173\n","lmnn_acc:  0.43478260869565216\n","knn_acc:  0.2826086956521739\n","Epoch [2/1000], Loss: 1.9819\n","Epoch [4/1000], Loss: 2.0126\n","Epoch [6/1000], Loss: 1.8636\n","Epoch [8/1000], Loss: 1.3873\n","Epoch [10/1000], Loss: 1.2980\n","Epoch [12/1000], Loss: 1.2364\n","Epoch [14/1000], Loss: 1.2154\n","Epoch [16/1000], Loss: 1.3926\n","Epoch [18/1000], Loss: 1.2335\n","Epoch [20/1000], Loss: 1.1205\n","Epoch [22/1000], Loss: 1.1481\n","Epoch [24/1000], Loss: 1.1347\n","Epoch [26/1000], Loss: 1.3578\n","Epoch [28/1000], Loss: 1.1447\n","Epoch [30/1000], Loss: 1.0722\n","Epoch [32/1000], Loss: 1.1401\n","Epoch [34/1000], Loss: 1.1617\n","Epoch [36/1000], Loss: 1.2266\n","Epoch [38/1000], Loss: 1.1104\n","Epoch [40/1000], Loss: 1.0923\n","Epoch [42/1000], Loss: 1.2227\n","Epoch [44/1000], Loss: 1.2003\n","Epoch [46/1000], Loss: 1.0130\n","Epoch [48/1000], Loss: 0.9842\n","patience exceeded, loading best model\n","nnknn acc:  0.34782608695652173\n","lmnn_acc:  0.2608695652173913\n","knn_acc:  0.30434782608695654\n","Epoch [2/1000], Loss: 2.0728\n","Epoch [4/1000], Loss: 1.5131\n","Epoch [6/1000], Loss: 1.3270\n","Epoch [8/1000], Loss: 1.6387\n","Epoch [10/1000], Loss: 1.4489\n","Epoch [12/1000], Loss: 1.3238\n","Epoch [14/1000], Loss: 1.4212\n","Epoch [16/1000], Loss: 1.2627\n","Epoch [18/1000], Loss: 1.4249\n","Epoch [20/1000], Loss: 1.3669\n","Epoch [22/1000], Loss: 1.2625\n","Epoch [24/1000], Loss: 1.3741\n","Epoch [26/1000], Loss: 1.3027\n","Epoch [28/1000], Loss: 1.1992\n","Epoch [30/1000], Loss: 1.2263\n","Epoch [32/1000], Loss: 1.2773\n","Epoch [34/1000], Loss: 1.1811\n","Epoch [36/1000], Loss: 1.3038\n","Epoch [38/1000], Loss: 0.9681\n","Epoch [40/1000], Loss: 1.2914\n","Epoch [42/1000], Loss: 1.3022\n","Epoch [44/1000], Loss: 1.1176\n","Epoch [46/1000], Loss: 1.0875\n","Epoch [48/1000], Loss: 1.0341\n","Epoch [50/1000], Loss: 1.0382\n","Epoch [52/1000], Loss: 1.0695\n","Epoch [54/1000], Loss: 0.9142\n","Epoch [56/1000], Loss: 1.0355\n","Epoch [58/1000], Loss: 0.8224\n","Epoch [60/1000], Loss: 1.2087\n","Epoch [62/1000], Loss: 1.0931\n","Epoch [64/1000], Loss: 1.0007\n","Epoch [66/1000], Loss: 0.8964\n","Epoch [68/1000], Loss: 1.1147\n","Epoch [70/1000], Loss: 1.1848\n","Epoch [72/1000], Loss: 1.1517\n","Epoch [74/1000], Loss: 1.0104\n","Epoch [76/1000], Loss: 0.8398\n","Epoch [78/1000], Loss: 1.0926\n","Epoch [80/1000], Loss: 1.1009\n","Epoch [82/1000], Loss: 0.9800\n","Epoch [84/1000], Loss: 1.2996\n","Epoch [86/1000], Loss: 1.0482\n","Epoch [88/1000], Loss: 0.7994\n","Epoch [90/1000], Loss: 0.9227\n","Epoch [92/1000], Loss: 0.7652\n","Epoch [94/1000], Loss: 0.9625\n","Epoch [96/1000], Loss: 0.8217\n","Epoch [98/1000], Loss: 0.9867\n","patience exceeded, loading best model\n","nnknn acc:  0.45652173913043476\n","lmnn_acc:  0.391304347826087\n","knn_acc:  0.41304347826086957\n","Epoch [2/1000], Loss: 1.9372\n","Epoch [4/1000], Loss: 2.0089\n","Epoch [6/1000], Loss: 2.1761\n","Epoch [8/1000], Loss: 1.4823\n","Epoch [10/1000], Loss: 1.7878\n","Epoch [12/1000], Loss: 1.3963\n","Epoch [14/1000], Loss: 1.2172\n","Epoch [16/1000], Loss: 1.2109\n","Epoch [18/1000], Loss: 1.2140\n","Epoch [20/1000], Loss: 1.2871\n","Epoch [22/1000], Loss: 1.0796\n","Epoch [24/1000], Loss: 1.1396\n","Epoch [26/1000], Loss: 1.2429\n","Epoch [28/1000], Loss: 1.1899\n","Epoch [30/1000], Loss: 1.2347\n","Epoch [32/1000], Loss: 1.1389\n","Epoch [34/1000], Loss: 1.0128\n","Epoch [36/1000], Loss: 1.1825\n","Epoch [38/1000], Loss: 1.1105\n","Epoch [40/1000], Loss: 1.0349\n","Epoch [42/1000], Loss: 1.2300\n","Epoch [44/1000], Loss: 1.2056\n","Epoch [46/1000], Loss: 1.2378\n","Epoch [48/1000], Loss: 1.0873\n","Epoch [50/1000], Loss: 1.2100\n","Epoch [52/1000], Loss: 1.0634\n","Epoch [54/1000], Loss: 0.9613\n","Epoch [56/1000], Loss: 0.8447\n","Epoch [58/1000], Loss: 0.9216\n","patience exceeded, loading best model\n","nnknn acc:  0.41304347826086957\n","lmnn_acc:  0.32608695652173914\n","knn_acc:  0.2391304347826087\n","Epoch [2/1000], Loss: 2.0572\n","Epoch [4/1000], Loss: 1.6112\n","Epoch [6/1000], Loss: 1.6626\n","Epoch [8/1000], Loss: 1.3550\n","Epoch [10/1000], Loss: 1.3818\n","Epoch [12/1000], Loss: 1.4100\n","Epoch [14/1000], Loss: 1.4032\n","Epoch [16/1000], Loss: 1.2759\n","Epoch [18/1000], Loss: 1.3548\n","Epoch [20/1000], Loss: 1.2060\n","Epoch [22/1000], Loss: 1.2300\n","Epoch [24/1000], Loss: 1.2248\n","Epoch [26/1000], Loss: 1.2624\n","Epoch [28/1000], Loss: 1.3147\n","Epoch [30/1000], Loss: 1.3841\n","Epoch [32/1000], Loss: 1.3428\n","Epoch [34/1000], Loss: 0.9606\n","Epoch [36/1000], Loss: 0.9893\n","Epoch [38/1000], Loss: 1.2415\n","Epoch [40/1000], Loss: 1.5237\n","Epoch [42/1000], Loss: 1.4330\n","Epoch [44/1000], Loss: 1.0981\n","Epoch [46/1000], Loss: 1.1103\n","Epoch [48/1000], Loss: 1.2401\n","Epoch [50/1000], Loss: 1.2057\n","Epoch [52/1000], Loss: 1.2524\n","Epoch [54/1000], Loss: 1.4104\n","Epoch [56/1000], Loss: 1.0083\n","Epoch [58/1000], Loss: 1.2198\n","Epoch [60/1000], Loss: 0.8846\n","Epoch [62/1000], Loss: 1.0686\n","Epoch [64/1000], Loss: 1.0313\n","Epoch [66/1000], Loss: 1.3114\n","Epoch [68/1000], Loss: 1.1329\n","Epoch [70/1000], Loss: 1.1221\n","Epoch [72/1000], Loss: 0.9123\n","Epoch [74/1000], Loss: 1.0335\n","Epoch [76/1000], Loss: 0.7831\n","Epoch [78/1000], Loss: 0.8983\n","Epoch [80/1000], Loss: 0.7916\n","Epoch [82/1000], Loss: 0.9614\n","Epoch [84/1000], Loss: 0.8863\n","Epoch [86/1000], Loss: 0.6412\n","Epoch [88/1000], Loss: 0.8141\n","patience exceeded, loading best model\n","nnknn acc:  0.32608695652173914\n","Average accuracy:0.404\n","KNN accuracy:0.293\n","LMNN/NCA accuracy:0.326\n"]}],"source":["accuracies = []\n","knn_accuracies = []\n","lmnn_accuracies = []\n","PATH = os.path.join(folder_name, f'checkpoints/classifier_{dataset_name}.h5')\n","cfg.PATH = PATH\n","k_fold = KFold(n_splits=10, shuffle=True, random_state = None)\n","enable_lmnn = True\n","\n","for train_index, test_index in k_fold.split(Xs):\n","  # Get training and testing data\n","  X_train, X_test = Xs[train_index], Xs[test_index]\n","  y_train, y_test = ys[train_index], ys[test_index]\n","  if(enable_lmnn):\n","    # https://contrib.scikit-learn.org/metric-learn/supervised.html#lmnn\n","    lmnn = LMNN(n_neighbors=5, learn_rate=1e-6)\n","    ##TODO, change here if you need to use a different one\n","    # lmnn = metric_learn.MLKR()\n","    # lmnn = metric_learn.NCA(max_iter=1000)\n","    lmnn.fit(X_train,y_train)\n","    knn = KNeighborsClassifier(n_neighbors=5,metric=lmnn.get_metric())\n","    knn.fit(X_train,y_train)\n","    # klmnn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n","    lmnn_acc = accuracy_score(knn.predict(X_test), y_test)\n","    lmnn_accuracies.append(lmnn_acc)\n","    print(\"lmnn_acc: \",lmnn_acc)\n","    # continue\n","\n","  knn =  KNeighborsClassifier(n_neighbors=cfg.top_k)\n","  knn.fit(X_train, y_train)\n","  knn_acc  = accuracy_score(knn.predict(X_test), y_test)\n","  knn_accuracies.append(knn_acc)\n","  print(\"knn_acc: \", knn_acc)\n","\n","  best_accuracy, model = train_cls(X_train,y_train, X_test, y_test, cfg)\n","  accuracies.append(best_accuracy)\n","  print(\"nnknn acc: \", best_accuracy)\n","\n","\n","print(f\"Average accuracy:{np.mean(accuracies):.3f}\")\n","print(f\"KNN accuracy:{np.mean(knn_accuracies):.3f}\")\n","print(f\"LMNN/NCA accuracy:{np.mean(lmnn_accuracies):.3f}\")\n"]},{"cell_type":"code","source":["print(f\"Average accuracy:{np.mean(accuracies):.3f}\")\n","print(f\"KNN accuracy:{np.mean(knn_accuracies):.3f}\")\n","print(f\"LMNN/NCA accuracy:{np.mean(lmnn_accuracies):.3f}\")"],"metadata":{"id":"hmEeDk1RXAwp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719041612461,"user_tz":-480,"elapsed":20,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"40705558-41b7-4c34-a5b6-14a5ea4f5048"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy:0.404\n","KNN accuracy:0.293\n","LMNN/NCA accuracy:0.326\n"]}]},{"cell_type":"markdown","metadata":{"id":"QVO3SDp9mG4u"},"source":["# Regression with NNKNN"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1719034725705,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"},"user_tz":-480},"id":"x2uuoXv9ceJL","outputId":"c3151b54-da2a-4541-b304-8670464c923c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique values: [-1.3401817  -0.44672722  0.44672722  1.3401817 ]\n","Counts: [115 115 115 115]\n"]}],"source":["unique_values, counts = np.unique(ys, return_counts=True)\n","print(f\"Unique values: {unique_values}\")\n","print(f\"Counts: {counts}\")\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"7ll4n1HNmG4u","executionInfo":{"status":"ok","timestamp":1719034726234,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def train_reg(X_train,y_train, X_test, y_test, cfg:DictConfig):\n","  X_train = X_train.to(device)\n","  y_train = y_train.to(device)\n","  X_test = X_test.to(device)\n","\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=cfg.batch_size, shuffle=True)\n","  test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=cfg.batch_size, shuffle=False)\n","\n","\n","    # Train model\n","  model = Rmodel.NN_k_NN_regression(X_train,\n","                                    y_train,\n","                                    cfg.ca_weight_sharing,\n","                                    cfg.top_case_enabled,\n","                                    cfg.top_k,\n","                                    cfg.discount,\n","                                    cfg.class_weight_sharing,\n","                                    device=device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate) #, weight_decay=1e-5)\n","\n","  patience_counter = 0\n","  for epoch in range(cfg.training_epochs):\n","    # break # no training\n","    epoch_msg = True\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      _, _, _, predicted_number = model(X_train_batch)\n","      # break\n","      loss = criterion(predicted_number.squeeze(), y_train_batch)\n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      if epoch_msg and (epoch + 1) % 2 == 0:\n","        epoch_msg = False\n","        print(f'Epoch [{epoch + 1}/{cfg.training_epochs}], Loss: {loss.item():.4f}')\n","\n","    model.eval()\n","    with torch.no_grad():\n","      predicted_numbers = []\n","      for X_test_batch, _ in test_loader:\n","        X_test_batch = X_test_batch.to(device)\n","        _, _, _, predicted_number = model(X_test_batch)\n","        predicted_numbers.extend(predicted_number.squeeze().cpu().detach())\n","\n","      predicted_numbers = torch.Tensor(predicted_numbers)\n","      accuracy_temp = criterion(y_test, predicted_numbers)\n","\n","    if epoch == 0:\n","      best_accuracy = accuracy_temp\n","      torch.save(model.state_dict(), cfg.PATH)\n","    elif accuracy_temp < best_accuracy:\n","      torch.save(model.state_dict(), cfg.PATH)\n","      best_accuracy = accuracy_temp\n","      patience_counter = 0\n","    elif patience_counter > cfg.patience:\n","      model.eval()\n","      print(\"patience exceeded, loading best model\")\n","      break\n","    else:\n","      patience_counter += 1\n","\n","  _, case_activations, topk_avg, predicted_number = model(X_test)\n","\n","  top_case_indices = torch.topk(case_activations, 5, dim=1)[1].cpu()\n","\n","  accuracy = criterion(y_test, predicted_number.squeeze().cpu())\n","  y_train = y_train.cpu()\n","  top_k_average_accuracy = mean_squared_error(torch.mean(y_train[top_case_indices], dim=1), y_test)\n","  # top_k_average_accuracy = mean_squared_error(topk_avg.cpu(), y_test)\n","  return best_accuracy, accuracy, top_k_average_accuracy, model"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"9yCDMIuMRN38","executionInfo":{"status":"ok","timestamp":1719034726234,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# prompt: load_model_reg()\n","\n","def load_model_reg(X_train,y_train,cfg):\n","  # Define the model architecture\n","  model = Rmodel.NN_k_NN_regression(\n","      X_train,\n","      y_train,\n","      cfg.ca_weight_sharing,\n","      cfg.top_case_enabled,\n","      cfg.top_k,\n","      cfg.discount,\n","      cfg.class_weight_sharing,\n","      device=device\n","  )\n","  # Load the state dictionary\n","  model.load_state_dict(torch.load(cfg.path))\n","  model.to(device)\n","  model.eval()\n","  return model\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DN8UlUVpmG4v","executionInfo":{"status":"ok","timestamp":1719034973069,"user_tz":-480,"elapsed":246841,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"2c84b9e8-c91a-43c5-b864-d4574c2745d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [2/2000], Loss: 0.9824\n","Epoch [4/2000], Loss: 0.9979\n","Epoch [6/2000], Loss: 1.1160\n","Epoch [8/2000], Loss: 1.2820\n","Epoch [10/2000], Loss: 0.9681\n","Epoch [12/2000], Loss: 1.0968\n","Epoch [14/2000], Loss: 1.0549\n","Epoch [16/2000], Loss: 1.0445\n","Epoch [18/2000], Loss: 0.9969\n","Epoch [20/2000], Loss: 1.0323\n","Epoch [22/2000], Loss: 0.8884\n","Epoch [24/2000], Loss: 1.0314\n","Epoch [26/2000], Loss: 0.9428\n","Epoch [28/2000], Loss: 1.1208\n","Epoch [30/2000], Loss: 0.9594\n","Epoch [32/2000], Loss: 1.0688\n","Epoch [34/2000], Loss: 1.0028\n","Epoch [36/2000], Loss: 0.8363\n","Epoch [38/2000], Loss: 0.8800\n","Epoch [40/2000], Loss: 0.6802\n","Epoch [42/2000], Loss: 0.9955\n","patience exceeded, loading best model\n","Epoch [2/2000], Loss: 0.8782\n","Epoch [4/2000], Loss: 0.9787\n","Epoch [6/2000], Loss: 1.0730\n","Epoch [8/2000], Loss: 1.0274\n","Epoch [10/2000], Loss: 1.0489\n","Epoch [12/2000], Loss: 0.9963\n","Epoch [14/2000], Loss: 1.2338\n","Epoch [16/2000], Loss: 1.3437\n","Epoch [18/2000], Loss: 1.0548\n","Epoch [20/2000], Loss: 1.1276\n","Epoch [22/2000], Loss: 1.0233\n","Epoch [24/2000], Loss: 1.1151\n","Epoch [26/2000], Loss: 0.9598\n","Epoch [28/2000], Loss: 1.2246\n","Epoch [30/2000], Loss: 0.8707\n","Epoch [32/2000], Loss: 1.0954\n","Epoch [34/2000], Loss: 0.8640\n","Epoch [36/2000], Loss: 1.0216\n","Epoch [38/2000], Loss: 0.9758\n","Epoch [40/2000], Loss: 1.1314\n","Epoch [42/2000], Loss: 1.0068\n","Epoch [44/2000], Loss: 1.0478\n","Epoch [46/2000], Loss: 1.1035\n","patience exceeded, loading best model\n","Epoch [2/2000], Loss: 1.1662\n","Epoch [4/2000], Loss: 0.8539\n","Epoch [6/2000], Loss: 0.7683\n","Epoch [8/2000], Loss: 0.9605\n","Epoch [10/2000], Loss: 1.0154\n","Epoch [12/2000], Loss: 0.9871\n","Epoch [14/2000], Loss: 1.1899\n","Epoch [16/2000], Loss: 1.1668\n","Epoch [18/2000], Loss: 1.0715\n","Epoch [20/2000], Loss: 1.1010\n","Epoch [22/2000], Loss: 1.1539\n","Epoch [24/2000], Loss: 1.0996\n","Epoch [26/2000], Loss: 1.0457\n","Epoch [28/2000], Loss: 1.1290\n","Epoch [30/2000], Loss: 0.9892\n","Epoch [32/2000], Loss: 0.8330\n","Epoch [34/2000], Loss: 1.1543\n","Epoch [36/2000], Loss: 0.9710\n","Epoch [38/2000], Loss: 1.0072\n","Epoch [40/2000], Loss: 0.6975\n","Epoch [42/2000], Loss: 0.8904\n","Epoch [44/2000], Loss: 0.8956\n","Epoch [46/2000], Loss: 0.9967\n","Epoch [48/2000], Loss: 1.1384\n","Epoch [50/2000], Loss: 0.9788\n","Epoch [52/2000], Loss: 1.2040\n","Epoch [54/2000], Loss: 0.8844\n","Epoch [56/2000], Loss: 0.8686\n","Epoch [58/2000], Loss: 0.9280\n","Epoch [60/2000], Loss: 0.8107\n","Epoch [62/2000], Loss: 0.9706\n","Epoch [64/2000], Loss: 1.1184\n","Epoch [66/2000], Loss: 1.0825\n","Epoch [68/2000], Loss: 1.1399\n","Epoch [70/2000], Loss: 0.8369\n","Epoch [72/2000], Loss: 1.0419\n","Epoch [74/2000], Loss: 0.8302\n","Epoch [76/2000], Loss: 0.9962\n","Epoch [78/2000], Loss: 1.1314\n","Epoch [80/2000], Loss: 1.0262\n","Epoch [82/2000], Loss: 0.8618\n","Epoch [84/2000], Loss: 1.1044\n","Epoch [86/2000], Loss: 0.9069\n","Epoch [88/2000], Loss: 0.9844\n","Epoch [90/2000], Loss: 1.4415\n","Epoch [92/2000], Loss: 1.1690\n","Epoch [94/2000], Loss: 1.0339\n","Epoch [96/2000], Loss: 1.2449\n","Epoch [98/2000], Loss: 1.3198\n","Epoch [100/2000], Loss: 0.8724\n","Epoch [102/2000], Loss: 0.7614\n","Epoch [104/2000], Loss: 1.1253\n","Epoch [106/2000], Loss: 1.1837\n","Epoch [108/2000], Loss: 0.9013\n","Epoch [110/2000], Loss: 0.8684\n","Epoch [112/2000], Loss: 0.9033\n","Epoch [114/2000], Loss: 0.8435\n","Epoch [116/2000], Loss: 1.0692\n","Epoch [118/2000], Loss: 0.9201\n","Epoch [120/2000], Loss: 0.9888\n","Epoch [122/2000], Loss: 1.1262\n","Epoch [124/2000], Loss: 0.9755\n","Epoch [126/2000], Loss: 0.8561\n","Epoch [128/2000], Loss: 1.4060\n","Epoch [130/2000], Loss: 0.9148\n","Epoch [132/2000], Loss: 1.0339\n","Epoch [134/2000], Loss: 1.3928\n","Epoch [136/2000], Loss: 1.0381\n","Epoch [138/2000], Loss: 0.9244\n","Epoch [140/2000], Loss: 1.0553\n","Epoch [142/2000], Loss: 0.7544\n","Epoch [144/2000], Loss: 1.2495\n","Epoch [146/2000], Loss: 1.1100\n","Epoch [148/2000], Loss: 1.0176\n","Epoch [150/2000], Loss: 1.1140\n","Epoch [152/2000], Loss: 1.1213\n","Epoch [154/2000], Loss: 1.1799\n","Epoch [156/2000], Loss: 0.8339\n","Epoch [158/2000], Loss: 0.9378\n","Epoch [160/2000], Loss: 1.0338\n","Epoch [162/2000], Loss: 0.9101\n","Epoch [164/2000], Loss: 0.9855\n","Epoch [166/2000], Loss: 1.1069\n","Epoch [168/2000], Loss: 1.0608\n","Epoch [170/2000], Loss: 0.8285\n","Epoch [172/2000], Loss: 1.1004\n","Epoch [174/2000], Loss: 0.9234\n","Epoch [176/2000], Loss: 1.1687\n","Epoch [178/2000], Loss: 1.0477\n","Epoch [180/2000], Loss: 0.8626\n","Epoch [182/2000], Loss: 0.9253\n","Epoch [184/2000], Loss: 1.2165\n","Epoch [186/2000], Loss: 1.0578\n","Epoch [188/2000], Loss: 1.3090\n","Epoch [190/2000], Loss: 0.7820\n","Epoch [192/2000], Loss: 1.1192\n","Epoch [194/2000], Loss: 1.1575\n","Epoch [196/2000], Loss: 0.6418\n","Epoch [198/2000], Loss: 0.9898\n","Epoch [200/2000], Loss: 0.7268\n","Epoch [202/2000], Loss: 1.0089\n","Epoch [204/2000], Loss: 0.8927\n","Epoch [206/2000], Loss: 0.8021\n","Epoch [208/2000], Loss: 0.7572\n","Epoch [210/2000], Loss: 1.0292\n","Epoch [212/2000], Loss: 0.9054\n","Epoch [214/2000], Loss: 1.0019\n","Epoch [216/2000], Loss: 1.0727\n","Epoch [218/2000], Loss: 0.9605\n","Epoch [220/2000], Loss: 1.4159\n","Epoch [222/2000], Loss: 1.0373\n","Epoch [224/2000], Loss: 0.9859\n","Epoch [226/2000], Loss: 0.8617\n","Epoch [228/2000], Loss: 1.2968\n","Epoch [230/2000], Loss: 1.0404\n","Epoch [232/2000], Loss: 0.8888\n","Epoch [234/2000], Loss: 1.1107\n","Epoch [236/2000], Loss: 1.1152\n","Epoch [238/2000], Loss: 1.1425\n","Epoch [240/2000], Loss: 0.8795\n","Epoch [242/2000], Loss: 1.1723\n","Epoch [244/2000], Loss: 0.8678\n","Epoch [246/2000], Loss: 1.2147\n","Epoch [248/2000], Loss: 0.7971\n","Epoch [250/2000], Loss: 0.9729\n","Epoch [252/2000], Loss: 1.1245\n","Epoch [254/2000], Loss: 1.1470\n","Epoch [256/2000], Loss: 1.1482\n","Epoch [258/2000], Loss: 1.0917\n","Epoch [260/2000], Loss: 1.1318\n","Epoch [262/2000], Loss: 0.8917\n","Epoch [264/2000], Loss: 1.0823\n","Epoch [266/2000], Loss: 1.1080\n","Epoch [268/2000], Loss: 1.0733\n","Epoch [270/2000], Loss: 1.0073\n","Epoch [272/2000], Loss: 1.0169\n","Epoch [274/2000], Loss: 1.0622\n","Epoch [276/2000], Loss: 1.1414\n","Epoch [278/2000], Loss: 0.9936\n","Epoch [280/2000], Loss: 0.7321\n","Epoch [282/2000], Loss: 0.9800\n","Epoch [284/2000], Loss: 0.8563\n","Epoch [286/2000], Loss: 1.2288\n","Epoch [288/2000], Loss: 0.8570\n","Epoch [290/2000], Loss: 0.9423\n","Epoch [292/2000], Loss: 0.8786\n","Epoch [294/2000], Loss: 0.8505\n","Epoch [296/2000], Loss: 1.0071\n","Epoch [298/2000], Loss: 1.0361\n","Epoch [300/2000], Loss: 0.9856\n","Epoch [302/2000], Loss: 0.8095\n","Epoch [304/2000], Loss: 1.0239\n","Epoch [306/2000], Loss: 0.8388\n","Epoch [308/2000], Loss: 1.0990\n","Epoch [310/2000], Loss: 1.2853\n","Epoch [312/2000], Loss: 0.9900\n","Epoch [314/2000], Loss: 1.1505\n","Epoch [316/2000], Loss: 0.7098\n","Epoch [318/2000], Loss: 0.8583\n","Epoch [320/2000], Loss: 0.9709\n","Epoch [322/2000], Loss: 0.9363\n","Epoch [324/2000], Loss: 1.0224\n","Epoch [326/2000], Loss: 0.8553\n","Epoch [328/2000], Loss: 0.9168\n","Epoch [330/2000], Loss: 1.0300\n","Epoch [332/2000], Loss: 1.0697\n","Epoch [334/2000], Loss: 0.9480\n","Epoch [336/2000], Loss: 1.2023\n","Epoch [338/2000], Loss: 0.9210\n","Epoch [340/2000], Loss: 1.1492\n","Epoch [342/2000], Loss: 1.2422\n","Epoch [344/2000], Loss: 0.9158\n","Epoch [346/2000], Loss: 0.8601\n","Epoch [348/2000], Loss: 1.0508\n","Epoch [350/2000], Loss: 1.2229\n","Epoch [352/2000], Loss: 0.6576\n","Epoch [354/2000], Loss: 1.2003\n","Epoch [356/2000], Loss: 0.9695\n","Epoch [358/2000], Loss: 1.0886\n","Epoch [360/2000], Loss: 0.9264\n","Epoch [362/2000], Loss: 1.1916\n","Epoch [364/2000], Loss: 1.0173\n","Epoch [366/2000], Loss: 1.1007\n","Epoch [368/2000], Loss: 0.8550\n","Epoch [370/2000], Loss: 0.8779\n","Epoch [372/2000], Loss: 1.1357\n","Epoch [374/2000], Loss: 0.7107\n","Epoch [376/2000], Loss: 1.0532\n","Epoch [378/2000], Loss: 0.6946\n","Epoch [380/2000], Loss: 0.9518\n","Epoch [382/2000], Loss: 0.9564\n","Epoch [384/2000], Loss: 0.9599\n","Epoch [386/2000], Loss: 1.0824\n","Epoch [388/2000], Loss: 1.1206\n","Epoch [390/2000], Loss: 1.1909\n","Epoch [392/2000], Loss: 1.0299\n","Epoch [394/2000], Loss: 1.0775\n","Epoch [396/2000], Loss: 0.8848\n","Epoch [398/2000], Loss: 1.0899\n","Epoch [400/2000], Loss: 1.0361\n","Epoch [402/2000], Loss: 0.9653\n","Epoch [404/2000], Loss: 0.8586\n","Epoch [406/2000], Loss: 0.8878\n","Epoch [408/2000], Loss: 1.1957\n","Epoch [410/2000], Loss: 1.2615\n","Epoch [412/2000], Loss: 0.9165\n","Epoch [414/2000], Loss: 1.0478\n","Epoch [416/2000], Loss: 0.9884\n","Epoch [418/2000], Loss: 1.2289\n","Epoch [420/2000], Loss: 0.9724\n","Epoch [422/2000], Loss: 0.8175\n","Epoch [424/2000], Loss: 0.9014\n","Epoch [426/2000], Loss: 1.2471\n","Epoch [428/2000], Loss: 0.8915\n","Epoch [430/2000], Loss: 1.0701\n","Epoch [432/2000], Loss: 1.2234\n","Epoch [434/2000], Loss: 0.9404\n","Epoch [436/2000], Loss: 0.9463\n","Epoch [438/2000], Loss: 1.0820\n","Epoch [440/2000], Loss: 1.2153\n","Epoch [442/2000], Loss: 1.0272\n","Epoch [444/2000], Loss: 0.7751\n","Epoch [446/2000], Loss: 1.0905\n","Epoch [448/2000], Loss: 1.1545\n","Epoch [450/2000], Loss: 0.6828\n","Epoch [452/2000], Loss: 1.0319\n","Epoch [454/2000], Loss: 0.9023\n","Epoch [456/2000], Loss: 0.9983\n","Epoch [458/2000], Loss: 1.1240\n","Epoch [460/2000], Loss: 1.0328\n","Epoch [462/2000], Loss: 0.8524\n","Epoch [464/2000], Loss: 0.9450\n","Epoch [466/2000], Loss: 1.0975\n","Epoch [468/2000], Loss: 1.0403\n","Epoch [470/2000], Loss: 0.8743\n","Epoch [472/2000], Loss: 0.9670\n","Epoch [474/2000], Loss: 1.2737\n","Epoch [476/2000], Loss: 0.9809\n","Epoch [478/2000], Loss: 1.1370\n","Epoch [480/2000], Loss: 0.8495\n","Epoch [482/2000], Loss: 0.9562\n","Epoch [484/2000], Loss: 0.7745\n","Epoch [486/2000], Loss: 1.0023\n","Epoch [488/2000], Loss: 0.9415\n","Epoch [490/2000], Loss: 1.1595\n","Epoch [492/2000], Loss: 1.2092\n","Epoch [494/2000], Loss: 1.0401\n","Epoch [496/2000], Loss: 1.0684\n","Epoch [498/2000], Loss: 1.1611\n","Epoch [500/2000], Loss: 1.0332\n","Epoch [502/2000], Loss: 0.7833\n","Epoch [504/2000], Loss: 0.7394\n","Epoch [506/2000], Loss: 1.2267\n","Epoch [508/2000], Loss: 1.1330\n","Epoch [510/2000], Loss: 0.7993\n","Epoch [512/2000], Loss: 1.0620\n","Epoch [514/2000], Loss: 0.9787\n","Epoch [516/2000], Loss: 1.1188\n","Epoch [518/2000], Loss: 1.0766\n","Epoch [520/2000], Loss: 1.0492\n","Epoch [522/2000], Loss: 0.9861\n","Epoch [524/2000], Loss: 1.1025\n","Epoch [526/2000], Loss: 0.7517\n","Epoch [528/2000], Loss: 0.9650\n","Epoch [530/2000], Loss: 1.0148\n","Epoch [532/2000], Loss: 1.0747\n","Epoch [534/2000], Loss: 0.9567\n","Epoch [536/2000], Loss: 0.9402\n","Epoch [538/2000], Loss: 0.9072\n","Epoch [540/2000], Loss: 1.1883\n","Epoch [542/2000], Loss: 0.7089\n","Epoch [544/2000], Loss: 1.0149\n","Epoch [546/2000], Loss: 1.0023\n","Epoch [548/2000], Loss: 1.1684\n","Epoch [550/2000], Loss: 0.9146\n","Epoch [552/2000], Loss: 0.9984\n","Epoch [554/2000], Loss: 1.1524\n","Epoch [556/2000], Loss: 0.9962\n","Epoch [558/2000], Loss: 1.0701\n","Epoch [560/2000], Loss: 0.7956\n","Epoch [562/2000], Loss: 0.8601\n","Epoch [564/2000], Loss: 1.1973\n","Epoch [566/2000], Loss: 1.0307\n","Epoch [568/2000], Loss: 0.9722\n","Epoch [570/2000], Loss: 0.9900\n","Epoch [572/2000], Loss: 0.9607\n","Epoch [574/2000], Loss: 1.0054\n","Epoch [576/2000], Loss: 1.0902\n","Epoch [578/2000], Loss: 0.9911\n","Epoch [580/2000], Loss: 1.1564\n","Epoch [582/2000], Loss: 0.8463\n","Epoch [584/2000], Loss: 0.7775\n","Epoch [586/2000], Loss: 1.0705\n","Epoch [588/2000], Loss: 0.9761\n","Epoch [590/2000], Loss: 1.1142\n","Epoch [592/2000], Loss: 0.9293\n","Epoch [594/2000], Loss: 0.8651\n","Epoch [596/2000], Loss: 0.8834\n","Epoch [598/2000], Loss: 0.9438\n","Epoch [600/2000], Loss: 0.9601\n","Epoch [602/2000], Loss: 1.0587\n","Epoch [604/2000], Loss: 1.0406\n","Epoch [606/2000], Loss: 0.9464\n","Epoch [608/2000], Loss: 0.9097\n","Epoch [610/2000], Loss: 1.0337\n","Epoch [612/2000], Loss: 0.9847\n","Epoch [614/2000], Loss: 0.8669\n","Epoch [616/2000], Loss: 0.9156\n","Epoch [618/2000], Loss: 0.9564\n","Epoch [620/2000], Loss: 0.8419\n","Epoch [622/2000], Loss: 1.0347\n","Epoch [624/2000], Loss: 1.1440\n","Epoch [626/2000], Loss: 0.7554\n","Epoch [628/2000], Loss: 1.0335\n","Epoch [630/2000], Loss: 1.0351\n","Epoch [632/2000], Loss: 1.2734\n","Epoch [634/2000], Loss: 1.0369\n","Epoch [636/2000], Loss: 1.2830\n","Epoch [638/2000], Loss: 0.9947\n","Epoch [640/2000], Loss: 1.0820\n","Epoch [642/2000], Loss: 0.7132\n","Epoch [644/2000], Loss: 0.8661\n","Epoch [646/2000], Loss: 0.9938\n","Epoch [648/2000], Loss: 0.7446\n","Epoch [650/2000], Loss: 0.8694\n","Epoch [652/2000], Loss: 0.9777\n","Epoch [654/2000], Loss: 1.0181\n","Epoch [656/2000], Loss: 0.9623\n","Epoch [658/2000], Loss: 1.2571\n","Epoch [660/2000], Loss: 1.0145\n","Epoch [662/2000], Loss: 0.9540\n","Epoch [664/2000], Loss: 1.1082\n","Epoch [666/2000], Loss: 1.0394\n","Epoch [668/2000], Loss: 1.0390\n","Epoch [670/2000], Loss: 1.0230\n","Epoch [672/2000], Loss: 0.7811\n","Epoch [674/2000], Loss: 1.0688\n","Epoch [676/2000], Loss: 1.0779\n","Epoch [678/2000], Loss: 1.0212\n","Epoch [680/2000], Loss: 1.0928\n","Epoch [682/2000], Loss: 1.2006\n","Epoch [684/2000], Loss: 1.0837\n","Epoch [686/2000], Loss: 1.0801\n","Epoch [688/2000], Loss: 1.0521\n","Epoch [690/2000], Loss: 1.0047\n","Epoch [692/2000], Loss: 0.9985\n","Epoch [694/2000], Loss: 1.2863\n","Epoch [696/2000], Loss: 0.9527\n","Epoch [698/2000], Loss: 1.1213\n","Epoch [700/2000], Loss: 0.9149\n","Epoch [702/2000], Loss: 0.7774\n","Epoch [704/2000], Loss: 0.9996\n","Epoch [706/2000], Loss: 0.7477\n","Epoch [708/2000], Loss: 1.0655\n","Epoch [710/2000], Loss: 1.0918\n","Epoch [712/2000], Loss: 0.9631\n","Epoch [714/2000], Loss: 1.0164\n","Epoch [716/2000], Loss: 1.0070\n","Epoch [718/2000], Loss: 0.9950\n","Epoch [720/2000], Loss: 0.8294\n","Epoch [722/2000], Loss: 1.1220\n","Epoch [724/2000], Loss: 0.9193\n","Epoch [726/2000], Loss: 0.6399\n","Epoch [728/2000], Loss: 1.0079\n","Epoch [730/2000], Loss: 0.7249\n","Epoch [732/2000], Loss: 1.0161\n","Epoch [734/2000], Loss: 0.8870\n","Epoch [736/2000], Loss: 1.0146\n","Epoch [738/2000], Loss: 1.1027\n","Epoch [740/2000], Loss: 1.1664\n","Epoch [742/2000], Loss: 0.9972\n","Epoch [744/2000], Loss: 0.9226\n","Epoch [746/2000], Loss: 0.9589\n","Epoch [748/2000], Loss: 1.0445\n","Epoch [750/2000], Loss: 0.8216\n","Epoch [752/2000], Loss: 0.9284\n","Epoch [754/2000], Loss: 1.0365\n","Epoch [756/2000], Loss: 1.1278\n","Epoch [758/2000], Loss: 0.9172\n","Epoch [760/2000], Loss: 0.8639\n","Epoch [762/2000], Loss: 0.9266\n","Epoch [764/2000], Loss: 0.7044\n","Epoch [766/2000], Loss: 0.7432\n","Epoch [768/2000], Loss: 0.7789\n","Epoch [770/2000], Loss: 1.0402\n","Epoch [772/2000], Loss: 0.8781\n","Epoch [774/2000], Loss: 0.9154\n","Epoch [776/2000], Loss: 0.9628\n","Epoch [778/2000], Loss: 0.7046\n","Epoch [780/2000], Loss: 0.9483\n","Epoch [782/2000], Loss: 1.0621\n","Epoch [784/2000], Loss: 0.8877\n","Epoch [786/2000], Loss: 1.1010\n","Epoch [788/2000], Loss: 0.9108\n","Epoch [790/2000], Loss: 1.1111\n","Epoch [792/2000], Loss: 0.8676\n","Epoch [794/2000], Loss: 1.1596\n","Epoch [796/2000], Loss: 0.8698\n","Epoch [798/2000], Loss: 0.8225\n","Epoch [800/2000], Loss: 1.0031\n","Epoch [802/2000], Loss: 1.0734\n","Epoch [804/2000], Loss: 1.0293\n","Epoch [806/2000], Loss: 0.8223\n","Epoch [808/2000], Loss: 1.0725\n","Epoch [810/2000], Loss: 1.1266\n","Epoch [812/2000], Loss: 0.9842\n","Epoch [814/2000], Loss: 1.0292\n","Epoch [816/2000], Loss: 0.8613\n","Epoch [818/2000], Loss: 1.0450\n","Epoch [820/2000], Loss: 0.9160\n","Epoch [822/2000], Loss: 0.7955\n","Epoch [824/2000], Loss: 0.9231\n","Epoch [826/2000], Loss: 0.9611\n","Epoch [828/2000], Loss: 0.7922\n","Epoch [830/2000], Loss: 0.9786\n","Epoch [832/2000], Loss: 1.0248\n","Epoch [834/2000], Loss: 0.8210\n","Epoch [836/2000], Loss: 0.8861\n","Epoch [838/2000], Loss: 1.1693\n","Epoch [840/2000], Loss: 0.9141\n","Epoch [842/2000], Loss: 0.8943\n","Epoch [844/2000], Loss: 1.0042\n","Epoch [846/2000], Loss: 0.8467\n","Epoch [848/2000], Loss: 1.0175\n","Epoch [850/2000], Loss: 1.0073\n","Epoch [852/2000], Loss: 0.8213\n","Epoch [854/2000], Loss: 0.5850\n","Epoch [856/2000], Loss: 0.8627\n","Epoch [858/2000], Loss: 1.1163\n","Epoch [860/2000], Loss: 0.8276\n","Epoch [862/2000], Loss: 1.0534\n","Epoch [864/2000], Loss: 0.9336\n","Epoch [866/2000], Loss: 0.8383\n","Epoch [868/2000], Loss: 0.8704\n","Epoch [870/2000], Loss: 0.8247\n","Epoch [872/2000], Loss: 0.6925\n","Epoch [874/2000], Loss: 0.9903\n","Epoch [876/2000], Loss: 0.9261\n","Epoch [878/2000], Loss: 0.9114\n","Epoch [880/2000], Loss: 0.8671\n","Epoch [882/2000], Loss: 0.8946\n","Epoch [884/2000], Loss: 0.9420\n","Epoch [886/2000], Loss: 0.9348\n","Epoch [888/2000], Loss: 1.1387\n","Epoch [890/2000], Loss: 0.8281\n","Epoch [892/2000], Loss: 1.0673\n","Epoch [894/2000], Loss: 0.9385\n","Epoch [896/2000], Loss: 1.3700\n","Epoch [898/2000], Loss: 0.9902\n","Epoch [900/2000], Loss: 0.8664\n","Epoch [902/2000], Loss: 0.9771\n","Epoch [904/2000], Loss: 0.9659\n","Epoch [906/2000], Loss: 1.0479\n","Epoch [908/2000], Loss: 1.0807\n","Epoch [910/2000], Loss: 0.9363\n","Epoch [912/2000], Loss: 1.0973\n","Epoch [914/2000], Loss: 0.7178\n","Epoch [916/2000], Loss: 0.7890\n","Epoch [918/2000], Loss: 0.8261\n","Epoch [920/2000], Loss: 0.6956\n","Epoch [922/2000], Loss: 0.9815\n","Epoch [924/2000], Loss: 0.8200\n","Epoch [926/2000], Loss: 1.2838\n","Epoch [928/2000], Loss: 0.8155\n","Epoch [930/2000], Loss: 0.7465\n","Epoch [932/2000], Loss: 0.8147\n","Epoch [934/2000], Loss: 0.8782\n","Epoch [936/2000], Loss: 1.0011\n","Epoch [938/2000], Loss: 0.6779\n","Epoch [940/2000], Loss: 1.0123\n","Epoch [942/2000], Loss: 1.2160\n","Epoch [944/2000], Loss: 1.1011\n","Epoch [946/2000], Loss: 0.9042\n","Epoch [948/2000], Loss: 1.0166\n","Epoch [950/2000], Loss: 0.8037\n","Epoch [952/2000], Loss: 0.8952\n","Epoch [954/2000], Loss: 0.9383\n","Epoch [956/2000], Loss: 0.8256\n","Epoch [958/2000], Loss: 1.0428\n","Epoch [960/2000], Loss: 1.1688\n","Epoch [962/2000], Loss: 0.9959\n","Epoch [964/2000], Loss: 0.9479\n","Epoch [966/2000], Loss: 0.8374\n","Epoch [968/2000], Loss: 0.8985\n","Epoch [970/2000], Loss: 1.0768\n","Epoch [972/2000], Loss: 1.1144\n","Epoch [974/2000], Loss: 1.1075\n","Epoch [976/2000], Loss: 0.7104\n","Epoch [978/2000], Loss: 0.9859\n","Epoch [980/2000], Loss: 0.9051\n","Epoch [982/2000], Loss: 0.9356\n","Epoch [984/2000], Loss: 0.7467\n","Epoch [986/2000], Loss: 0.9794\n","Epoch [988/2000], Loss: 0.9839\n","Epoch [990/2000], Loss: 0.7923\n","Epoch [992/2000], Loss: 0.8656\n","Epoch [994/2000], Loss: 0.9622\n","Epoch [996/2000], Loss: 0.8167\n","Epoch [998/2000], Loss: 1.0290\n","Epoch [1000/2000], Loss: 1.2519\n","Epoch [1002/2000], Loss: 0.7453\n","Epoch [1004/2000], Loss: 0.8107\n","Epoch [1006/2000], Loss: 1.0080\n","Epoch [1008/2000], Loss: 1.1984\n","Epoch [1010/2000], Loss: 0.8978\n","Epoch [1012/2000], Loss: 0.5931\n","Epoch [1014/2000], Loss: 0.9089\n","Epoch [1016/2000], Loss: 1.1355\n","Epoch [1018/2000], Loss: 0.9293\n","Epoch [1020/2000], Loss: 0.8624\n","Epoch [1022/2000], Loss: 0.8536\n","Epoch [1024/2000], Loss: 0.9199\n","Epoch [1026/2000], Loss: 0.8528\n","Epoch [1028/2000], Loss: 0.6014\n","Epoch [1030/2000], Loss: 1.0579\n","Epoch [1032/2000], Loss: 0.9111\n","Epoch [1034/2000], Loss: 1.1313\n","Epoch [1036/2000], Loss: 1.0947\n","Epoch [1038/2000], Loss: 0.6488\n","Epoch [1040/2000], Loss: 0.8355\n","Epoch [1042/2000], Loss: 1.0674\n","Epoch [1044/2000], Loss: 0.7814\n","Epoch [1046/2000], Loss: 0.7808\n","Epoch [1048/2000], Loss: 0.6873\n","Epoch [1050/2000], Loss: 1.0472\n","Epoch [1052/2000], Loss: 1.0408\n","Epoch [1054/2000], Loss: 0.8676\n","Epoch [1056/2000], Loss: 1.0245\n","Epoch [1058/2000], Loss: 0.9423\n","Epoch [1060/2000], Loss: 0.7597\n","Epoch [1062/2000], Loss: 0.9714\n","Epoch [1064/2000], Loss: 1.1137\n","Epoch [1066/2000], Loss: 0.7408\n","Epoch [1068/2000], Loss: 0.8724\n","Epoch [1070/2000], Loss: 0.8401\n","Epoch [1072/2000], Loss: 0.7359\n","Epoch [1074/2000], Loss: 0.7494\n","Epoch [1076/2000], Loss: 0.5594\n","Epoch [1078/2000], Loss: 0.9665\n","Epoch [1080/2000], Loss: 0.7913\n","Epoch [1082/2000], Loss: 0.7599\n","Epoch [1084/2000], Loss: 0.8816\n","Epoch [1086/2000], Loss: 0.9074\n","Epoch [1088/2000], Loss: 1.1114\n","Epoch [1090/2000], Loss: 0.7999\n","Epoch [1092/2000], Loss: 0.7515\n","Epoch [1094/2000], Loss: 0.8721\n","Epoch [1096/2000], Loss: 0.7643\n","Epoch [1098/2000], Loss: 0.8418\n","Epoch [1100/2000], Loss: 0.6805\n","Epoch [1102/2000], Loss: 0.9013\n","Epoch [1104/2000], Loss: 0.9817\n","Epoch [1106/2000], Loss: 0.9330\n","Epoch [1108/2000], Loss: 0.8275\n","Epoch [1110/2000], Loss: 1.2730\n","Epoch [1112/2000], Loss: 0.8586\n","Epoch [1114/2000], Loss: 1.0036\n","Epoch [1116/2000], Loss: 1.1020\n","Epoch [1118/2000], Loss: 0.8391\n","Epoch [1120/2000], Loss: 0.5818\n","Epoch [1122/2000], Loss: 1.1473\n","Epoch [1124/2000], Loss: 0.6577\n","Epoch [1126/2000], Loss: 1.0025\n","Epoch [1128/2000], Loss: 0.9099\n","Epoch [1130/2000], Loss: 1.1673\n","Epoch [1132/2000], Loss: 0.9508\n","Epoch [1134/2000], Loss: 0.9628\n","Epoch [1136/2000], Loss: 1.2278\n","Epoch [1138/2000], Loss: 1.0726\n","Epoch [1140/2000], Loss: 0.8289\n","Epoch [1142/2000], Loss: 0.8358\n","Epoch [1144/2000], Loss: 0.7617\n","Epoch [1146/2000], Loss: 0.9051\n","Epoch [1148/2000], Loss: 0.8858\n","Epoch [1150/2000], Loss: 1.0196\n","Epoch [1152/2000], Loss: 0.8404\n","Epoch [1154/2000], Loss: 0.9217\n","Epoch [1156/2000], Loss: 0.6292\n","Epoch [1158/2000], Loss: 1.0865\n","Epoch [1160/2000], Loss: 1.0168\n","Epoch [1162/2000], Loss: 0.6889\n","Epoch [1164/2000], Loss: 1.1515\n","Epoch [1166/2000], Loss: 0.8186\n","Epoch [1168/2000], Loss: 1.0159\n","Epoch [1170/2000], Loss: 0.8538\n","Epoch [1172/2000], Loss: 0.7673\n","Epoch [1174/2000], Loss: 1.0608\n","Epoch [1176/2000], Loss: 0.8759\n","Epoch [1178/2000], Loss: 0.8389\n","Epoch [1180/2000], Loss: 0.8502\n","Epoch [1182/2000], Loss: 0.8450\n","Epoch [1184/2000], Loss: 0.8232\n","Epoch [1186/2000], Loss: 1.1391\n","Epoch [1188/2000], Loss: 1.1205\n","Epoch [1190/2000], Loss: 1.0499\n","Epoch [1192/2000], Loss: 0.7380\n","Epoch [1194/2000], Loss: 0.9055\n","Epoch [1196/2000], Loss: 1.0110\n","Epoch [1198/2000], Loss: 0.8560\n","Epoch [1200/2000], Loss: 1.1041\n","Epoch [1202/2000], Loss: 0.8223\n","Epoch [1204/2000], Loss: 1.0139\n","Epoch [1206/2000], Loss: 0.8216\n","Epoch [1208/2000], Loss: 1.0550\n","Epoch [1210/2000], Loss: 0.8903\n","Epoch [1212/2000], Loss: 1.0050\n","Epoch [1214/2000], Loss: 0.8766\n","Epoch [1216/2000], Loss: 1.0107\n","Epoch [1218/2000], Loss: 0.8732\n","Epoch [1220/2000], Loss: 0.7868\n","Epoch [1222/2000], Loss: 0.7545\n","Epoch [1224/2000], Loss: 0.7912\n","Epoch [1226/2000], Loss: 0.9434\n","Epoch [1228/2000], Loss: 0.8608\n","Epoch [1230/2000], Loss: 1.0963\n","Epoch [1232/2000], Loss: 1.2677\n","Epoch [1234/2000], Loss: 0.9548\n","Epoch [1236/2000], Loss: 0.9965\n","Epoch [1238/2000], Loss: 0.9413\n","Epoch [1240/2000], Loss: 0.9708\n","Epoch [1242/2000], Loss: 0.8573\n","Epoch [1244/2000], Loss: 1.1119\n","Epoch [1246/2000], Loss: 0.8982\n","Epoch [1248/2000], Loss: 0.7714\n","Epoch [1250/2000], Loss: 0.8585\n","Epoch [1252/2000], Loss: 0.8502\n","Epoch [1254/2000], Loss: 0.9394\n","Epoch [1256/2000], Loss: 0.9856\n","Epoch [1258/2000], Loss: 0.9117\n","Epoch [1260/2000], Loss: 1.1311\n","Epoch [1262/2000], Loss: 0.8650\n","Epoch [1264/2000], Loss: 0.9232\n","Epoch [1266/2000], Loss: 0.9181\n","Epoch [1268/2000], Loss: 1.0178\n","Epoch [1270/2000], Loss: 0.7500\n","Epoch [1272/2000], Loss: 0.7257\n","Epoch [1274/2000], Loss: 1.1044\n","Epoch [1276/2000], Loss: 0.6519\n","Epoch [1278/2000], Loss: 0.8207\n","Epoch [1280/2000], Loss: 0.9423\n","Epoch [1282/2000], Loss: 0.7955\n","Epoch [1284/2000], Loss: 0.9156\n","Epoch [1286/2000], Loss: 0.7235\n","Epoch [1288/2000], Loss: 0.6111\n","Epoch [1290/2000], Loss: 1.0561\n","Epoch [1292/2000], Loss: 0.9121\n","Epoch [1294/2000], Loss: 0.9069\n","Epoch [1296/2000], Loss: 0.9164\n","Epoch [1298/2000], Loss: 1.1989\n","Epoch [1300/2000], Loss: 0.8345\n","Epoch [1302/2000], Loss: 0.7700\n","Epoch [1304/2000], Loss: 0.8170\n","Epoch [1306/2000], Loss: 0.7192\n","Epoch [1308/2000], Loss: 0.9125\n","Epoch [1310/2000], Loss: 0.7808\n","Epoch [1312/2000], Loss: 0.8751\n","Epoch [1314/2000], Loss: 1.1563\n","Epoch [1316/2000], Loss: 0.9090\n","Epoch [1318/2000], Loss: 0.9810\n","Epoch [1320/2000], Loss: 0.6171\n","Epoch [1322/2000], Loss: 0.9603\n","Epoch [1324/2000], Loss: 0.8402\n","Epoch [1326/2000], Loss: 0.7689\n","Epoch [1328/2000], Loss: 0.9358\n","Epoch [1330/2000], Loss: 0.8674\n","Epoch [1332/2000], Loss: 1.1625\n","Epoch [1334/2000], Loss: 0.8148\n","Epoch [1336/2000], Loss: 0.8992\n","Epoch [1338/2000], Loss: 0.9575\n","Epoch [1340/2000], Loss: 0.8667\n","Epoch [1342/2000], Loss: 0.8462\n","Epoch [1344/2000], Loss: 1.1871\n","Epoch [1346/2000], Loss: 0.9040\n","Epoch [1348/2000], Loss: 0.8588\n","Epoch [1350/2000], Loss: 0.7871\n","Epoch [1352/2000], Loss: 0.8180\n","Epoch [1354/2000], Loss: 1.0050\n","Epoch [1356/2000], Loss: 0.6824\n","Epoch [1358/2000], Loss: 0.8554\n","Epoch [1360/2000], Loss: 0.9882\n","Epoch [1362/2000], Loss: 0.6696\n","Epoch [1364/2000], Loss: 0.7601\n","Epoch [1366/2000], Loss: 0.9109\n","Epoch [1368/2000], Loss: 0.8220\n","Epoch [1370/2000], Loss: 0.8215\n","Epoch [1372/2000], Loss: 1.0799\n","Epoch [1374/2000], Loss: 0.8239\n","Epoch [1376/2000], Loss: 0.9452\n","Epoch [1378/2000], Loss: 0.8801\n","Epoch [1380/2000], Loss: 1.0184\n","Epoch [1382/2000], Loss: 0.8638\n","Epoch [1384/2000], Loss: 0.5967\n","Epoch [1386/2000], Loss: 0.8269\n","Epoch [1388/2000], Loss: 0.9434\n","Epoch [1390/2000], Loss: 0.8985\n","Epoch [1392/2000], Loss: 0.9013\n","Epoch [1394/2000], Loss: 0.8940\n","Epoch [1396/2000], Loss: 0.8662\n","Epoch [1398/2000], Loss: 1.1580\n","Epoch [1400/2000], Loss: 0.9782\n","Epoch [1402/2000], Loss: 0.8357\n","Epoch [1404/2000], Loss: 1.2022\n","Epoch [1406/2000], Loss: 1.0318\n","Epoch [1408/2000], Loss: 0.6980\n","Epoch [1410/2000], Loss: 1.1439\n","Epoch [1412/2000], Loss: 0.6499\n","Epoch [1414/2000], Loss: 0.9306\n","Epoch [1416/2000], Loss: 0.8400\n","Epoch [1418/2000], Loss: 1.1061\n","Epoch [1420/2000], Loss: 1.0226\n","Epoch [1422/2000], Loss: 0.6883\n","Epoch [1424/2000], Loss: 0.8379\n","Epoch [1426/2000], Loss: 0.7536\n","Epoch [1428/2000], Loss: 0.9284\n","Epoch [1430/2000], Loss: 0.7523\n","Epoch [1432/2000], Loss: 0.7358\n","Epoch [1434/2000], Loss: 0.9176\n","Epoch [1436/2000], Loss: 0.8705\n","Epoch [1438/2000], Loss: 0.8068\n","Epoch [1440/2000], Loss: 0.9629\n","Epoch [1442/2000], Loss: 0.9927\n","Epoch [1444/2000], Loss: 0.7547\n","Epoch [1446/2000], Loss: 0.9187\n","Epoch [1448/2000], Loss: 0.8254\n","Epoch [1450/2000], Loss: 0.8586\n","Epoch [1452/2000], Loss: 0.9897\n","Epoch [1454/2000], Loss: 0.8238\n","Epoch [1456/2000], Loss: 0.7127\n","Epoch [1458/2000], Loss: 0.7090\n","Epoch [1460/2000], Loss: 0.6054\n","Epoch [1462/2000], Loss: 1.1210\n","Epoch [1464/2000], Loss: 1.0317\n","Epoch [1466/2000], Loss: 0.8461\n","Epoch [1468/2000], Loss: 0.7475\n","Epoch [1470/2000], Loss: 0.7392\n","Epoch [1472/2000], Loss: 0.8767\n","Epoch [1474/2000], Loss: 0.9993\n","Epoch [1476/2000], Loss: 0.9291\n","Epoch [1478/2000], Loss: 1.0476\n","Epoch [1480/2000], Loss: 0.7619\n","Epoch [1482/2000], Loss: 0.5987\n","Epoch [1484/2000], Loss: 1.0502\n","Epoch [1486/2000], Loss: 0.8563\n","Epoch [1488/2000], Loss: 0.8212\n","Epoch [1490/2000], Loss: 0.7762\n","Epoch [1492/2000], Loss: 0.6095\n","Epoch [1494/2000], Loss: 0.9597\n","Epoch [1496/2000], Loss: 0.9089\n","Epoch [1498/2000], Loss: 1.0844\n","Epoch [1500/2000], Loss: 0.6721\n","Epoch [1502/2000], Loss: 0.8228\n","Epoch [1504/2000], Loss: 0.9110\n","Epoch [1506/2000], Loss: 1.0155\n","Epoch [1508/2000], Loss: 0.8383\n","Epoch [1510/2000], Loss: 0.8651\n","Epoch [1512/2000], Loss: 0.8991\n","Epoch [1514/2000], Loss: 0.5432\n","Epoch [1516/2000], Loss: 0.9206\n","Epoch [1518/2000], Loss: 0.8876\n","Epoch [1520/2000], Loss: 1.0637\n","Epoch [1522/2000], Loss: 0.9570\n","Epoch [1524/2000], Loss: 0.6995\n","Epoch [1526/2000], Loss: 0.8256\n","Epoch [1528/2000], Loss: 0.8257\n","Epoch [1530/2000], Loss: 0.8687\n","Epoch [1532/2000], Loss: 0.8883\n","Epoch [1534/2000], Loss: 0.8690\n","Epoch [1536/2000], Loss: 0.8183\n","Epoch [1538/2000], Loss: 0.9008\n","Epoch [1540/2000], Loss: 0.7249\n","Epoch [1542/2000], Loss: 0.7442\n","Epoch [1544/2000], Loss: 0.8682\n","Epoch [1546/2000], Loss: 0.8990\n","Epoch [1548/2000], Loss: 0.9690\n","Epoch [1550/2000], Loss: 0.8167\n","Epoch [1552/2000], Loss: 0.5818\n","Epoch [1554/2000], Loss: 1.0818\n","Epoch [1556/2000], Loss: 0.7612\n","Epoch [1558/2000], Loss: 0.9363\n","Epoch [1560/2000], Loss: 1.0908\n","Epoch [1562/2000], Loss: 0.7220\n","Epoch [1564/2000], Loss: 0.8504\n","Epoch [1566/2000], Loss: 0.9715\n","Epoch [1568/2000], Loss: 0.8308\n","Epoch [1570/2000], Loss: 0.8359\n","Epoch [1572/2000], Loss: 0.7720\n","Epoch [1574/2000], Loss: 0.7847\n","Epoch [1576/2000], Loss: 0.7839\n","Epoch [1578/2000], Loss: 0.8642\n","Epoch [1580/2000], Loss: 0.7220\n","Epoch [1582/2000], Loss: 0.9445\n","Epoch [1584/2000], Loss: 0.8837\n","Epoch [1586/2000], Loss: 0.7364\n","Epoch [1588/2000], Loss: 0.6346\n","Epoch [1590/2000], Loss: 0.7729\n","Epoch [1592/2000], Loss: 1.0129\n","Epoch [1594/2000], Loss: 0.8533\n","Epoch [1596/2000], Loss: 0.4701\n","Epoch [1598/2000], Loss: 0.9524\n","Epoch [1600/2000], Loss: 1.1308\n","Epoch [1602/2000], Loss: 1.0864\n","Epoch [1604/2000], Loss: 0.9505\n","Epoch [1606/2000], Loss: 0.8036\n","Epoch [1608/2000], Loss: 0.8137\n","Epoch [1610/2000], Loss: 0.6372\n","Epoch [1612/2000], Loss: 1.0045\n","Epoch [1614/2000], Loss: 0.7703\n","Epoch [1616/2000], Loss: 0.9461\n","Epoch [1618/2000], Loss: 0.7083\n","Epoch [1620/2000], Loss: 0.7293\n","Epoch [1622/2000], Loss: 1.0760\n","Epoch [1624/2000], Loss: 0.7558\n","Epoch [1626/2000], Loss: 0.7610\n","Epoch [1628/2000], Loss: 0.9036\n","Epoch [1630/2000], Loss: 0.8642\n","Epoch [1632/2000], Loss: 0.8071\n","Epoch [1634/2000], Loss: 0.6916\n","Epoch [1636/2000], Loss: 0.8366\n","Epoch [1638/2000], Loss: 0.8689\n","Epoch [1640/2000], Loss: 0.6724\n","Epoch [1642/2000], Loss: 1.0480\n","Epoch [1644/2000], Loss: 0.9447\n","Epoch [1646/2000], Loss: 0.8735\n","Epoch [1648/2000], Loss: 1.0142\n","Epoch [1650/2000], Loss: 0.8945\n","Epoch [1652/2000], Loss: 0.8849\n","Epoch [1654/2000], Loss: 0.6930\n","Epoch [1656/2000], Loss: 0.7577\n","Epoch [1658/2000], Loss: 1.0110\n","Epoch [1660/2000], Loss: 0.6922\n","Epoch [1662/2000], Loss: 0.8369\n","Epoch [1664/2000], Loss: 0.7831\n","Epoch [1666/2000], Loss: 0.6578\n","Epoch [1668/2000], Loss: 1.0157\n","Epoch [1670/2000], Loss: 0.6256\n","Epoch [1672/2000], Loss: 0.8261\n","Epoch [1674/2000], Loss: 0.6273\n","Epoch [1676/2000], Loss: 0.6986\n","Epoch [1678/2000], Loss: 1.0658\n","Epoch [1680/2000], Loss: 0.7644\n","Epoch [1682/2000], Loss: 0.6924\n","Epoch [1684/2000], Loss: 0.8017\n","Epoch [1686/2000], Loss: 1.1511\n","Epoch [1688/2000], Loss: 0.8932\n","Epoch [1690/2000], Loss: 0.6554\n","Epoch [1692/2000], Loss: 0.9086\n","Epoch [1694/2000], Loss: 0.8201\n","Epoch [1696/2000], Loss: 0.6517\n","Epoch [1698/2000], Loss: 0.8773\n","Epoch [1700/2000], Loss: 0.6558\n","Epoch [1702/2000], Loss: 0.8848\n","Epoch [1704/2000], Loss: 0.8465\n","Epoch [1706/2000], Loss: 1.2520\n","Epoch [1708/2000], Loss: 1.0700\n","Epoch [1710/2000], Loss: 0.9363\n","Epoch [1712/2000], Loss: 0.6541\n","Epoch [1714/2000], Loss: 0.8119\n","Epoch [1716/2000], Loss: 0.5898\n","Epoch [1718/2000], Loss: 0.8986\n","Epoch [1720/2000], Loss: 0.7996\n","Epoch [1722/2000], Loss: 0.8431\n","Epoch [1724/2000], Loss: 0.7504\n","Epoch [1726/2000], Loss: 0.7430\n","Epoch [1728/2000], Loss: 0.8874\n","Epoch [1730/2000], Loss: 0.7794\n","Epoch [1732/2000], Loss: 0.8489\n","Epoch [1734/2000], Loss: 0.9531\n","Epoch [1736/2000], Loss: 0.6786\n","Epoch [1738/2000], Loss: 0.8173\n","Epoch [1740/2000], Loss: 0.5071\n","Epoch [1742/2000], Loss: 0.8631\n","Epoch [1744/2000], Loss: 0.7737\n","Epoch [1746/2000], Loss: 0.7168\n","Epoch [1748/2000], Loss: 0.7844\n","Epoch [1750/2000], Loss: 0.9120\n","Epoch [1752/2000], Loss: 0.7524\n","Epoch [1754/2000], Loss: 0.9551\n","Epoch [1756/2000], Loss: 1.1709\n","Epoch [1758/2000], Loss: 0.9168\n","Epoch [1760/2000], Loss: 0.6378\n","Epoch [1762/2000], Loss: 0.9334\n","Epoch [1764/2000], Loss: 0.7691\n","Epoch [1766/2000], Loss: 0.8998\n","Epoch [1768/2000], Loss: 0.7874\n","Epoch [1770/2000], Loss: 0.7181\n","Epoch [1772/2000], Loss: 0.8935\n","Epoch [1774/2000], Loss: 0.8887\n","Epoch [1776/2000], Loss: 0.8448\n","Epoch [1778/2000], Loss: 0.8476\n","Epoch [1780/2000], Loss: 0.9361\n","Epoch [1782/2000], Loss: 1.0787\n","Epoch [1784/2000], Loss: 0.9602\n","Epoch [1786/2000], Loss: 0.8783\n","Epoch [1788/2000], Loss: 0.8323\n","Epoch [1790/2000], Loss: 0.9798\n","Epoch [1792/2000], Loss: 0.7196\n","Epoch [1794/2000], Loss: 0.8910\n","Epoch [1796/2000], Loss: 0.7154\n","Epoch [1798/2000], Loss: 0.8671\n","Epoch [1800/2000], Loss: 0.7176\n","Epoch [1802/2000], Loss: 0.7003\n","Epoch [1804/2000], Loss: 0.7363\n","Epoch [1806/2000], Loss: 0.7986\n","Epoch [1808/2000], Loss: 0.6687\n","Epoch [1810/2000], Loss: 0.8540\n","Epoch [1812/2000], Loss: 1.0126\n","Epoch [1814/2000], Loss: 0.6099\n","Epoch [1816/2000], Loss: 0.7214\n","Epoch [1818/2000], Loss: 0.8639\n","Epoch [1820/2000], Loss: 0.8525\n","Epoch [1822/2000], Loss: 0.9630\n","Epoch [1824/2000], Loss: 0.6354\n","Epoch [1826/2000], Loss: 1.1737\n","Epoch [1828/2000], Loss: 0.9490\n","Epoch [1830/2000], Loss: 1.0235\n","Epoch [1832/2000], Loss: 0.4728\n","Epoch [1834/2000], Loss: 0.6399\n","Epoch [1836/2000], Loss: 0.6338\n","Epoch [1838/2000], Loss: 1.0217\n","Epoch [1840/2000], Loss: 0.9914\n","Epoch [1842/2000], Loss: 0.8425\n","Epoch [1844/2000], Loss: 0.7690\n","Epoch [1846/2000], Loss: 0.7321\n","Epoch [1848/2000], Loss: 0.8072\n","Epoch [1850/2000], Loss: 0.7490\n","Epoch [1852/2000], Loss: 0.8455\n","Epoch [1854/2000], Loss: 0.8135\n","Epoch [1856/2000], Loss: 0.7707\n","Epoch [1858/2000], Loss: 1.1561\n","Epoch [1860/2000], Loss: 0.8048\n","Epoch [1862/2000], Loss: 0.9029\n","Epoch [1864/2000], Loss: 0.9714\n","Epoch [1866/2000], Loss: 0.8260\n","Epoch [1868/2000], Loss: 0.7710\n","Epoch [1870/2000], Loss: 0.7937\n","Epoch [1872/2000], Loss: 0.6731\n","Epoch [1874/2000], Loss: 1.0146\n","Epoch [1876/2000], Loss: 0.5201\n","Epoch [1878/2000], Loss: 0.8617\n","Epoch [1880/2000], Loss: 1.2011\n","Epoch [1882/2000], Loss: 0.7892\n","Epoch [1884/2000], Loss: 0.9598\n","Epoch [1886/2000], Loss: 0.6037\n","Epoch [1888/2000], Loss: 0.8183\n","Epoch [1890/2000], Loss: 0.8599\n","Epoch [1892/2000], Loss: 0.6706\n","Epoch [1894/2000], Loss: 0.8074\n","Epoch [1896/2000], Loss: 0.9683\n","Epoch [1898/2000], Loss: 0.9553\n","Epoch [1900/2000], Loss: 1.0695\n","Epoch [1902/2000], Loss: 0.8109\n","Epoch [1904/2000], Loss: 0.9318\n","Epoch [1906/2000], Loss: 0.7567\n","Epoch [1908/2000], Loss: 0.6290\n","Epoch [1910/2000], Loss: 0.9652\n","Epoch [1912/2000], Loss: 0.5774\n","Epoch [1914/2000], Loss: 0.9215\n","Epoch [1916/2000], Loss: 0.8230\n","Epoch [1918/2000], Loss: 0.7263\n","Epoch [1920/2000], Loss: 0.7262\n","Epoch [1922/2000], Loss: 0.7237\n","Epoch [1924/2000], Loss: 0.7186\n","Epoch [1926/2000], Loss: 0.7997\n","Epoch [1928/2000], Loss: 0.6039\n","Epoch [1930/2000], Loss: 0.8761\n","Epoch [1932/2000], Loss: 0.9761\n","Epoch [1934/2000], Loss: 0.6433\n","Epoch [1936/2000], Loss: 0.8592\n","Epoch [1938/2000], Loss: 0.7048\n","Epoch [1940/2000], Loss: 0.7239\n","Epoch [1942/2000], Loss: 0.8910\n","Epoch [1944/2000], Loss: 0.9109\n","Epoch [1946/2000], Loss: 0.9720\n","Epoch [1948/2000], Loss: 0.7353\n","Epoch [1950/2000], Loss: 0.9893\n","Epoch [1952/2000], Loss: 0.8815\n","Epoch [1954/2000], Loss: 0.8215\n","Epoch [1956/2000], Loss: 0.6575\n","Epoch [1958/2000], Loss: 1.0951\n","Epoch [1960/2000], Loss: 0.8106\n","Epoch [1962/2000], Loss: 0.8478\n","Epoch [1964/2000], Loss: 0.7945\n","Epoch [1966/2000], Loss: 0.7908\n","Epoch [1968/2000], Loss: 0.9325\n","Epoch [1970/2000], Loss: 0.8216\n","Epoch [1972/2000], Loss: 0.6361\n","Epoch [1974/2000], Loss: 0.6741\n","Epoch [1976/2000], Loss: 0.9001\n","Epoch [1978/2000], Loss: 0.7734\n","Epoch [1980/2000], Loss: 0.9066\n","Epoch [1982/2000], Loss: 0.7817\n","Epoch [1984/2000], Loss: 0.8238\n","Epoch [1986/2000], Loss: 0.5339\n","Epoch [1988/2000], Loss: 0.9445\n","Epoch [1990/2000], Loss: 1.0261\n","Epoch [1992/2000], Loss: 0.8432\n","Epoch [1994/2000], Loss: 0.7182\n","Epoch [1996/2000], Loss: 0.9676\n","Epoch [1998/2000], Loss: 0.9316\n","Epoch [2000/2000], Loss: 0.7192\n","Epoch [2/2000], Loss: 1.0850\n","Epoch [4/2000], Loss: 1.1259\n","Epoch [6/2000], Loss: 1.0460\n","Epoch [8/2000], Loss: 0.8960\n","Epoch [10/2000], Loss: 0.9346\n","Epoch [12/2000], Loss: 1.2234\n","Epoch [14/2000], Loss: 1.1823\n","Epoch [16/2000], Loss: 0.9915\n","Epoch [18/2000], Loss: 0.8952\n","Epoch [20/2000], Loss: 1.2037\n","Epoch [22/2000], Loss: 1.1495\n","Epoch [24/2000], Loss: 1.0356\n","Epoch [26/2000], Loss: 0.9886\n","Epoch [28/2000], Loss: 0.9599\n","Epoch [30/2000], Loss: 1.1237\n","Epoch [32/2000], Loss: 0.8980\n","Epoch [34/2000], Loss: 1.0722\n","Epoch [36/2000], Loss: 1.2213\n","Epoch [38/2000], Loss: 0.9252\n","Epoch [40/2000], Loss: 1.2335\n","Epoch [42/2000], Loss: 0.9022\n","Epoch [44/2000], Loss: 0.9927\n","Epoch [46/2000], Loss: 1.1837\n","patience exceeded, loading best model\n","Epoch [2/2000], Loss: 1.0001\n","Epoch [4/2000], Loss: 0.9691\n","Epoch [6/2000], Loss: 0.9797\n","Epoch [8/2000], Loss: 1.0069\n","Epoch [10/2000], Loss: 1.0284\n","Epoch [12/2000], Loss: 0.9264\n","Epoch [14/2000], Loss: 1.1876\n","Epoch [16/2000], Loss: 1.0993\n","Epoch [18/2000], Loss: 1.1835\n","Epoch [20/2000], Loss: 1.1283\n","Epoch [22/2000], Loss: 1.1910\n","Epoch [24/2000], Loss: 0.8490\n","Epoch [26/2000], Loss: 1.2397\n","Epoch [28/2000], Loss: 0.8948\n","Epoch [30/2000], Loss: 1.1631\n","Epoch [32/2000], Loss: 0.8271\n","Epoch [34/2000], Loss: 1.0527\n","Epoch [36/2000], Loss: 1.2544\n","Epoch [38/2000], Loss: 0.8503\n","Epoch [40/2000], Loss: 0.8900\n","Epoch [42/2000], Loss: 1.1947\n","patience exceeded, loading best model\n","Epoch [2/2000], Loss: 1.2254\n","Epoch [4/2000], Loss: 1.0468\n","Epoch [6/2000], Loss: 1.1524\n","Epoch [8/2000], Loss: 1.1250\n","Epoch [10/2000], Loss: 1.1609\n","Epoch [12/2000], Loss: 1.1518\n","Epoch [14/2000], Loss: 0.9559\n","Epoch [16/2000], Loss: 1.0085\n","Epoch [18/2000], Loss: 1.0435\n","Epoch [20/2000], Loss: 1.1445\n","Epoch [22/2000], Loss: 1.0483\n","Epoch [24/2000], Loss: 1.0882\n","Epoch [26/2000], Loss: 1.1179\n","Epoch [28/2000], Loss: 1.2501\n","Epoch [30/2000], Loss: 0.7259\n","Epoch [32/2000], Loss: 1.0948\n","Epoch [34/2000], Loss: 0.9172\n","Epoch [36/2000], Loss: 0.9534\n","Epoch [38/2000], Loss: 1.0222\n","Epoch [40/2000], Loss: 1.2828\n","Epoch [42/2000], Loss: 0.9582\n","Epoch [44/2000], Loss: 1.2190\n","Epoch [46/2000], Loss: 1.0122\n","Epoch [48/2000], Loss: 1.0564\n","Epoch [50/2000], Loss: 1.1569\n","Epoch [52/2000], Loss: 0.9022\n","Epoch [54/2000], Loss: 1.2114\n","Epoch [56/2000], Loss: 0.8703\n","Epoch [58/2000], Loss: 0.7917\n","Epoch [60/2000], Loss: 0.8886\n","Epoch [62/2000], Loss: 0.7998\n","Epoch [64/2000], Loss: 0.6818\n","Epoch [66/2000], Loss: 0.9725\n","Epoch [68/2000], Loss: 1.0192\n","Epoch [70/2000], Loss: 1.2352\n","Epoch [72/2000], Loss: 0.9431\n","Epoch [74/2000], Loss: 0.8512\n","Epoch [76/2000], Loss: 1.0690\n","Epoch [78/2000], Loss: 1.0494\n","Epoch [80/2000], Loss: 0.9684\n","Epoch [82/2000], Loss: 0.9572\n","Epoch [84/2000], Loss: 1.2830\n","Epoch [86/2000], Loss: 1.1842\n","Epoch [88/2000], Loss: 0.8312\n","Epoch [90/2000], Loss: 0.9833\n","Epoch [92/2000], Loss: 0.9237\n","Epoch [94/2000], Loss: 1.0945\n","Epoch [96/2000], Loss: 1.1943\n","Epoch [98/2000], Loss: 0.8460\n","Epoch [100/2000], Loss: 0.8363\n","Epoch [102/2000], Loss: 1.1382\n","Epoch [104/2000], Loss: 1.0094\n","Epoch [106/2000], Loss: 1.1006\n","Epoch [108/2000], Loss: 0.9376\n","Epoch [110/2000], Loss: 0.7294\n","Epoch [112/2000], Loss: 0.9222\n","Epoch [114/2000], Loss: 1.0623\n","Epoch [116/2000], Loss: 1.0077\n","Epoch [118/2000], Loss: 1.1234\n","Epoch [120/2000], Loss: 0.7585\n","Epoch [122/2000], Loss: 0.6944\n","Epoch [124/2000], Loss: 0.8623\n","Epoch [126/2000], Loss: 0.9446\n","Epoch [128/2000], Loss: 0.9799\n","Epoch [130/2000], Loss: 0.9057\n","Epoch [132/2000], Loss: 1.0635\n","Epoch [134/2000], Loss: 1.1650\n","Epoch [136/2000], Loss: 1.0051\n","Epoch [138/2000], Loss: 1.0907\n","Epoch [140/2000], Loss: 0.9875\n","Epoch [142/2000], Loss: 1.1756\n","Epoch [144/2000], Loss: 0.8389\n","Epoch [146/2000], Loss: 1.1319\n","Epoch [148/2000], Loss: 0.8059\n","Epoch [150/2000], Loss: 0.8828\n","Epoch [152/2000], Loss: 0.9535\n","Epoch [154/2000], Loss: 0.9980\n","Epoch [156/2000], Loss: 0.7517\n","Epoch [158/2000], Loss: 1.0195\n","Epoch [160/2000], Loss: 0.9569\n","Epoch [162/2000], Loss: 1.0925\n","Epoch [164/2000], Loss: 0.6932\n","Epoch [166/2000], Loss: 1.0547\n","Epoch [168/2000], Loss: 1.2932\n","Epoch [170/2000], Loss: 1.0994\n","Epoch [172/2000], Loss: 0.9359\n","Epoch [174/2000], Loss: 1.1406\n","Epoch [176/2000], Loss: 1.1356\n","Epoch [178/2000], Loss: 0.9215\n","Epoch [180/2000], Loss: 1.1536\n","Epoch [182/2000], Loss: 0.8552\n","Epoch [184/2000], Loss: 1.0382\n","Epoch [186/2000], Loss: 1.2595\n","Epoch [188/2000], Loss: 1.0823\n","Epoch [190/2000], Loss: 0.6653\n","Epoch [192/2000], Loss: 0.9463\n","Epoch [194/2000], Loss: 0.8515\n","Epoch [196/2000], Loss: 0.9439\n","Epoch [198/2000], Loss: 1.1164\n","Epoch [200/2000], Loss: 1.0441\n","Epoch [202/2000], Loss: 1.2403\n","Epoch [204/2000], Loss: 1.0929\n","Epoch [206/2000], Loss: 1.2199\n","Epoch [208/2000], Loss: 0.8794\n","Epoch [210/2000], Loss: 1.1247\n","Epoch [212/2000], Loss: 1.2551\n","Epoch [214/2000], Loss: 0.8581\n","Epoch [216/2000], Loss: 1.0152\n","Epoch [218/2000], Loss: 1.2604\n","Epoch [220/2000], Loss: 1.0161\n","Epoch [222/2000], Loss: 0.8340\n","Epoch [224/2000], Loss: 0.6237\n","Epoch [226/2000], Loss: 0.9657\n","Epoch [228/2000], Loss: 1.0987\n","Epoch [230/2000], Loss: 1.1919\n","Epoch [232/2000], Loss: 1.3619\n","Epoch [234/2000], Loss: 1.0679\n","Epoch [236/2000], Loss: 0.9974\n","Epoch [238/2000], Loss: 0.9030\n","Epoch [240/2000], Loss: 1.1206\n","Epoch [242/2000], Loss: 0.7921\n","Epoch [244/2000], Loss: 0.8352\n","Epoch [246/2000], Loss: 0.9819\n","Epoch [248/2000], Loss: 1.0604\n","Epoch [250/2000], Loss: 0.9972\n","Epoch [252/2000], Loss: 1.0692\n","Epoch [254/2000], Loss: 1.2064\n","Epoch [256/2000], Loss: 1.0004\n","Epoch [258/2000], Loss: 0.8614\n","Epoch [260/2000], Loss: 1.0195\n","Epoch [262/2000], Loss: 0.8342\n","Epoch [264/2000], Loss: 0.8120\n","Epoch [266/2000], Loss: 1.1601\n","Epoch [268/2000], Loss: 1.2864\n","Epoch [270/2000], Loss: 1.0767\n","Epoch [272/2000], Loss: 1.3381\n","Epoch [274/2000], Loss: 1.1784\n","Epoch [276/2000], Loss: 0.8999\n","Epoch [278/2000], Loss: 0.9617\n","Epoch [280/2000], Loss: 1.0791\n","Epoch [282/2000], Loss: 0.9404\n","Epoch [284/2000], Loss: 1.1720\n","Epoch [286/2000], Loss: 1.2902\n","Epoch [288/2000], Loss: 0.9967\n","Epoch [290/2000], Loss: 1.0354\n","Epoch [292/2000], Loss: 1.1282\n","Epoch [294/2000], Loss: 0.8986\n","Epoch [296/2000], Loss: 1.0986\n","Epoch [298/2000], Loss: 1.1859\n","Epoch [300/2000], Loss: 0.7264\n","Epoch [302/2000], Loss: 1.0248\n","Epoch [304/2000], Loss: 1.0164\n","Epoch [306/2000], Loss: 1.1264\n","Epoch [308/2000], Loss: 0.7310\n","Epoch [310/2000], Loss: 0.7655\n","Epoch [312/2000], Loss: 0.8154\n","Epoch [314/2000], Loss: 1.1818\n","Epoch [316/2000], Loss: 1.2255\n","Epoch [318/2000], Loss: 1.1401\n","Epoch [320/2000], Loss: 1.1121\n","Epoch [322/2000], Loss: 1.0657\n","Epoch [324/2000], Loss: 1.2133\n","Epoch [326/2000], Loss: 0.9574\n","Epoch [328/2000], Loss: 0.9167\n","Epoch [330/2000], Loss: 1.1603\n","Epoch [332/2000], Loss: 1.1313\n","Epoch [334/2000], Loss: 1.1244\n","Epoch [336/2000], Loss: 1.0308\n","Epoch [338/2000], Loss: 0.8266\n","Epoch [340/2000], Loss: 1.2119\n","Epoch [342/2000], Loss: 1.1924\n","Epoch [344/2000], Loss: 1.2412\n","Epoch [346/2000], Loss: 0.9518\n","Epoch [348/2000], Loss: 1.0840\n","Epoch [350/2000], Loss: 0.9537\n","Epoch [352/2000], Loss: 1.3306\n","Epoch [354/2000], Loss: 1.1779\n","Epoch [356/2000], Loss: 1.1018\n","Epoch [358/2000], Loss: 1.0362\n","Epoch [360/2000], Loss: 1.3878\n","Epoch [362/2000], Loss: 1.1932\n","Epoch [364/2000], Loss: 0.7975\n","Epoch [366/2000], Loss: 0.9240\n","Epoch [368/2000], Loss: 1.1281\n","Epoch [370/2000], Loss: 0.8511\n","Epoch [372/2000], Loss: 0.7221\n","Epoch [374/2000], Loss: 0.9036\n","Epoch [376/2000], Loss: 1.0711\n","Epoch [378/2000], Loss: 0.9936\n","Epoch [380/2000], Loss: 1.2166\n","Epoch [382/2000], Loss: 1.0419\n","Epoch [384/2000], Loss: 0.6418\n","Epoch [386/2000], Loss: 0.9064\n","Epoch [388/2000], Loss: 0.8480\n","Epoch [390/2000], Loss: 0.9169\n","Epoch [392/2000], Loss: 1.1222\n","Epoch [394/2000], Loss: 1.0446\n","Epoch [396/2000], Loss: 1.2512\n","Epoch [398/2000], Loss: 0.8266\n","Epoch [400/2000], Loss: 1.0543\n","Epoch [402/2000], Loss: 1.1088\n","Epoch [404/2000], Loss: 0.8067\n","Epoch [406/2000], Loss: 0.9854\n","Epoch [408/2000], Loss: 1.0295\n","Epoch [410/2000], Loss: 0.9107\n","Epoch [412/2000], Loss: 0.9655\n","Epoch [414/2000], Loss: 1.0685\n","Epoch [416/2000], Loss: 1.0393\n","Epoch [418/2000], Loss: 1.0533\n","Epoch [420/2000], Loss: 1.0749\n","Epoch [422/2000], Loss: 1.0376\n","Epoch [424/2000], Loss: 0.8464\n","Epoch [426/2000], Loss: 0.6863\n","Epoch [428/2000], Loss: 1.0610\n","Epoch [430/2000], Loss: 0.9915\n","Epoch [432/2000], Loss: 0.9165\n","Epoch [434/2000], Loss: 0.9887\n","Epoch [436/2000], Loss: 1.1048\n","Epoch [438/2000], Loss: 0.8294\n","Epoch [440/2000], Loss: 0.9957\n","Epoch [442/2000], Loss: 1.0494\n","Epoch [444/2000], Loss: 0.9656\n","Epoch [446/2000], Loss: 0.9294\n","Epoch [448/2000], Loss: 0.9344\n","Epoch [450/2000], Loss: 1.0310\n","Epoch [452/2000], Loss: 1.0606\n","Epoch [454/2000], Loss: 0.6483\n","Epoch [456/2000], Loss: 1.0604\n","Epoch [458/2000], Loss: 1.0972\n","Epoch [460/2000], Loss: 0.9347\n","Epoch [462/2000], Loss: 0.9381\n","Epoch [464/2000], Loss: 0.7728\n","Epoch [466/2000], Loss: 1.1815\n","Epoch [468/2000], Loss: 0.9163\n","Epoch [470/2000], Loss: 1.1980\n","Epoch [472/2000], Loss: 0.9950\n","Epoch [474/2000], Loss: 0.8856\n","Epoch [476/2000], Loss: 1.2247\n","Epoch [478/2000], Loss: 1.2135\n","Epoch [480/2000], Loss: 1.0092\n","Epoch [482/2000], Loss: 1.0719\n","Epoch [484/2000], Loss: 1.0284\n","Epoch [486/2000], Loss: 0.9780\n","Epoch [488/2000], Loss: 1.0173\n","Epoch [490/2000], Loss: 1.0845\n","Epoch [492/2000], Loss: 1.3503\n","Epoch [494/2000], Loss: 1.1346\n","Epoch [496/2000], Loss: 1.2462\n","Epoch [498/2000], Loss: 1.0000\n","Epoch [500/2000], Loss: 1.0742\n","Epoch [502/2000], Loss: 1.1520\n","Epoch [504/2000], Loss: 0.8447\n","Epoch [506/2000], Loss: 0.7246\n","Epoch [508/2000], Loss: 0.9552\n","Epoch [510/2000], Loss: 1.1375\n","Epoch [512/2000], Loss: 0.8277\n","Epoch [514/2000], Loss: 0.9460\n","Epoch [516/2000], Loss: 1.2507\n","Epoch [518/2000], Loss: 1.0706\n","Epoch [520/2000], Loss: 1.2508\n","Epoch [522/2000], Loss: 1.1192\n","Epoch [524/2000], Loss: 1.1497\n","Epoch [526/2000], Loss: 0.8529\n","Epoch [528/2000], Loss: 0.9168\n","Epoch [530/2000], Loss: 0.9060\n","Epoch [532/2000], Loss: 0.7290\n","Epoch [534/2000], Loss: 0.8338\n","Epoch [536/2000], Loss: 0.9930\n","Epoch [538/2000], Loss: 0.8888\n","Epoch [540/2000], Loss: 0.7958\n","Epoch [542/2000], Loss: 0.7699\n","Epoch [544/2000], Loss: 1.0413\n","Epoch [546/2000], Loss: 0.8212\n","Epoch [548/2000], Loss: 1.0723\n","Epoch [550/2000], Loss: 1.0247\n","Epoch [552/2000], Loss: 1.0981\n","Epoch [554/2000], Loss: 1.1678\n","Epoch [556/2000], Loss: 0.7992\n","Epoch [558/2000], Loss: 0.9810\n","Epoch [560/2000], Loss: 0.9837\n","Epoch [562/2000], Loss: 1.0738\n","Epoch [564/2000], Loss: 0.7656\n","Epoch [566/2000], Loss: 1.0253\n","Epoch [568/2000], Loss: 0.9794\n","Epoch [570/2000], Loss: 1.0004\n","Epoch [572/2000], Loss: 1.1482\n","Epoch [574/2000], Loss: 1.1754\n","Epoch [576/2000], Loss: 1.0527\n","Epoch [578/2000], Loss: 0.8949\n","Epoch [580/2000], Loss: 0.8781\n","Epoch [582/2000], Loss: 0.7777\n","Epoch [584/2000], Loss: 1.0427\n","Epoch [586/2000], Loss: 0.9423\n","Epoch [588/2000], Loss: 0.9766\n","Epoch [590/2000], Loss: 1.2810\n","Epoch [592/2000], Loss: 1.0429\n","Epoch [594/2000], Loss: 0.8830\n","Epoch [596/2000], Loss: 0.9191\n","Epoch [598/2000], Loss: 0.9735\n","Epoch [600/2000], Loss: 1.0530\n","Epoch [602/2000], Loss: 0.9742\n","Epoch [604/2000], Loss: 0.9676\n","Epoch [606/2000], Loss: 0.9819\n","Epoch [608/2000], Loss: 0.8940\n","Epoch [610/2000], Loss: 0.8420\n","Epoch [612/2000], Loss: 0.8936\n","Epoch [614/2000], Loss: 1.0268\n","Epoch [616/2000], Loss: 1.0450\n","Epoch [618/2000], Loss: 1.0412\n","Epoch [620/2000], Loss: 1.1380\n","Epoch [622/2000], Loss: 0.7629\n","Epoch [624/2000], Loss: 0.6400\n","Epoch [626/2000], Loss: 0.9844\n","Epoch [628/2000], Loss: 1.2324\n","Epoch [630/2000], Loss: 1.0237\n","Epoch [632/2000], Loss: 0.9822\n","Epoch [634/2000], Loss: 0.9438\n","Epoch [636/2000], Loss: 0.9164\n","Epoch [638/2000], Loss: 0.9243\n","Epoch [640/2000], Loss: 0.8710\n","Epoch [642/2000], Loss: 1.2322\n","Epoch [644/2000], Loss: 1.3231\n","Epoch [646/2000], Loss: 0.8789\n","Epoch [648/2000], Loss: 1.0793\n","Epoch [650/2000], Loss: 0.9582\n","Epoch [652/2000], Loss: 0.9457\n","Epoch [654/2000], Loss: 0.9700\n","Epoch [656/2000], Loss: 0.9537\n","Epoch [658/2000], Loss: 1.0848\n","Epoch [660/2000], Loss: 0.8515\n","Epoch [662/2000], Loss: 1.0896\n","Epoch [664/2000], Loss: 1.0574\n","Epoch [666/2000], Loss: 1.0398\n","Epoch [668/2000], Loss: 0.9350\n","Epoch [670/2000], Loss: 0.9235\n","Epoch [672/2000], Loss: 1.0214\n","Epoch [674/2000], Loss: 0.8868\n","Epoch [676/2000], Loss: 0.9385\n","Epoch [678/2000], Loss: 1.1427\n","Epoch [680/2000], Loss: 0.6606\n","Epoch [682/2000], Loss: 0.8116\n","Epoch [684/2000], Loss: 0.7449\n","Epoch [686/2000], Loss: 1.0120\n","Epoch [688/2000], Loss: 0.8093\n","Epoch [690/2000], Loss: 0.7598\n","Epoch [692/2000], Loss: 1.2660\n","Epoch [694/2000], Loss: 1.0538\n","Epoch [696/2000], Loss: 0.7235\n","Epoch [698/2000], Loss: 0.8929\n","Epoch [700/2000], Loss: 0.7510\n","Epoch [702/2000], Loss: 1.0431\n","Epoch [704/2000], Loss: 1.2463\n","Epoch [706/2000], Loss: 0.7072\n","Epoch [708/2000], Loss: 0.8948\n","Epoch [710/2000], Loss: 0.9092\n","Epoch [712/2000], Loss: 0.9290\n","Epoch [714/2000], Loss: 0.8364\n","Epoch [716/2000], Loss: 1.0328\n","Epoch [718/2000], Loss: 0.8302\n","Epoch [720/2000], Loss: 1.1134\n","Epoch [722/2000], Loss: 0.9565\n","Epoch [724/2000], Loss: 0.8549\n","Epoch [726/2000], Loss: 1.1020\n","Epoch [728/2000], Loss: 0.9210\n","Epoch [730/2000], Loss: 0.9970\n","Epoch [732/2000], Loss: 1.0320\n","Epoch [734/2000], Loss: 0.9082\n","Epoch [736/2000], Loss: 1.1178\n","Epoch [738/2000], Loss: 1.1735\n","Epoch [740/2000], Loss: 0.8919\n","Epoch [742/2000], Loss: 0.7881\n","Epoch [744/2000], Loss: 0.9352\n","Epoch [746/2000], Loss: 0.8461\n","Epoch [748/2000], Loss: 0.8681\n","Epoch [750/2000], Loss: 1.2770\n","Epoch [752/2000], Loss: 0.9469\n","Epoch [754/2000], Loss: 0.8589\n","Epoch [756/2000], Loss: 0.9376\n","Epoch [758/2000], Loss: 1.0836\n","Epoch [760/2000], Loss: 1.3900\n","Epoch [762/2000], Loss: 1.0163\n","Epoch [764/2000], Loss: 1.2169\n","Epoch [766/2000], Loss: 1.0599\n","Epoch [768/2000], Loss: 0.8348\n","Epoch [770/2000], Loss: 0.9065\n","Epoch [772/2000], Loss: 0.9645\n","Epoch [774/2000], Loss: 1.2353\n","Epoch [776/2000], Loss: 0.9948\n","Epoch [778/2000], Loss: 1.1321\n","Epoch [780/2000], Loss: 1.1422\n","Epoch [782/2000], Loss: 1.0548\n","Epoch [784/2000], Loss: 0.9553\n","Epoch [786/2000], Loss: 0.9203\n","Epoch [788/2000], Loss: 0.9671\n","Epoch [790/2000], Loss: 0.6823\n","Epoch [792/2000], Loss: 1.1276\n","Epoch [794/2000], Loss: 1.1729\n","Epoch [796/2000], Loss: 1.1799\n","Epoch [798/2000], Loss: 0.9546\n","Epoch [800/2000], Loss: 0.8342\n","Epoch [802/2000], Loss: 1.0243\n","Epoch [804/2000], Loss: 0.7977\n","Epoch [806/2000], Loss: 1.0974\n","Epoch [808/2000], Loss: 0.8536\n","Epoch [810/2000], Loss: 1.1966\n","Epoch [812/2000], Loss: 0.8144\n","Epoch [814/2000], Loss: 1.0651\n","Epoch [816/2000], Loss: 0.9497\n","Epoch [818/2000], Loss: 0.8380\n","Epoch [820/2000], Loss: 0.9180\n","Epoch [822/2000], Loss: 0.6892\n","Epoch [824/2000], Loss: 1.0728\n","Epoch [826/2000], Loss: 0.9777\n","Epoch [828/2000], Loss: 1.3916\n","Epoch [830/2000], Loss: 0.8754\n","Epoch [832/2000], Loss: 0.8128\n","Epoch [834/2000], Loss: 0.9588\n","Epoch [836/2000], Loss: 1.1141\n","Epoch [838/2000], Loss: 1.0908\n","Epoch [840/2000], Loss: 0.8944\n","Epoch [842/2000], Loss: 0.9884\n","Epoch [844/2000], Loss: 0.9930\n","Epoch [846/2000], Loss: 1.0529\n","Epoch [848/2000], Loss: 1.1010\n","Epoch [850/2000], Loss: 1.1407\n","Epoch [852/2000], Loss: 0.8053\n","Epoch [854/2000], Loss: 1.0833\n","Epoch [856/2000], Loss: 1.2072\n","Epoch [858/2000], Loss: 1.1512\n","Epoch [860/2000], Loss: 0.9747\n","Epoch [862/2000], Loss: 0.8734\n","Epoch [864/2000], Loss: 0.7990\n","Epoch [866/2000], Loss: 0.8528\n","Epoch [868/2000], Loss: 0.9558\n","Epoch [870/2000], Loss: 0.8459\n","Epoch [872/2000], Loss: 1.0773\n","Epoch [874/2000], Loss: 0.7806\n","Epoch [876/2000], Loss: 0.6538\n","Epoch [878/2000], Loss: 0.9438\n","Epoch [880/2000], Loss: 1.0835\n","Epoch [882/2000], Loss: 0.5999\n","Epoch [884/2000], Loss: 1.0436\n","Epoch [886/2000], Loss: 0.9063\n","Epoch [888/2000], Loss: 0.7797\n","Epoch [890/2000], Loss: 1.0662\n","Epoch [892/2000], Loss: 1.0021\n","Epoch [894/2000], Loss: 1.0226\n","Epoch [896/2000], Loss: 1.0091\n","Epoch [898/2000], Loss: 1.2489\n","Epoch [900/2000], Loss: 1.0282\n","Epoch [902/2000], Loss: 1.1132\n","Epoch [904/2000], Loss: 0.8829\n","Epoch [906/2000], Loss: 0.9835\n","Epoch [908/2000], Loss: 0.9546\n","Epoch [910/2000], Loss: 0.9045\n","Epoch [912/2000], Loss: 0.8776\n","Epoch [914/2000], Loss: 1.0421\n","Epoch [916/2000], Loss: 1.0506\n","Epoch [918/2000], Loss: 1.0843\n","Epoch [920/2000], Loss: 0.9163\n","Epoch [922/2000], Loss: 0.9102\n","Epoch [924/2000], Loss: 0.8891\n","Epoch [926/2000], Loss: 1.0766\n","Epoch [928/2000], Loss: 0.9041\n","Epoch [930/2000], Loss: 0.9890\n","Epoch [932/2000], Loss: 0.9663\n","Epoch [934/2000], Loss: 1.0573\n","Epoch [936/2000], Loss: 1.0733\n","Epoch [938/2000], Loss: 1.3088\n","Epoch [940/2000], Loss: 0.9223\n","Epoch [942/2000], Loss: 0.7840\n","Epoch [944/2000], Loss: 1.0023\n","Epoch [946/2000], Loss: 0.9614\n","Epoch [948/2000], Loss: 0.7811\n","Epoch [950/2000], Loss: 1.0250\n","Epoch [952/2000], Loss: 0.7865\n","Epoch [954/2000], Loss: 0.9546\n","Epoch [956/2000], Loss: 0.8917\n","Epoch [958/2000], Loss: 1.1862\n","Epoch [960/2000], Loss: 0.9944\n","Epoch [962/2000], Loss: 0.8988\n","Epoch [964/2000], Loss: 0.6851\n","Epoch [966/2000], Loss: 0.7697\n","Epoch [968/2000], Loss: 1.1084\n","Epoch [970/2000], Loss: 1.0341\n","Epoch [972/2000], Loss: 0.9296\n","Epoch [974/2000], Loss: 0.8511\n","Epoch [976/2000], Loss: 0.9502\n","Epoch [978/2000], Loss: 0.9260\n","Epoch [980/2000], Loss: 0.9170\n","Epoch [982/2000], Loss: 0.8021\n","Epoch [984/2000], Loss: 1.0203\n","Epoch [986/2000], Loss: 1.1587\n","Epoch [988/2000], Loss: 0.8998\n","Epoch [990/2000], Loss: 0.9731\n","Epoch [992/2000], Loss: 1.0103\n","Epoch [994/2000], Loss: 1.0733\n","Epoch [996/2000], Loss: 0.9954\n","Epoch [998/2000], Loss: 1.0328\n","Epoch [1000/2000], Loss: 0.8766\n","Epoch [1002/2000], Loss: 1.2261\n","Epoch [1004/2000], Loss: 0.8356\n","Epoch [1006/2000], Loss: 1.1093\n","Epoch [1008/2000], Loss: 0.9934\n","Epoch [1010/2000], Loss: 0.9285\n","Epoch [1012/2000], Loss: 1.1827\n","Epoch [1014/2000], Loss: 1.1707\n","Epoch [1016/2000], Loss: 0.9161\n","Epoch [1018/2000], Loss: 0.7548\n","Epoch [1020/2000], Loss: 0.8950\n","Epoch [1022/2000], Loss: 0.8334\n","Epoch [1024/2000], Loss: 0.7290\n","Epoch [1026/2000], Loss: 0.7684\n","Epoch [1028/2000], Loss: 0.9227\n","Epoch [1030/2000], Loss: 0.9474\n","Epoch [1032/2000], Loss: 0.7828\n","Epoch [1034/2000], Loss: 1.0079\n","Epoch [1036/2000], Loss: 1.1623\n","Epoch [1038/2000], Loss: 1.1279\n","Epoch [1040/2000], Loss: 0.9333\n","Epoch [1042/2000], Loss: 1.1418\n","Epoch [1044/2000], Loss: 0.9319\n","Epoch [1046/2000], Loss: 0.8280\n","Epoch [1048/2000], Loss: 0.9975\n","Epoch [1050/2000], Loss: 0.8372\n","Epoch [1052/2000], Loss: 1.0611\n","Epoch [1054/2000], Loss: 1.0194\n","Epoch [1056/2000], Loss: 0.8503\n","Epoch [1058/2000], Loss: 0.7814\n","Epoch [1060/2000], Loss: 0.9143\n","Epoch [1062/2000], Loss: 1.1186\n","Epoch [1064/2000], Loss: 0.9395\n","Epoch [1066/2000], Loss: 0.9833\n","Epoch [1068/2000], Loss: 0.8677\n","Epoch [1070/2000], Loss: 1.0552\n","Epoch [1072/2000], Loss: 0.6218\n","Epoch [1074/2000], Loss: 0.8487\n","Epoch [1076/2000], Loss: 0.8954\n","Epoch [1078/2000], Loss: 1.0543\n","Epoch [1080/2000], Loss: 1.0931\n","Epoch [1082/2000], Loss: 0.9547\n","Epoch [1084/2000], Loss: 0.8983\n","Epoch [1086/2000], Loss: 1.1367\n","Epoch [1088/2000], Loss: 1.0858\n","Epoch [1090/2000], Loss: 0.9775\n","Epoch [1092/2000], Loss: 1.3172\n","Epoch [1094/2000], Loss: 1.0212\n","Epoch [1096/2000], Loss: 0.8872\n","Epoch [1098/2000], Loss: 0.8386\n","Epoch [1100/2000], Loss: 1.1486\n","Epoch [1102/2000], Loss: 0.8933\n","Epoch [1104/2000], Loss: 1.0987\n","Epoch [1106/2000], Loss: 1.0162\n","Epoch [1108/2000], Loss: 0.9314\n","Epoch [1110/2000], Loss: 0.8703\n","Epoch [1112/2000], Loss: 0.8502\n","Epoch [1114/2000], Loss: 0.7210\n","Epoch [1116/2000], Loss: 1.0752\n","Epoch [1118/2000], Loss: 0.8049\n","Epoch [1120/2000], Loss: 0.9475\n","Epoch [1122/2000], Loss: 1.0995\n","Epoch [1124/2000], Loss: 0.8985\n","Epoch [1126/2000], Loss: 1.0543\n","Epoch [1128/2000], Loss: 0.9583\n","Epoch [1130/2000], Loss: 0.9455\n","Epoch [1132/2000], Loss: 0.9767\n","Epoch [1134/2000], Loss: 1.2092\n","Epoch [1136/2000], Loss: 1.1019\n","Epoch [1138/2000], Loss: 0.7625\n","Epoch [1140/2000], Loss: 0.8569\n","Epoch [1142/2000], Loss: 0.8540\n","Epoch [1144/2000], Loss: 0.7873\n","Epoch [1146/2000], Loss: 0.8706\n","Epoch [1148/2000], Loss: 0.9548\n","Epoch [1150/2000], Loss: 0.8326\n","Epoch [1152/2000], Loss: 0.7853\n","Epoch [1154/2000], Loss: 0.9797\n","Epoch [1156/2000], Loss: 0.9951\n","Epoch [1158/2000], Loss: 0.9871\n","Epoch [1160/2000], Loss: 0.6971\n","Epoch [1162/2000], Loss: 0.7881\n","Epoch [1164/2000], Loss: 0.8078\n","Epoch [1166/2000], Loss: 0.8723\n","Epoch [1168/2000], Loss: 0.9746\n","Epoch [1170/2000], Loss: 0.7185\n","Epoch [1172/2000], Loss: 0.9943\n","Epoch [1174/2000], Loss: 0.9448\n","Epoch [1176/2000], Loss: 1.0901\n","Epoch [1178/2000], Loss: 0.9480\n","Epoch [1180/2000], Loss: 0.7486\n","Epoch [1182/2000], Loss: 0.6987\n","Epoch [1184/2000], Loss: 0.9780\n","Epoch [1186/2000], Loss: 0.8047\n","Epoch [1188/2000], Loss: 0.8120\n","Epoch [1190/2000], Loss: 1.1367\n","Epoch [1192/2000], Loss: 1.0052\n","Epoch [1194/2000], Loss: 0.8765\n","Epoch [1196/2000], Loss: 0.8549\n","Epoch [1198/2000], Loss: 0.6496\n","Epoch [1200/2000], Loss: 0.9108\n","Epoch [1202/2000], Loss: 0.9546\n","Epoch [1204/2000], Loss: 0.9128\n","Epoch [1206/2000], Loss: 0.9164\n","Epoch [1208/2000], Loss: 1.0333\n","Epoch [1210/2000], Loss: 0.6708\n","Epoch [1212/2000], Loss: 0.9600\n","Epoch [1214/2000], Loss: 1.0575\n","Epoch [1216/2000], Loss: 1.1243\n","Epoch [1218/2000], Loss: 0.8956\n","Epoch [1220/2000], Loss: 0.8106\n","Epoch [1222/2000], Loss: 0.9830\n","Epoch [1224/2000], Loss: 0.7845\n","Epoch [1226/2000], Loss: 0.9428\n","Epoch [1228/2000], Loss: 0.8993\n","Epoch [1230/2000], Loss: 0.9791\n","Epoch [1232/2000], Loss: 0.8227\n","Epoch [1234/2000], Loss: 0.7356\n","Epoch [1236/2000], Loss: 0.7646\n","Epoch [1238/2000], Loss: 1.0187\n","Epoch [1240/2000], Loss: 0.9327\n","Epoch [1242/2000], Loss: 0.9443\n","Epoch [1244/2000], Loss: 0.8670\n","Epoch [1246/2000], Loss: 1.0698\n","Epoch [1248/2000], Loss: 0.9270\n","Epoch [1250/2000], Loss: 0.8522\n","Epoch [1252/2000], Loss: 0.8393\n","Epoch [1254/2000], Loss: 0.8295\n","Epoch [1256/2000], Loss: 0.9941\n","Epoch [1258/2000], Loss: 0.8683\n","Epoch [1260/2000], Loss: 0.8910\n","Epoch [1262/2000], Loss: 0.9206\n","Epoch [1264/2000], Loss: 0.9215\n","Epoch [1266/2000], Loss: 1.0461\n","Epoch [1268/2000], Loss: 0.8295\n","Epoch [1270/2000], Loss: 0.7303\n","Epoch [1272/2000], Loss: 0.7836\n","Epoch [1274/2000], Loss: 0.7855\n","Epoch [1276/2000], Loss: 0.9958\n","Epoch [1278/2000], Loss: 1.0600\n","Epoch [1280/2000], Loss: 0.8398\n","Epoch [1282/2000], Loss: 0.6057\n","Epoch [1284/2000], Loss: 1.0076\n","Epoch [1286/2000], Loss: 0.9366\n","Epoch [1288/2000], Loss: 0.8759\n","Epoch [1290/2000], Loss: 0.8607\n","Epoch [1292/2000], Loss: 0.6246\n","Epoch [1294/2000], Loss: 0.8370\n","Epoch [1296/2000], Loss: 0.6711\n","Epoch [1298/2000], Loss: 0.8399\n","Epoch [1300/2000], Loss: 0.9609\n","Epoch [1302/2000], Loss: 0.7296\n","Epoch [1304/2000], Loss: 1.1369\n","Epoch [1306/2000], Loss: 0.7240\n","Epoch [1308/2000], Loss: 0.8298\n","Epoch [1310/2000], Loss: 0.7443\n","Epoch [1312/2000], Loss: 0.7940\n","Epoch [1314/2000], Loss: 0.8937\n","Epoch [1316/2000], Loss: 1.0012\n","Epoch [1318/2000], Loss: 0.9971\n","Epoch [1320/2000], Loss: 0.9235\n","Epoch [1322/2000], Loss: 1.1562\n","Epoch [1324/2000], Loss: 0.7141\n","Epoch [1326/2000], Loss: 1.0553\n","Epoch [1328/2000], Loss: 1.0471\n","Epoch [1330/2000], Loss: 0.7364\n","Epoch [1332/2000], Loss: 1.0247\n","Epoch [1334/2000], Loss: 0.6050\n","Epoch [1336/2000], Loss: 1.0867\n","Epoch [1338/2000], Loss: 0.9331\n","Epoch [1340/2000], Loss: 1.0226\n","Epoch [1342/2000], Loss: 0.8657\n","Epoch [1344/2000], Loss: 1.1435\n","Epoch [1346/2000], Loss: 0.9872\n","Epoch [1348/2000], Loss: 0.7985\n","Epoch [1350/2000], Loss: 0.9515\n","Epoch [1352/2000], Loss: 0.9567\n","Epoch [1354/2000], Loss: 0.7255\n","Epoch [1356/2000], Loss: 0.6197\n","Epoch [1358/2000], Loss: 0.8975\n","Epoch [1360/2000], Loss: 1.0893\n","Epoch [1362/2000], Loss: 0.5606\n","Epoch [1364/2000], Loss: 0.7978\n","Epoch [1366/2000], Loss: 0.9052\n","Epoch [1368/2000], Loss: 1.0215\n","Epoch [1370/2000], Loss: 0.9478\n","Epoch [1372/2000], Loss: 1.0048\n","Epoch [1374/2000], Loss: 1.0873\n","Epoch [1376/2000], Loss: 1.0254\n","Epoch [1378/2000], Loss: 0.8931\n","Epoch [1380/2000], Loss: 0.9239\n","Epoch [1382/2000], Loss: 0.9028\n","Epoch [1384/2000], Loss: 0.7920\n","Epoch [1386/2000], Loss: 0.8882\n","Epoch [1388/2000], Loss: 1.0400\n","Epoch [1390/2000], Loss: 0.6664\n","Epoch [1392/2000], Loss: 0.9460\n","Epoch [1394/2000], Loss: 0.8208\n","Epoch [1396/2000], Loss: 1.0975\n","Epoch [1398/2000], Loss: 1.2128\n","Epoch [1400/2000], Loss: 0.7562\n","Epoch [1402/2000], Loss: 0.8132\n","Epoch [1404/2000], Loss: 0.7979\n","Epoch [1406/2000], Loss: 0.8966\n","Epoch [1408/2000], Loss: 0.8044\n","Epoch [1410/2000], Loss: 0.9190\n","Epoch [1412/2000], Loss: 0.7383\n","Epoch [1414/2000], Loss: 0.9384\n","Epoch [1416/2000], Loss: 1.2378\n","Epoch [1418/2000], Loss: 0.7314\n","Epoch [1420/2000], Loss: 0.6842\n","Epoch [1422/2000], Loss: 0.8193\n","Epoch [1424/2000], Loss: 0.8751\n","Epoch [1426/2000], Loss: 0.9963\n","Epoch [1428/2000], Loss: 0.9677\n","Epoch [1430/2000], Loss: 0.9381\n","Epoch [1432/2000], Loss: 0.8005\n","Epoch [1434/2000], Loss: 1.0045\n","Epoch [1436/2000], Loss: 0.9841\n","Epoch [1438/2000], Loss: 0.6793\n","Epoch [1440/2000], Loss: 0.9275\n","Epoch [1442/2000], Loss: 0.8884\n","Epoch [1444/2000], Loss: 0.8589\n","Epoch [1446/2000], Loss: 0.7583\n","Epoch [1448/2000], Loss: 1.1280\n","Epoch [1450/2000], Loss: 1.1604\n","Epoch [1452/2000], Loss: 0.9974\n","Epoch [1454/2000], Loss: 1.0167\n","Epoch [1456/2000], Loss: 0.7850\n","Epoch [1458/2000], Loss: 0.9772\n","Epoch [1460/2000], Loss: 0.7720\n","Epoch [1462/2000], Loss: 1.2003\n","Epoch [1464/2000], Loss: 0.8969\n","Epoch [1466/2000], Loss: 0.9223\n","Epoch [1468/2000], Loss: 0.9875\n","Epoch [1470/2000], Loss: 0.7512\n","Epoch [1472/2000], Loss: 0.9241\n","Epoch [1474/2000], Loss: 1.0174\n","Epoch [1476/2000], Loss: 1.1030\n","Epoch [1478/2000], Loss: 0.9867\n","Epoch [1480/2000], Loss: 0.8691\n","Epoch [1482/2000], Loss: 0.9113\n","Epoch [1484/2000], Loss: 0.7803\n","Epoch [1486/2000], Loss: 0.8354\n","Epoch [1488/2000], Loss: 1.0607\n","Epoch [1490/2000], Loss: 1.1676\n","Epoch [1492/2000], Loss: 0.9001\n","Epoch [1494/2000], Loss: 0.8252\n","Epoch [1496/2000], Loss: 0.7962\n","Epoch [1498/2000], Loss: 0.9094\n","Epoch [1500/2000], Loss: 0.7782\n","Epoch [1502/2000], Loss: 0.8795\n","Epoch [1504/2000], Loss: 0.9906\n","Epoch [1506/2000], Loss: 1.0774\n","Epoch [1508/2000], Loss: 0.7678\n","Epoch [1510/2000], Loss: 1.1420\n","Epoch [1512/2000], Loss: 0.8466\n","Epoch [1514/2000], Loss: 0.6413\n","Epoch [1516/2000], Loss: 0.8269\n","Epoch [1518/2000], Loss: 0.7181\n","Epoch [1520/2000], Loss: 1.0570\n","Epoch [1522/2000], Loss: 0.8241\n","Epoch [1524/2000], Loss: 1.0140\n","Epoch [1526/2000], Loss: 1.1454\n","Epoch [1528/2000], Loss: 0.9727\n","Epoch [1530/2000], Loss: 0.9736\n","Epoch [1532/2000], Loss: 0.7600\n","Epoch [1534/2000], Loss: 0.9975\n","Epoch [1536/2000], Loss: 0.5997\n","Epoch [1538/2000], Loss: 1.1604\n","Epoch [1540/2000], Loss: 1.0500\n","Epoch [1542/2000], Loss: 0.9073\n","Epoch [1544/2000], Loss: 0.6478\n","Epoch [1546/2000], Loss: 0.9181\n","Epoch [1548/2000], Loss: 0.8113\n","Epoch [1550/2000], Loss: 0.9610\n","Epoch [1552/2000], Loss: 0.8356\n","Epoch [1554/2000], Loss: 0.8168\n","Epoch [1556/2000], Loss: 0.9364\n","Epoch [1558/2000], Loss: 0.7348\n","Epoch [1560/2000], Loss: 0.9785\n","Epoch [1562/2000], Loss: 0.5823\n","Epoch [1564/2000], Loss: 0.6945\n","Epoch [1566/2000], Loss: 0.7533\n","Epoch [1568/2000], Loss: 0.8605\n","Epoch [1570/2000], Loss: 0.8021\n","Epoch [1572/2000], Loss: 0.9050\n","Epoch [1574/2000], Loss: 0.9753\n","Epoch [1576/2000], Loss: 0.9735\n","Epoch [1578/2000], Loss: 0.9637\n","Epoch [1580/2000], Loss: 0.7808\n","Epoch [1582/2000], Loss: 0.7812\n","Epoch [1584/2000], Loss: 0.6082\n","Epoch [1586/2000], Loss: 0.8392\n","Epoch [1588/2000], Loss: 0.6976\n","Epoch [1590/2000], Loss: 0.8540\n","Epoch [1592/2000], Loss: 0.8125\n","Epoch [1594/2000], Loss: 0.7875\n","Epoch [1596/2000], Loss: 0.9928\n","Epoch [1598/2000], Loss: 0.8204\n","Epoch [1600/2000], Loss: 0.8076\n","Epoch [1602/2000], Loss: 0.9978\n","Epoch [1604/2000], Loss: 0.6362\n","Epoch [1606/2000], Loss: 0.8954\n","Epoch [1608/2000], Loss: 0.9604\n","Epoch [1610/2000], Loss: 0.9816\n","Epoch [1612/2000], Loss: 0.7504\n","Epoch [1614/2000], Loss: 1.0103\n","Epoch [1616/2000], Loss: 0.8281\n","Epoch [1618/2000], Loss: 0.8087\n","Epoch [1620/2000], Loss: 0.8134\n","Epoch [1622/2000], Loss: 0.8703\n","Epoch [1624/2000], Loss: 0.8348\n","Epoch [1626/2000], Loss: 0.7994\n","Epoch [1628/2000], Loss: 1.0603\n","Epoch [1630/2000], Loss: 0.7049\n","Epoch [1632/2000], Loss: 0.8779\n","Epoch [1634/2000], Loss: 0.8536\n","Epoch [1636/2000], Loss: 1.1253\n","Epoch [1638/2000], Loss: 1.1144\n","Epoch [1640/2000], Loss: 0.7524\n","Epoch [1642/2000], Loss: 0.8607\n","Epoch [1644/2000], Loss: 1.1697\n","Epoch [1646/2000], Loss: 1.0648\n","Epoch [1648/2000], Loss: 0.7619\n","Epoch [1650/2000], Loss: 0.8259\n","Epoch [1652/2000], Loss: 0.8845\n","Epoch [1654/2000], Loss: 0.8004\n","Epoch [1656/2000], Loss: 0.8867\n","Epoch [1658/2000], Loss: 0.8887\n","Epoch [1660/2000], Loss: 0.8181\n","Epoch [1662/2000], Loss: 1.1520\n","Epoch [1664/2000], Loss: 0.7918\n","Epoch [1666/2000], Loss: 0.8081\n","Epoch [1668/2000], Loss: 1.0500\n","Epoch [1670/2000], Loss: 0.6390\n","Epoch [1672/2000], Loss: 1.0789\n","Epoch [1674/2000], Loss: 0.8756\n","Epoch [1676/2000], Loss: 0.6978\n","Epoch [1678/2000], Loss: 0.9687\n","Epoch [1680/2000], Loss: 0.9332\n","Epoch [1682/2000], Loss: 0.9041\n","Epoch [1684/2000], Loss: 1.0167\n","Epoch [1686/2000], Loss: 1.1139\n","Epoch [1688/2000], Loss: 0.9038\n","Epoch [1690/2000], Loss: 0.7285\n","Epoch [1692/2000], Loss: 0.7857\n","Epoch [1694/2000], Loss: 1.0419\n","Epoch [1696/2000], Loss: 0.8562\n","Epoch [1698/2000], Loss: 0.9372\n","Epoch [1700/2000], Loss: 0.9075\n","Epoch [1702/2000], Loss: 0.8565\n","Epoch [1704/2000], Loss: 0.7706\n","Epoch [1706/2000], Loss: 0.8363\n","Epoch [1708/2000], Loss: 0.4605\n","Epoch [1710/2000], Loss: 0.7089\n","Epoch [1712/2000], Loss: 0.9614\n","Epoch [1714/2000], Loss: 1.1737\n","Epoch [1716/2000], Loss: 0.8074\n","Epoch [1718/2000], Loss: 0.7506\n","Epoch [1720/2000], Loss: 0.9462\n","Epoch [1722/2000], Loss: 1.1815\n","Epoch [1724/2000], Loss: 0.6808\n","Epoch [1726/2000], Loss: 0.8145\n","Epoch [1728/2000], Loss: 0.9899\n","Epoch [1730/2000], Loss: 0.6660\n","Epoch [1732/2000], Loss: 1.0378\n","Epoch [1734/2000], Loss: 0.9663\n","Epoch [1736/2000], Loss: 1.0933\n","Epoch [1738/2000], Loss: 1.0570\n","Epoch [1740/2000], Loss: 0.6391\n","Epoch [1742/2000], Loss: 0.8355\n","Epoch [1744/2000], Loss: 0.7392\n","Epoch [1746/2000], Loss: 1.1042\n","Epoch [1748/2000], Loss: 0.9805\n","Epoch [1750/2000], Loss: 0.8904\n","Epoch [1752/2000], Loss: 0.8076\n","Epoch [1754/2000], Loss: 0.5224\n","Epoch [1756/2000], Loss: 0.8025\n","Epoch [1758/2000], Loss: 0.6695\n","Epoch [1760/2000], Loss: 0.6422\n","Epoch [1762/2000], Loss: 0.6931\n","Epoch [1764/2000], Loss: 0.8820\n","Epoch [1766/2000], Loss: 0.8921\n","Epoch [1768/2000], Loss: 0.7959\n","Epoch [1770/2000], Loss: 0.8719\n","Epoch [1772/2000], Loss: 0.7719\n","Epoch [1774/2000], Loss: 0.9843\n","Epoch [1776/2000], Loss: 0.8333\n","Epoch [1778/2000], Loss: 0.8877\n","Epoch [1780/2000], Loss: 0.6980\n","Epoch [1782/2000], Loss: 0.8627\n","Epoch [1784/2000], Loss: 0.9279\n","Epoch [1786/2000], Loss: 0.7946\n","Epoch [1788/2000], Loss: 0.8871\n","Epoch [1790/2000], Loss: 0.8034\n","Epoch [1792/2000], Loss: 0.5784\n","Epoch [1794/2000], Loss: 0.8510\n","Epoch [1796/2000], Loss: 0.6619\n","Epoch [1798/2000], Loss: 0.8587\n","Epoch [1800/2000], Loss: 0.8346\n","Epoch [1802/2000], Loss: 0.7945\n","Epoch [1804/2000], Loss: 0.9432\n","Epoch [1806/2000], Loss: 0.9181\n","Epoch [1808/2000], Loss: 0.8919\n","Epoch [1810/2000], Loss: 0.9213\n","Epoch [1812/2000], Loss: 1.0360\n","Epoch [1814/2000], Loss: 0.8734\n","Epoch [1816/2000], Loss: 0.5953\n","Epoch [1818/2000], Loss: 0.7925\n","Epoch [1820/2000], Loss: 0.9472\n","Epoch [1822/2000], Loss: 0.8731\n","Epoch [1824/2000], Loss: 0.6307\n","Epoch [1826/2000], Loss: 0.8195\n","Epoch [1828/2000], Loss: 1.1266\n","Epoch [1830/2000], Loss: 0.9487\n","Epoch [1832/2000], Loss: 0.6742\n","Epoch [1834/2000], Loss: 0.8531\n","Epoch [1836/2000], Loss: 0.7107\n","Epoch [1838/2000], Loss: 0.9083\n","Epoch [1840/2000], Loss: 0.9695\n","Epoch [1842/2000], Loss: 0.8813\n","Epoch [1844/2000], Loss: 0.9618\n","Epoch [1846/2000], Loss: 0.5985\n","Epoch [1848/2000], Loss: 0.8585\n","Epoch [1850/2000], Loss: 0.8563\n","Epoch [1852/2000], Loss: 0.5394\n","Epoch [1854/2000], Loss: 0.8579\n","Epoch [1856/2000], Loss: 0.8033\n","Epoch [1858/2000], Loss: 0.8887\n","Epoch [1860/2000], Loss: 0.8970\n","Epoch [1862/2000], Loss: 1.1688\n","Epoch [1864/2000], Loss: 0.8741\n","Epoch [1866/2000], Loss: 0.9407\n","Epoch [1868/2000], Loss: 0.9581\n","Epoch [1870/2000], Loss: 0.7532\n","Epoch [1872/2000], Loss: 0.7811\n","Epoch [1874/2000], Loss: 0.8514\n","Epoch [1876/2000], Loss: 0.9075\n","Epoch [1878/2000], Loss: 0.7739\n","Epoch [1880/2000], Loss: 0.6289\n","Epoch [1882/2000], Loss: 0.7435\n","Epoch [1884/2000], Loss: 0.7211\n","Epoch [1886/2000], Loss: 0.9695\n","Epoch [1888/2000], Loss: 0.7766\n","Epoch [1890/2000], Loss: 0.8579\n","Epoch [1892/2000], Loss: 0.7570\n","Epoch [1894/2000], Loss: 0.8214\n","Epoch [1896/2000], Loss: 0.7097\n","Epoch [1898/2000], Loss: 0.8225\n","Epoch [1900/2000], Loss: 0.7064\n","Epoch [1902/2000], Loss: 0.9730\n","Epoch [1904/2000], Loss: 0.8032\n","Epoch [1906/2000], Loss: 0.9441\n","Epoch [1908/2000], Loss: 0.7902\n","Epoch [1910/2000], Loss: 0.7760\n","Epoch [1912/2000], Loss: 0.5215\n","Epoch [1914/2000], Loss: 0.7649\n","Epoch [1916/2000], Loss: 0.9605\n","Epoch [1918/2000], Loss: 0.9103\n","Epoch [1920/2000], Loss: 0.8675\n","Epoch [1922/2000], Loss: 0.7011\n","Epoch [1924/2000], Loss: 0.9906\n","Epoch [1926/2000], Loss: 0.8266\n","Epoch [1928/2000], Loss: 0.6319\n","Epoch [1930/2000], Loss: 0.9043\n","Epoch [1932/2000], Loss: 0.9165\n","Epoch [1934/2000], Loss: 0.8878\n","Epoch [1936/2000], Loss: 0.7773\n","Epoch [1938/2000], Loss: 0.9648\n","Epoch [1940/2000], Loss: 0.8318\n","Epoch [1942/2000], Loss: 0.8545\n","Epoch [1944/2000], Loss: 1.0679\n","Epoch [1946/2000], Loss: 0.7678\n","Epoch [1948/2000], Loss: 1.0158\n","Epoch [1950/2000], Loss: 0.9256\n","Epoch [1952/2000], Loss: 0.6789\n","Epoch [1954/2000], Loss: 0.8582\n","Epoch [1956/2000], Loss: 0.6674\n","Epoch [1958/2000], Loss: 0.7620\n","Epoch [1960/2000], Loss: 0.7845\n","Epoch [1962/2000], Loss: 0.6071\n","Epoch [1964/2000], Loss: 1.0315\n","Epoch [1966/2000], Loss: 0.8381\n","Epoch [1968/2000], Loss: 0.7165\n","Epoch [1970/2000], Loss: 0.7498\n","Epoch [1972/2000], Loss: 0.6680\n","Epoch [1974/2000], Loss: 0.9034\n","Epoch [1976/2000], Loss: 0.7897\n","Epoch [1978/2000], Loss: 0.9400\n","Epoch [1980/2000], Loss: 0.7792\n","Epoch [1982/2000], Loss: 0.8506\n","Epoch [1984/2000], Loss: 0.8435\n","Epoch [1986/2000], Loss: 0.8297\n","Epoch [1988/2000], Loss: 0.7448\n","Epoch [1990/2000], Loss: 0.9496\n","Epoch [1992/2000], Loss: 0.6371\n","Epoch [1994/2000], Loss: 0.8488\n","Epoch [1996/2000], Loss: 0.8083\n","Epoch [1998/2000], Loss: 0.6770\n","Epoch [2000/2000], Loss: 0.8602\n","Epoch [2/2000], Loss: 1.0148\n","Epoch [4/2000], Loss: 1.0010\n","Epoch [6/2000], Loss: 1.3501\n","Epoch [8/2000], Loss: 1.3885\n","Epoch [10/2000], Loss: 1.1172\n","Epoch [12/2000], Loss: 1.0287\n","Epoch [14/2000], Loss: 1.1617\n","Epoch [16/2000], Loss: 0.8109\n","Epoch [18/2000], Loss: 0.9663\n","Epoch [20/2000], Loss: 1.1192\n","Epoch [22/2000], Loss: 0.8658\n","Epoch [24/2000], Loss: 1.3074\n","Epoch [26/2000], Loss: 0.7473\n","Epoch [28/2000], Loss: 0.9216\n","Epoch [30/2000], Loss: 1.0578\n","Epoch [32/2000], Loss: 0.9068\n","Epoch [34/2000], Loss: 1.0910\n","Epoch [36/2000], Loss: 0.7775\n","Epoch [38/2000], Loss: 1.1991\n","Epoch [40/2000], Loss: 1.0797\n","Epoch [42/2000], Loss: 0.9973\n","patience exceeded, loading best model\n","Epoch [2/2000], Loss: 1.2881\n","Epoch [4/2000], Loss: 1.2607\n","Epoch [6/2000], Loss: 1.1450\n","Epoch [8/2000], Loss: 0.9505\n","Epoch [10/2000], Loss: 1.2940\n","Epoch [12/2000], Loss: 1.0161\n","Epoch [14/2000], Loss: 1.1806\n","Epoch [16/2000], Loss: 1.2397\n","Epoch [18/2000], Loss: 1.0458\n","Epoch [20/2000], Loss: 1.4523\n","Epoch [22/2000], Loss: 1.2046\n","Epoch [24/2000], Loss: 1.2106\n","Epoch [26/2000], Loss: 0.8834\n","Epoch [28/2000], Loss: 1.2879\n","Epoch [30/2000], Loss: 0.9968\n","Epoch [32/2000], Loss: 0.9466\n","Epoch [34/2000], Loss: 1.2115\n","Epoch [36/2000], Loss: 1.1787\n","Epoch [38/2000], Loss: 1.1035\n","Epoch [40/2000], Loss: 1.5047\n","Epoch [42/2000], Loss: 1.0708\n","Epoch [44/2000], Loss: 1.0008\n","Epoch [46/2000], Loss: 1.0700\n","Epoch [48/2000], Loss: 1.0945\n","Epoch [50/2000], Loss: 1.0535\n","Epoch [52/2000], Loss: 1.2432\n","Epoch [54/2000], Loss: 1.2338\n","Epoch [56/2000], Loss: 1.0570\n","Epoch [58/2000], Loss: 1.1018\n","Epoch [60/2000], Loss: 1.0845\n","Epoch [62/2000], Loss: 0.9930\n","Epoch [64/2000], Loss: 1.1869\n","Epoch [66/2000], Loss: 1.1357\n","Epoch [68/2000], Loss: 1.2004\n","Epoch [70/2000], Loss: 1.1991\n","Epoch [72/2000], Loss: 0.9909\n","Epoch [74/2000], Loss: 1.2631\n","Epoch [76/2000], Loss: 0.7779\n","Epoch [78/2000], Loss: 1.1859\n","Epoch [80/2000], Loss: 1.0938\n","Epoch [82/2000], Loss: 1.1742\n","Epoch [84/2000], Loss: 1.0556\n","Epoch [86/2000], Loss: 1.1332\n","Epoch [88/2000], Loss: 0.9685\n","Epoch [90/2000], Loss: 0.9113\n","Epoch [92/2000], Loss: 1.2271\n","Epoch [94/2000], Loss: 0.8101\n","Epoch [96/2000], Loss: 1.1764\n","Epoch [98/2000], Loss: 1.2228\n","Epoch [100/2000], Loss: 0.9345\n","Epoch [102/2000], Loss: 1.1360\n","Epoch [104/2000], Loss: 0.9889\n","Epoch [106/2000], Loss: 0.7520\n","Epoch [108/2000], Loss: 1.2610\n","Epoch [110/2000], Loss: 0.9732\n","Epoch [112/2000], Loss: 0.9895\n","Epoch [114/2000], Loss: 0.9086\n","Epoch [116/2000], Loss: 1.0104\n","Epoch [118/2000], Loss: 1.2154\n","Epoch [120/2000], Loss: 1.0118\n","Epoch [122/2000], Loss: 1.3369\n","Epoch [124/2000], Loss: 0.9819\n","Epoch [126/2000], Loss: 0.9264\n","Epoch [128/2000], Loss: 0.8586\n","Epoch [130/2000], Loss: 1.0330\n","Epoch [132/2000], Loss: 1.0542\n","Epoch [134/2000], Loss: 1.0435\n","Epoch [136/2000], Loss: 1.4342\n","Epoch [138/2000], Loss: 1.1273\n","Epoch [140/2000], Loss: 0.9537\n","Epoch [142/2000], Loss: 0.9757\n","Epoch [144/2000], Loss: 1.0384\n","Epoch [146/2000], Loss: 1.0139\n","Epoch [148/2000], Loss: 1.1258\n","Epoch [150/2000], Loss: 0.8529\n","Epoch [152/2000], Loss: 1.0037\n","Epoch [154/2000], Loss: 1.0302\n","Epoch [156/2000], Loss: 1.0403\n","Epoch [158/2000], Loss: 0.8818\n","Epoch [160/2000], Loss: 1.2231\n","Epoch [162/2000], Loss: 1.0033\n","Epoch [164/2000], Loss: 1.0711\n","Epoch [166/2000], Loss: 1.0454\n","Epoch [168/2000], Loss: 1.0651\n","Epoch [170/2000], Loss: 1.3875\n","Epoch [172/2000], Loss: 1.3667\n","Epoch [174/2000], Loss: 1.0325\n","Epoch [176/2000], Loss: 1.1411\n","Epoch [178/2000], Loss: 0.8283\n","Epoch [180/2000], Loss: 1.0634\n","Epoch [182/2000], Loss: 0.8331\n","Epoch [184/2000], Loss: 0.9093\n","Epoch [186/2000], Loss: 1.2077\n","Epoch [188/2000], Loss: 1.0760\n","Epoch [190/2000], Loss: 1.0834\n","Epoch [192/2000], Loss: 1.0361\n","Epoch [194/2000], Loss: 0.7594\n","Epoch [196/2000], Loss: 1.2468\n","Epoch [198/2000], Loss: 0.9048\n","Epoch [200/2000], Loss: 1.0317\n","Epoch [202/2000], Loss: 1.1014\n","Epoch [204/2000], Loss: 1.2058\n","Epoch [206/2000], Loss: 1.4668\n","Epoch [208/2000], Loss: 1.2828\n","Epoch [210/2000], Loss: 0.9518\n","Epoch [212/2000], Loss: 0.9465\n","Epoch [214/2000], Loss: 0.6947\n","Epoch [216/2000], Loss: 1.0556\n","Epoch [218/2000], Loss: 1.0964\n","Epoch [220/2000], Loss: 1.0200\n","Epoch [222/2000], Loss: 1.0692\n","Epoch [224/2000], Loss: 0.8673\n","Epoch [226/2000], Loss: 1.1576\n","Epoch [228/2000], Loss: 1.0022\n","Epoch [230/2000], Loss: 0.8777\n","Epoch [232/2000], Loss: 1.2753\n","Epoch [234/2000], Loss: 1.1560\n","Epoch [236/2000], Loss: 0.8017\n","Epoch [238/2000], Loss: 1.1629\n","Epoch [240/2000], Loss: 0.9980\n","Epoch [242/2000], Loss: 1.1270\n","Epoch [244/2000], Loss: 1.1255\n","Epoch [246/2000], Loss: 0.7577\n","Epoch [248/2000], Loss: 1.2128\n","Epoch [250/2000], Loss: 0.8559\n","Epoch [252/2000], Loss: 0.9733\n","Epoch [254/2000], Loss: 1.0337\n","Epoch [256/2000], Loss: 1.1793\n","Epoch [258/2000], Loss: 1.1732\n","Epoch [260/2000], Loss: 1.1301\n","Epoch [262/2000], Loss: 1.1835\n","Epoch [264/2000], Loss: 1.0270\n","Epoch [266/2000], Loss: 0.9206\n","Epoch [268/2000], Loss: 1.1630\n","Epoch [270/2000], Loss: 1.2327\n","Epoch [272/2000], Loss: 0.9909\n","Epoch [274/2000], Loss: 1.0373\n","Epoch [276/2000], Loss: 0.7667\n","Epoch [278/2000], Loss: 0.8150\n","Epoch [280/2000], Loss: 1.0441\n","Epoch [282/2000], Loss: 1.1773\n","Epoch [284/2000], Loss: 0.9136\n","Epoch [286/2000], Loss: 0.8890\n","Epoch [288/2000], Loss: 1.0711\n","Epoch [290/2000], Loss: 0.8667\n","Epoch [292/2000], Loss: 0.8872\n","Epoch [294/2000], Loss: 0.8532\n","Epoch [296/2000], Loss: 1.0791\n","Epoch [298/2000], Loss: 0.9343\n","Epoch [300/2000], Loss: 1.0929\n","Epoch [302/2000], Loss: 1.0510\n","Epoch [304/2000], Loss: 0.9842\n","Epoch [306/2000], Loss: 1.1363\n","Epoch [308/2000], Loss: 1.0202\n","Epoch [310/2000], Loss: 1.1960\n","Epoch [312/2000], Loss: 0.8670\n","Epoch [314/2000], Loss: 0.9790\n","Epoch [316/2000], Loss: 1.1355\n","Epoch [318/2000], Loss: 0.9198\n","Epoch [320/2000], Loss: 0.9072\n","Epoch [322/2000], Loss: 1.4100\n","Epoch [324/2000], Loss: 1.0901\n","Epoch [326/2000], Loss: 1.0810\n","Epoch [328/2000], Loss: 1.0435\n","Epoch [330/2000], Loss: 1.1667\n","Epoch [332/2000], Loss: 1.0936\n","Epoch [334/2000], Loss: 0.9316\n","Epoch [336/2000], Loss: 0.8573\n","Epoch [338/2000], Loss: 0.9458\n","Epoch [340/2000], Loss: 1.1319\n","Epoch [342/2000], Loss: 0.8808\n","Epoch [344/2000], Loss: 0.8339\n","Epoch [346/2000], Loss: 0.8892\n","Epoch [348/2000], Loss: 1.4118\n","Epoch [350/2000], Loss: 0.8468\n","Epoch [352/2000], Loss: 1.1370\n","Epoch [354/2000], Loss: 0.9158\n","Epoch [356/2000], Loss: 1.0514\n","Epoch [358/2000], Loss: 1.0512\n","Epoch [360/2000], Loss: 0.9831\n","Epoch [362/2000], Loss: 1.4793\n","Epoch [364/2000], Loss: 1.0770\n","Epoch [366/2000], Loss: 1.1391\n","Epoch [368/2000], Loss: 0.9270\n","Epoch [370/2000], Loss: 0.7057\n","Epoch [372/2000], Loss: 1.0333\n","Epoch [374/2000], Loss: 1.1468\n","Epoch [376/2000], Loss: 0.9053\n","Epoch [378/2000], Loss: 1.0143\n","Epoch [380/2000], Loss: 1.0400\n","Epoch [382/2000], Loss: 1.1916\n","Epoch [384/2000], Loss: 0.8404\n","Epoch [386/2000], Loss: 1.1715\n","Epoch [388/2000], Loss: 0.8526\n","Epoch [390/2000], Loss: 1.0377\n","Epoch [392/2000], Loss: 1.0680\n","Epoch [394/2000], Loss: 1.1507\n","Epoch [396/2000], Loss: 0.7923\n","Epoch [398/2000], Loss: 0.9879\n","Epoch [400/2000], Loss: 0.9526\n","Epoch [402/2000], Loss: 0.9155\n","Epoch [404/2000], Loss: 0.9943\n","Epoch [406/2000], Loss: 1.0044\n","Epoch [408/2000], Loss: 1.0850\n","Epoch [410/2000], Loss: 1.0227\n","Epoch [412/2000], Loss: 0.8930\n","Epoch [414/2000], Loss: 1.0376\n","Epoch [416/2000], Loss: 0.8626\n","Epoch [418/2000], Loss: 0.9437\n","Epoch [420/2000], Loss: 0.9858\n","Epoch [422/2000], Loss: 1.0944\n","Epoch [424/2000], Loss: 0.9605\n","Epoch [426/2000], Loss: 1.0897\n","Epoch [428/2000], Loss: 1.0386\n","Epoch [430/2000], Loss: 0.9875\n","Epoch [432/2000], Loss: 1.0954\n","Epoch [434/2000], Loss: 1.1846\n","Epoch [436/2000], Loss: 0.8761\n","Epoch [438/2000], Loss: 1.0841\n","Epoch [440/2000], Loss: 1.0474\n","Epoch [442/2000], Loss: 0.9021\n","Epoch [444/2000], Loss: 1.0339\n","Epoch [446/2000], Loss: 1.3157\n","Epoch [448/2000], Loss: 1.0769\n","Epoch [450/2000], Loss: 0.9612\n","Epoch [452/2000], Loss: 0.8439\n","Epoch [454/2000], Loss: 1.0551\n","Epoch [456/2000], Loss: 0.9511\n","Epoch [458/2000], Loss: 0.9928\n","Epoch [460/2000], Loss: 1.0307\n","Epoch [462/2000], Loss: 1.1332\n","Epoch [464/2000], Loss: 1.1907\n","Epoch [466/2000], Loss: 1.0924\n","Epoch [468/2000], Loss: 1.0742\n","Epoch [470/2000], Loss: 1.1580\n","Epoch [472/2000], Loss: 0.9439\n","Epoch [474/2000], Loss: 0.8780\n","Epoch [476/2000], Loss: 1.1322\n","Epoch [478/2000], Loss: 0.9804\n","Epoch [480/2000], Loss: 1.1034\n","Epoch [482/2000], Loss: 1.1532\n","Epoch [484/2000], Loss: 1.0195\n","Epoch [486/2000], Loss: 0.9396\n","Epoch [488/2000], Loss: 1.0197\n","Epoch [490/2000], Loss: 0.9787\n","Epoch [492/2000], Loss: 1.1715\n","Epoch [494/2000], Loss: 1.3012\n","Epoch [496/2000], Loss: 1.1469\n","Epoch [498/2000], Loss: 0.9398\n","Epoch [500/2000], Loss: 1.1230\n","Epoch [502/2000], Loss: 0.9669\n","Epoch [504/2000], Loss: 1.0280\n","Epoch [506/2000], Loss: 0.9517\n","Epoch [508/2000], Loss: 1.1151\n","Epoch [510/2000], Loss: 1.1305\n","Epoch [512/2000], Loss: 0.8500\n","Epoch [514/2000], Loss: 1.0169\n","Epoch [516/2000], Loss: 1.0671\n","Epoch [518/2000], Loss: 0.7748\n","Epoch [520/2000], Loss: 0.8108\n","Epoch [522/2000], Loss: 0.9321\n","Epoch [524/2000], Loss: 1.0007\n","Epoch [526/2000], Loss: 0.8164\n","Epoch [528/2000], Loss: 0.8961\n","Epoch [530/2000], Loss: 0.8444\n","Epoch [532/2000], Loss: 0.9641\n","Epoch [534/2000], Loss: 0.8353\n","Epoch [536/2000], Loss: 1.0647\n","Epoch [538/2000], Loss: 1.3195\n","Epoch [540/2000], Loss: 1.0083\n","Epoch [542/2000], Loss: 1.2490\n","Epoch [544/2000], Loss: 0.9857\n","Epoch [546/2000], Loss: 1.0031\n","Epoch [548/2000], Loss: 0.9591\n","Epoch [550/2000], Loss: 1.1368\n","Epoch [552/2000], Loss: 0.9143\n","Epoch [554/2000], Loss: 1.1972\n","Epoch [556/2000], Loss: 0.7997\n","Epoch [558/2000], Loss: 1.1857\n","Epoch [560/2000], Loss: 1.0698\n","Epoch [562/2000], Loss: 1.0382\n","Epoch [564/2000], Loss: 1.1358\n","Epoch [566/2000], Loss: 1.0938\n","Epoch [568/2000], Loss: 0.9928\n","Epoch [570/2000], Loss: 1.0850\n","Epoch [572/2000], Loss: 1.1453\n","Epoch [574/2000], Loss: 0.7665\n","Epoch [576/2000], Loss: 1.1704\n","Epoch [578/2000], Loss: 0.8931\n","Epoch [580/2000], Loss: 0.9413\n","Epoch [582/2000], Loss: 1.0796\n","Epoch [584/2000], Loss: 0.8559\n","Epoch [586/2000], Loss: 1.0976\n","Epoch [588/2000], Loss: 0.8315\n","Epoch [590/2000], Loss: 0.9260\n","Epoch [592/2000], Loss: 0.8981\n","Epoch [594/2000], Loss: 0.9759\n","Epoch [596/2000], Loss: 0.9276\n","Epoch [598/2000], Loss: 1.3714\n","Epoch [600/2000], Loss: 0.9874\n","Epoch [602/2000], Loss: 0.8645\n","Epoch [604/2000], Loss: 1.0167\n","Epoch [606/2000], Loss: 0.9648\n","Epoch [608/2000], Loss: 0.9993\n","Epoch [610/2000], Loss: 1.0219\n","Epoch [612/2000], Loss: 1.1013\n","Epoch [614/2000], Loss: 1.0022\n","Epoch [616/2000], Loss: 0.9086\n","Epoch [618/2000], Loss: 1.0413\n","Epoch [620/2000], Loss: 0.9837\n","Epoch [622/2000], Loss: 0.8850\n","Epoch [624/2000], Loss: 1.0195\n","Epoch [626/2000], Loss: 0.8096\n","Epoch [628/2000], Loss: 1.1512\n","Epoch [630/2000], Loss: 0.9357\n","Epoch [632/2000], Loss: 1.0776\n","Epoch [634/2000], Loss: 0.8979\n","Epoch [636/2000], Loss: 0.8263\n","Epoch [638/2000], Loss: 1.0100\n","Epoch [640/2000], Loss: 1.3255\n","Epoch [642/2000], Loss: 1.1669\n","Epoch [644/2000], Loss: 1.0344\n","Epoch [646/2000], Loss: 1.0656\n","Epoch [648/2000], Loss: 1.0296\n","Epoch [650/2000], Loss: 1.1121\n","Epoch [652/2000], Loss: 0.9379\n","Epoch [654/2000], Loss: 0.9530\n","Epoch [656/2000], Loss: 0.9214\n","Epoch [658/2000], Loss: 1.2143\n","Epoch [660/2000], Loss: 1.1413\n","Epoch [662/2000], Loss: 1.0903\n","Epoch [664/2000], Loss: 1.0273\n","Epoch [666/2000], Loss: 0.7186\n","Epoch [668/2000], Loss: 1.1409\n","Epoch [670/2000], Loss: 0.9252\n","Epoch [672/2000], Loss: 1.0053\n","Epoch [674/2000], Loss: 1.3457\n","Epoch [676/2000], Loss: 0.8533\n","Epoch [678/2000], Loss: 1.1086\n","Epoch [680/2000], Loss: 0.9388\n","Epoch [682/2000], Loss: 0.7699\n","Epoch [684/2000], Loss: 0.9569\n","Epoch [686/2000], Loss: 1.0585\n","Epoch [688/2000], Loss: 1.0641\n","Epoch [690/2000], Loss: 1.0554\n","Epoch [692/2000], Loss: 1.2275\n","Epoch [694/2000], Loss: 0.8962\n","Epoch [696/2000], Loss: 0.8794\n","Epoch [698/2000], Loss: 1.0411\n","Epoch [700/2000], Loss: 0.9148\n","Epoch [702/2000], Loss: 0.9542\n","Epoch [704/2000], Loss: 1.0364\n","Epoch [706/2000], Loss: 1.1605\n","Epoch [708/2000], Loss: 1.0008\n","Epoch [710/2000], Loss: 1.0082\n","Epoch [712/2000], Loss: 1.1306\n","Epoch [714/2000], Loss: 1.2730\n","Epoch [716/2000], Loss: 0.9375\n","Epoch [718/2000], Loss: 0.8221\n","Epoch [720/2000], Loss: 1.0999\n","Epoch [722/2000], Loss: 1.0255\n","Epoch [724/2000], Loss: 1.0304\n","Epoch [726/2000], Loss: 1.0609\n","Epoch [728/2000], Loss: 1.0597\n","Epoch [730/2000], Loss: 0.9324\n","Epoch [732/2000], Loss: 1.0036\n","Epoch [734/2000], Loss: 0.8765\n","Epoch [736/2000], Loss: 0.8385\n","Epoch [738/2000], Loss: 1.0831\n","Epoch [740/2000], Loss: 0.9981\n","Epoch [742/2000], Loss: 0.8562\n","Epoch [744/2000], Loss: 1.0747\n","Epoch [746/2000], Loss: 1.0962\n","Epoch [748/2000], Loss: 1.0977\n","Epoch [750/2000], Loss: 0.8183\n","Epoch [752/2000], Loss: 0.9473\n","Epoch [754/2000], Loss: 1.1447\n","Epoch [756/2000], Loss: 0.8850\n","Epoch [758/2000], Loss: 0.9692\n","Epoch [760/2000], Loss: 1.2002\n","Epoch [762/2000], Loss: 1.1740\n","Epoch [764/2000], Loss: 0.8068\n","Epoch [766/2000], Loss: 1.1680\n","Epoch [768/2000], Loss: 0.9650\n","Epoch [770/2000], Loss: 0.8582\n","Epoch [772/2000], Loss: 1.1632\n","Epoch [774/2000], Loss: 0.7920\n","Epoch [776/2000], Loss: 1.0255\n","Epoch [778/2000], Loss: 0.9596\n","Epoch [780/2000], Loss: 0.9758\n","Epoch [782/2000], Loss: 1.0600\n","Epoch [784/2000], Loss: 1.1213\n","Epoch [786/2000], Loss: 0.9385\n","Epoch [788/2000], Loss: 0.7316\n","Epoch [790/2000], Loss: 1.0135\n","Epoch [792/2000], Loss: 0.7287\n","Epoch [794/2000], Loss: 0.9877\n","Epoch [796/2000], Loss: 0.8908\n","Epoch [798/2000], Loss: 0.9086\n","Epoch [800/2000], Loss: 0.9223\n","Epoch [802/2000], Loss: 1.0782\n","Epoch [804/2000], Loss: 1.0112\n","Epoch [806/2000], Loss: 0.9048\n","Epoch [808/2000], Loss: 0.7591\n","Epoch [810/2000], Loss: 1.3812\n","Epoch [812/2000], Loss: 0.9600\n","Epoch [814/2000], Loss: 0.9009\n","Epoch [816/2000], Loss: 0.8470\n","Epoch [818/2000], Loss: 0.7300\n","Epoch [820/2000], Loss: 0.9624\n","Epoch [822/2000], Loss: 0.8059\n","Epoch [824/2000], Loss: 0.7877\n","Epoch [826/2000], Loss: 0.9881\n","Epoch [828/2000], Loss: 0.8943\n","Epoch [830/2000], Loss: 1.2912\n","Epoch [832/2000], Loss: 0.8959\n","Epoch [834/2000], Loss: 1.0662\n","Epoch [836/2000], Loss: 0.8879\n","Epoch [838/2000], Loss: 0.9554\n","Epoch [840/2000], Loss: 1.0323\n","Epoch [842/2000], Loss: 1.0729\n","Epoch [844/2000], Loss: 0.9647\n","Epoch [846/2000], Loss: 0.8565\n","Epoch [848/2000], Loss: 0.9647\n","Epoch [850/2000], Loss: 1.0117\n","Epoch [852/2000], Loss: 0.8375\n","Epoch [854/2000], Loss: 1.0035\n","Epoch [856/2000], Loss: 1.1066\n","Epoch [858/2000], Loss: 0.8686\n","Epoch [860/2000], Loss: 0.9456\n","Epoch [862/2000], Loss: 0.8591\n","Epoch [864/2000], Loss: 0.9345\n","Epoch [866/2000], Loss: 0.8357\n","Epoch [868/2000], Loss: 0.9809\n","Epoch [870/2000], Loss: 0.9795\n","Epoch [872/2000], Loss: 1.0083\n","Epoch [874/2000], Loss: 0.9580\n","Epoch [876/2000], Loss: 0.8766\n","Epoch [878/2000], Loss: 0.8640\n","Epoch [880/2000], Loss: 1.1184\n","Epoch [882/2000], Loss: 0.8346\n","Epoch [884/2000], Loss: 0.9314\n","Epoch [886/2000], Loss: 0.9893\n","Epoch [888/2000], Loss: 0.9249\n","Epoch [890/2000], Loss: 0.9401\n","Epoch [892/2000], Loss: 1.0531\n","Epoch [894/2000], Loss: 0.9376\n","Epoch [896/2000], Loss: 0.9291\n","Epoch [898/2000], Loss: 1.0752\n","Epoch [900/2000], Loss: 0.9017\n","Epoch [902/2000], Loss: 0.8643\n","Epoch [904/2000], Loss: 0.9790\n","Epoch [906/2000], Loss: 0.9854\n","Epoch [908/2000], Loss: 0.8673\n","Epoch [910/2000], Loss: 0.7420\n","Epoch [912/2000], Loss: 1.2371\n","Epoch [914/2000], Loss: 0.9725\n","Epoch [916/2000], Loss: 0.8674\n","Epoch [918/2000], Loss: 0.9744\n","Epoch [920/2000], Loss: 0.7507\n","Epoch [922/2000], Loss: 0.8752\n","Epoch [924/2000], Loss: 0.8063\n","Epoch [926/2000], Loss: 1.0292\n","Epoch [928/2000], Loss: 0.9649\n","Epoch [930/2000], Loss: 1.0988\n","Epoch [932/2000], Loss: 0.8570\n","Epoch [934/2000], Loss: 1.0033\n","Epoch [936/2000], Loss: 1.0032\n","Epoch [938/2000], Loss: 1.0431\n","Epoch [940/2000], Loss: 0.9314\n","Epoch [942/2000], Loss: 0.8761\n","Epoch [944/2000], Loss: 1.0960\n","Epoch [946/2000], Loss: 1.0557\n","Epoch [948/2000], Loss: 1.0148\n","Epoch [950/2000], Loss: 1.1217\n","Epoch [952/2000], Loss: 0.8595\n","Epoch [954/2000], Loss: 0.9800\n","Epoch [956/2000], Loss: 1.0822\n","Epoch [958/2000], Loss: 1.0068\n","Epoch [960/2000], Loss: 1.0487\n","Epoch [962/2000], Loss: 0.7699\n","Epoch [964/2000], Loss: 1.1224\n","Epoch [966/2000], Loss: 0.8775\n","Epoch [968/2000], Loss: 0.8967\n","Epoch [970/2000], Loss: 0.9240\n","Epoch [972/2000], Loss: 0.9155\n","Epoch [974/2000], Loss: 1.0141\n","Epoch [976/2000], Loss: 0.9592\n","Epoch [978/2000], Loss: 0.9629\n","Epoch [980/2000], Loss: 0.9892\n","Epoch [982/2000], Loss: 1.0492\n","Epoch [984/2000], Loss: 1.0672\n","Epoch [986/2000], Loss: 0.6815\n","Epoch [988/2000], Loss: 0.6826\n","Epoch [990/2000], Loss: 1.0647\n","Epoch [992/2000], Loss: 0.8294\n","Epoch [994/2000], Loss: 1.1510\n","Epoch [996/2000], Loss: 0.8584\n","Epoch [998/2000], Loss: 1.0498\n","Epoch [1000/2000], Loss: 0.7489\n","Epoch [1002/2000], Loss: 1.1000\n","Epoch [1004/2000], Loss: 0.8882\n","Epoch [1006/2000], Loss: 0.7484\n","Epoch [1008/2000], Loss: 1.0208\n","Epoch [1010/2000], Loss: 0.7587\n","Epoch [1012/2000], Loss: 0.7490\n","Epoch [1014/2000], Loss: 0.5088\n","Epoch [1016/2000], Loss: 0.9996\n","Epoch [1018/2000], Loss: 1.1308\n","Epoch [1020/2000], Loss: 1.0113\n","Epoch [1022/2000], Loss: 1.0591\n","Epoch [1024/2000], Loss: 0.8968\n","Epoch [1026/2000], Loss: 0.9303\n","Epoch [1028/2000], Loss: 1.0079\n","Epoch [1030/2000], Loss: 1.0693\n","Epoch [1032/2000], Loss: 0.7246\n","Epoch [1034/2000], Loss: 1.1067\n","Epoch [1036/2000], Loss: 1.2069\n","Epoch [1038/2000], Loss: 1.1128\n","Epoch [1040/2000], Loss: 0.8882\n","Epoch [1042/2000], Loss: 0.9502\n","Epoch [1044/2000], Loss: 1.0741\n","Epoch [1046/2000], Loss: 1.1034\n","Epoch [1048/2000], Loss: 0.7774\n","Epoch [1050/2000], Loss: 0.9717\n","Epoch [1052/2000], Loss: 0.9050\n","Epoch [1054/2000], Loss: 1.2295\n","Epoch [1056/2000], Loss: 1.0357\n","Epoch [1058/2000], Loss: 0.9228\n","Epoch [1060/2000], Loss: 0.8860\n","Epoch [1062/2000], Loss: 0.9575\n","Epoch [1064/2000], Loss: 0.8205\n","Epoch [1066/2000], Loss: 1.0166\n","Epoch [1068/2000], Loss: 0.8676\n","Epoch [1070/2000], Loss: 1.1444\n","Epoch [1072/2000], Loss: 1.0638\n","Epoch [1074/2000], Loss: 0.7721\n","Epoch [1076/2000], Loss: 0.7536\n","Epoch [1078/2000], Loss: 1.0423\n","Epoch [1080/2000], Loss: 1.2140\n","Epoch [1082/2000], Loss: 0.9668\n","Epoch [1084/2000], Loss: 0.8211\n","Epoch [1086/2000], Loss: 1.0233\n","Epoch [1088/2000], Loss: 1.0537\n","Epoch [1090/2000], Loss: 0.8358\n","Epoch [1092/2000], Loss: 0.9523\n","Epoch [1094/2000], Loss: 0.8674\n","Epoch [1096/2000], Loss: 1.1619\n","Epoch [1098/2000], Loss: 1.0351\n","Epoch [1100/2000], Loss: 0.9156\n","Epoch [1102/2000], Loss: 0.9430\n","Epoch [1104/2000], Loss: 1.0123\n","Epoch [1106/2000], Loss: 1.0072\n","Epoch [1108/2000], Loss: 1.0162\n","Epoch [1110/2000], Loss: 0.9729\n","Epoch [1112/2000], Loss: 1.2169\n","Epoch [1114/2000], Loss: 0.7454\n","Epoch [1116/2000], Loss: 0.5728\n","Epoch [1118/2000], Loss: 0.9674\n","Epoch [1120/2000], Loss: 0.8949\n","Epoch [1122/2000], Loss: 0.8733\n","Epoch [1124/2000], Loss: 0.8315\n","Epoch [1126/2000], Loss: 0.7642\n","Epoch [1128/2000], Loss: 0.9220\n","Epoch [1130/2000], Loss: 1.0020\n","Epoch [1132/2000], Loss: 0.7156\n","Epoch [1134/2000], Loss: 0.9605\n","Epoch [1136/2000], Loss: 0.8005\n","Epoch [1138/2000], Loss: 1.0086\n","Epoch [1140/2000], Loss: 0.8071\n","Epoch [1142/2000], Loss: 0.9537\n","Epoch [1144/2000], Loss: 0.8904\n","Epoch [1146/2000], Loss: 0.8566\n","Epoch [1148/2000], Loss: 0.7962\n","Epoch [1150/2000], Loss: 0.7818\n","Epoch [1152/2000], Loss: 0.7972\n","Epoch [1154/2000], Loss: 0.7954\n","Epoch [1156/2000], Loss: 0.8353\n","Epoch [1158/2000], Loss: 0.9626\n","Epoch [1160/2000], Loss: 1.1538\n","Epoch [1162/2000], Loss: 0.7653\n","Epoch [1164/2000], Loss: 0.7866\n","Epoch [1166/2000], Loss: 0.8889\n","Epoch [1168/2000], Loss: 1.0231\n","Epoch [1170/2000], Loss: 0.8944\n","Epoch [1172/2000], Loss: 0.9084\n","Epoch [1174/2000], Loss: 1.1405\n","Epoch [1176/2000], Loss: 0.6402\n","Epoch [1178/2000], Loss: 1.0243\n","Epoch [1180/2000], Loss: 0.7051\n","Epoch [1182/2000], Loss: 0.9144\n","Epoch [1184/2000], Loss: 0.8755\n","Epoch [1186/2000], Loss: 0.9015\n","Epoch [1188/2000], Loss: 1.0090\n","Epoch [1190/2000], Loss: 0.7557\n","Epoch [1192/2000], Loss: 1.0255\n","Epoch [1194/2000], Loss: 0.7993\n","Epoch [1196/2000], Loss: 0.7384\n","Epoch [1198/2000], Loss: 0.8696\n","Epoch [1200/2000], Loss: 0.8165\n","Epoch [1202/2000], Loss: 1.0229\n","Epoch [1204/2000], Loss: 0.9553\n","Epoch [1206/2000], Loss: 1.1007\n","Epoch [1208/2000], Loss: 0.6461\n","Epoch [1210/2000], Loss: 0.8009\n","Epoch [1212/2000], Loss: 0.9394\n","Epoch [1214/2000], Loss: 1.1554\n","Epoch [1216/2000], Loss: 1.2441\n","Epoch [1218/2000], Loss: 0.7567\n","Epoch [1220/2000], Loss: 0.6400\n","Epoch [1222/2000], Loss: 1.1230\n","Epoch [1224/2000], Loss: 0.7276\n","Epoch [1226/2000], Loss: 1.1899\n","Epoch [1228/2000], Loss: 0.9922\n","Epoch [1230/2000], Loss: 0.6303\n","Epoch [1232/2000], Loss: 0.9545\n","Epoch [1234/2000], Loss: 0.6956\n","Epoch [1236/2000], Loss: 0.8859\n","Epoch [1238/2000], Loss: 0.9846\n","Epoch [1240/2000], Loss: 0.6945\n","Epoch [1242/2000], Loss: 0.8510\n","Epoch [1244/2000], Loss: 0.8101\n","Epoch [1246/2000], Loss: 0.9905\n","Epoch [1248/2000], Loss: 1.1111\n","Epoch [1250/2000], Loss: 0.7811\n","Epoch [1252/2000], Loss: 0.9120\n","Epoch [1254/2000], Loss: 0.6465\n","Epoch [1256/2000], Loss: 0.8250\n","Epoch [1258/2000], Loss: 1.1070\n","Epoch [1260/2000], Loss: 0.7833\n","Epoch [1262/2000], Loss: 0.7412\n","Epoch [1264/2000], Loss: 0.9857\n","Epoch [1266/2000], Loss: 0.6236\n","Epoch [1268/2000], Loss: 1.0107\n","Epoch [1270/2000], Loss: 0.8838\n","Epoch [1272/2000], Loss: 0.8999\n","Epoch [1274/2000], Loss: 1.0341\n","Epoch [1276/2000], Loss: 0.7895\n","Epoch [1278/2000], Loss: 1.0283\n","Epoch [1280/2000], Loss: 0.8317\n","Epoch [1282/2000], Loss: 0.9556\n","Epoch [1284/2000], Loss: 0.6964\n","Epoch [1286/2000], Loss: 0.9385\n","Epoch [1288/2000], Loss: 0.9027\n","Epoch [1290/2000], Loss: 0.9464\n","Epoch [1292/2000], Loss: 1.2364\n","Epoch [1294/2000], Loss: 0.8717\n","Epoch [1296/2000], Loss: 0.5495\n","Epoch [1298/2000], Loss: 1.1004\n","Epoch [1300/2000], Loss: 0.8133\n","Epoch [1302/2000], Loss: 0.7766\n","Epoch [1304/2000], Loss: 1.0979\n","Epoch [1306/2000], Loss: 0.7747\n","Epoch [1308/2000], Loss: 0.8661\n","Epoch [1310/2000], Loss: 0.9120\n","Epoch [1312/2000], Loss: 1.1084\n","Epoch [1314/2000], Loss: 0.9236\n","Epoch [1316/2000], Loss: 1.0678\n","Epoch [1318/2000], Loss: 0.7309\n","Epoch [1320/2000], Loss: 0.8442\n","Epoch [1322/2000], Loss: 0.9255\n","Epoch [1324/2000], Loss: 0.8410\n","Epoch [1326/2000], Loss: 0.6553\n","Epoch [1328/2000], Loss: 0.9801\n","Epoch [1330/2000], Loss: 0.9792\n","Epoch [1332/2000], Loss: 1.0379\n","Epoch [1334/2000], Loss: 0.9667\n","Epoch [1336/2000], Loss: 0.8329\n","Epoch [1338/2000], Loss: 0.8376\n","Epoch [1340/2000], Loss: 0.7847\n","Epoch [1342/2000], Loss: 0.9304\n","Epoch [1344/2000], Loss: 1.0444\n","Epoch [1346/2000], Loss: 0.9833\n","Epoch [1348/2000], Loss: 0.9471\n","Epoch [1350/2000], Loss: 0.9958\n","Epoch [1352/2000], Loss: 0.9372\n","Epoch [1354/2000], Loss: 0.8437\n","Epoch [1356/2000], Loss: 0.9045\n","Epoch [1358/2000], Loss: 0.8206\n","Epoch [1360/2000], Loss: 1.0689\n","Epoch [1362/2000], Loss: 1.1217\n","Epoch [1364/2000], Loss: 0.8302\n","Epoch [1366/2000], Loss: 0.9315\n","Epoch [1368/2000], Loss: 0.9762\n","Epoch [1370/2000], Loss: 0.9424\n","Epoch [1372/2000], Loss: 1.0647\n","Epoch [1374/2000], Loss: 0.8977\n","Epoch [1376/2000], Loss: 0.7664\n","Epoch [1378/2000], Loss: 1.0322\n","Epoch [1380/2000], Loss: 1.0394\n","Epoch [1382/2000], Loss: 0.6923\n","Epoch [1384/2000], Loss: 1.0027\n","Epoch [1386/2000], Loss: 1.0970\n","Epoch [1388/2000], Loss: 0.8564\n","Epoch [1390/2000], Loss: 0.8690\n","Epoch [1392/2000], Loss: 0.9336\n","Epoch [1394/2000], Loss: 0.7883\n","Epoch [1396/2000], Loss: 0.9771\n","Epoch [1398/2000], Loss: 0.7604\n","Epoch [1400/2000], Loss: 0.9538\n","Epoch [1402/2000], Loss: 0.7716\n","Epoch [1404/2000], Loss: 1.1466\n","Epoch [1406/2000], Loss: 1.0529\n","Epoch [1408/2000], Loss: 0.9742\n","Epoch [1410/2000], Loss: 0.7746\n","Epoch [1412/2000], Loss: 0.7936\n","Epoch [1414/2000], Loss: 0.9035\n","Epoch [1416/2000], Loss: 0.8545\n","Epoch [1418/2000], Loss: 0.7364\n","Epoch [1420/2000], Loss: 1.0150\n","Epoch [1422/2000], Loss: 1.0106\n","Epoch [1424/2000], Loss: 0.8447\n","Epoch [1426/2000], Loss: 0.7879\n","Epoch [1428/2000], Loss: 1.2594\n","Epoch [1430/2000], Loss: 0.9844\n","Epoch [1432/2000], Loss: 0.7586\n","Epoch [1434/2000], Loss: 0.8602\n","Epoch [1436/2000], Loss: 0.8652\n","Epoch [1438/2000], Loss: 1.0087\n","Epoch [1440/2000], Loss: 0.9066\n","Epoch [1442/2000], Loss: 0.7893\n","Epoch [1444/2000], Loss: 0.9756\n","Epoch [1446/2000], Loss: 0.8251\n","Epoch [1448/2000], Loss: 1.0437\n","Epoch [1450/2000], Loss: 0.6610\n","Epoch [1452/2000], Loss: 0.8832\n","Epoch [1454/2000], Loss: 0.9095\n","Epoch [1456/2000], Loss: 0.7695\n","Epoch [1458/2000], Loss: 0.9006\n","Epoch [1460/2000], Loss: 0.8123\n","Epoch [1462/2000], Loss: 0.9876\n","Epoch [1464/2000], Loss: 1.1688\n","Epoch [1466/2000], Loss: 0.8344\n","Epoch [1468/2000], Loss: 1.1201\n","Epoch [1470/2000], Loss: 0.9691\n","Epoch [1472/2000], Loss: 0.9770\n","Epoch [1474/2000], Loss: 0.8798\n","Epoch [1476/2000], Loss: 0.9332\n","Epoch [1478/2000], Loss: 0.8756\n","Epoch [1480/2000], Loss: 0.7326\n","Epoch [1482/2000], Loss: 1.0588\n","Epoch [1484/2000], Loss: 1.0975\n","Epoch [1486/2000], Loss: 1.0953\n","Epoch [1488/2000], Loss: 0.9715\n","Epoch [1490/2000], Loss: 0.9399\n","Epoch [1492/2000], Loss: 0.7027\n","Epoch [1494/2000], Loss: 0.5758\n","Epoch [1496/2000], Loss: 0.8554\n","Epoch [1498/2000], Loss: 0.7229\n","Epoch [1500/2000], Loss: 0.9656\n","Epoch [1502/2000], Loss: 0.7279\n","Epoch [1504/2000], Loss: 0.8974\n","Epoch [1506/2000], Loss: 0.9649\n","Epoch [1508/2000], Loss: 0.8728\n","Epoch [1510/2000], Loss: 0.9140\n","Epoch [1512/2000], Loss: 0.8946\n","Epoch [1514/2000], Loss: 0.6250\n","Epoch [1516/2000], Loss: 0.8975\n","Epoch [1518/2000], Loss: 0.7597\n","Epoch [1520/2000], Loss: 0.7921\n","Epoch [1522/2000], Loss: 0.8055\n","Epoch [1524/2000], Loss: 0.5922\n","Epoch [1526/2000], Loss: 1.1022\n","Epoch [1528/2000], Loss: 1.1242\n","Epoch [1530/2000], Loss: 0.9161\n","Epoch [1532/2000], Loss: 0.8028\n","Epoch [1534/2000], Loss: 0.5847\n","Epoch [1536/2000], Loss: 0.6951\n","Epoch [1538/2000], Loss: 0.7337\n","Epoch [1540/2000], Loss: 0.6770\n","Epoch [1542/2000], Loss: 0.9866\n","Epoch [1544/2000], Loss: 0.9556\n","Epoch [1546/2000], Loss: 0.8171\n","Epoch [1548/2000], Loss: 0.8731\n","Epoch [1550/2000], Loss: 0.9286\n","Epoch [1552/2000], Loss: 0.7566\n","Epoch [1554/2000], Loss: 1.0751\n","Epoch [1556/2000], Loss: 0.9405\n","Epoch [1558/2000], Loss: 0.9627\n","Epoch [1560/2000], Loss: 0.8806\n","Epoch [1562/2000], Loss: 0.6528\n","Epoch [1564/2000], Loss: 0.7425\n","Epoch [1566/2000], Loss: 0.7231\n","Epoch [1568/2000], Loss: 0.8746\n","Epoch [1570/2000], Loss: 0.8837\n","Epoch [1572/2000], Loss: 1.0502\n","Epoch [1574/2000], Loss: 0.9751\n","Epoch [1576/2000], Loss: 0.8119\n","Epoch [1578/2000], Loss: 0.8334\n","Epoch [1580/2000], Loss: 0.7967\n","Epoch [1582/2000], Loss: 0.7332\n","Epoch [1584/2000], Loss: 0.9945\n","Epoch [1586/2000], Loss: 0.8469\n","Epoch [1588/2000], Loss: 0.6192\n","Epoch [1590/2000], Loss: 0.8004\n","Epoch [1592/2000], Loss: 0.8794\n","Epoch [1594/2000], Loss: 0.6465\n","Epoch [1596/2000], Loss: 0.7110\n","Epoch [1598/2000], Loss: 0.9069\n","Epoch [1600/2000], Loss: 1.0249\n","Epoch [1602/2000], Loss: 0.7177\n","Epoch [1604/2000], Loss: 0.7270\n","Epoch [1606/2000], Loss: 0.8906\n","Epoch [1608/2000], Loss: 0.7584\n","Epoch [1610/2000], Loss: 1.0122\n","Epoch [1612/2000], Loss: 0.7496\n","Epoch [1614/2000], Loss: 1.0709\n","Epoch [1616/2000], Loss: 0.7357\n","Epoch [1618/2000], Loss: 0.9972\n","Epoch [1620/2000], Loss: 0.7541\n","Epoch [1622/2000], Loss: 0.8795\n","Epoch [1624/2000], Loss: 0.8995\n","Epoch [1626/2000], Loss: 0.7977\n","Epoch [1628/2000], Loss: 0.9824\n","Epoch [1630/2000], Loss: 0.9196\n","Epoch [1632/2000], Loss: 0.9376\n","Epoch [1634/2000], Loss: 0.8524\n","Epoch [1636/2000], Loss: 1.0766\n","Epoch [1638/2000], Loss: 0.8809\n","Epoch [1640/2000], Loss: 0.7775\n","Epoch [1642/2000], Loss: 1.0158\n","Epoch [1644/2000], Loss: 0.7231\n","Epoch [1646/2000], Loss: 0.9476\n","Epoch [1648/2000], Loss: 0.7725\n","Epoch [1650/2000], Loss: 0.9197\n","Epoch [1652/2000], Loss: 1.1703\n","Epoch [1654/2000], Loss: 0.7211\n","Epoch [1656/2000], Loss: 0.9414\n","Epoch [1658/2000], Loss: 0.8913\n","Epoch [1660/2000], Loss: 1.0176\n","Epoch [1662/2000], Loss: 0.5518\n","Epoch [1664/2000], Loss: 0.8786\n","Epoch [1666/2000], Loss: 0.8616\n","Epoch [1668/2000], Loss: 0.7121\n","Epoch [1670/2000], Loss: 0.6965\n","Epoch [1672/2000], Loss: 0.9972\n","Epoch [1674/2000], Loss: 0.8699\n","Epoch [1676/2000], Loss: 0.9109\n","Epoch [1678/2000], Loss: 0.9689\n","Epoch [1680/2000], Loss: 0.6297\n","Epoch [1682/2000], Loss: 0.9852\n","Epoch [1684/2000], Loss: 0.9394\n","Epoch [1686/2000], Loss: 1.1158\n","Epoch [1688/2000], Loss: 0.9210\n","Epoch [1690/2000], Loss: 0.6177\n","Epoch [1692/2000], Loss: 0.8461\n","Epoch [1694/2000], Loss: 0.9862\n","Epoch [1696/2000], Loss: 1.0271\n","Epoch [1698/2000], Loss: 1.2665\n","Epoch [1700/2000], Loss: 0.7160\n","Epoch [1702/2000], Loss: 0.7653\n","Epoch [1704/2000], Loss: 0.6837\n","Epoch [1706/2000], Loss: 1.0143\n","Epoch [1708/2000], Loss: 0.7760\n","Epoch [1710/2000], Loss: 0.9623\n","Epoch [1712/2000], Loss: 0.8983\n","Epoch [1714/2000], Loss: 0.9191\n","Epoch [1716/2000], Loss: 0.9144\n","Epoch [1718/2000], Loss: 0.9777\n","Epoch [1720/2000], Loss: 0.7398\n","Epoch [1722/2000], Loss: 0.8351\n","Epoch [1724/2000], Loss: 1.0508\n","Epoch [1726/2000], Loss: 1.0123\n","Epoch [1728/2000], Loss: 1.0231\n","Epoch [1730/2000], Loss: 0.7701\n","Epoch [1732/2000], Loss: 0.7714\n","Epoch [1734/2000], Loss: 0.8441\n","Epoch [1736/2000], Loss: 0.6260\n","Epoch [1738/2000], Loss: 0.8797\n","Epoch [1740/2000], Loss: 0.7592\n","Epoch [1742/2000], Loss: 0.6560\n","Epoch [1744/2000], Loss: 0.6131\n","Epoch [1746/2000], Loss: 0.8200\n","Epoch [1748/2000], Loss: 0.7578\n","Epoch [1750/2000], Loss: 0.7879\n","Epoch [1752/2000], Loss: 0.7136\n","Epoch [1754/2000], Loss: 0.9425\n","Epoch [1756/2000], Loss: 0.9488\n","Epoch [1758/2000], Loss: 0.6757\n","Epoch [1760/2000], Loss: 0.7800\n","Epoch [1762/2000], Loss: 0.9756\n","Epoch [1764/2000], Loss: 0.8175\n","Epoch [1766/2000], Loss: 0.7326\n","Epoch [1768/2000], Loss: 0.9148\n","Epoch [1770/2000], Loss: 0.7748\n","Epoch [1772/2000], Loss: 0.8729\n","Epoch [1774/2000], Loss: 0.9830\n","Epoch [1776/2000], Loss: 0.9359\n","Epoch [1778/2000], Loss: 0.9057\n","Epoch [1780/2000], Loss: 0.9069\n","Epoch [1782/2000], Loss: 0.6530\n","Epoch [1784/2000], Loss: 0.8373\n","Epoch [1786/2000], Loss: 0.5977\n","Epoch [1788/2000], Loss: 0.8899\n","Epoch [1790/2000], Loss: 0.8048\n","Epoch [1792/2000], Loss: 0.9114\n","Epoch [1794/2000], Loss: 0.7029\n","Epoch [1796/2000], Loss: 0.8445\n","Epoch [1798/2000], Loss: 0.8852\n","Epoch [1800/2000], Loss: 0.5751\n","Epoch [1802/2000], Loss: 0.9592\n","Epoch [1804/2000], Loss: 0.8641\n","Epoch [1806/2000], Loss: 0.7264\n","Epoch [1808/2000], Loss: 0.7100\n","Epoch [1810/2000], Loss: 0.7140\n","Epoch [1812/2000], Loss: 0.8357\n","Epoch [1814/2000], Loss: 0.7550\n","Epoch [1816/2000], Loss: 0.8811\n","Epoch [1818/2000], Loss: 0.8208\n","Epoch [1820/2000], Loss: 0.7363\n","Epoch [1822/2000], Loss: 0.8304\n","Epoch [1824/2000], Loss: 0.8894\n","Epoch [1826/2000], Loss: 0.8276\n","Epoch [1828/2000], Loss: 0.7584\n","Epoch [1830/2000], Loss: 0.7255\n","Epoch [1832/2000], Loss: 0.9333\n","Epoch [1834/2000], Loss: 0.9943\n","Epoch [1836/2000], Loss: 0.8552\n","Epoch [1838/2000], Loss: 0.7953\n","Epoch [1840/2000], Loss: 1.0109\n","Epoch [1842/2000], Loss: 0.7437\n","Epoch [1844/2000], Loss: 0.8446\n","Epoch [1846/2000], Loss: 0.9858\n","Epoch [1848/2000], Loss: 1.1912\n","Epoch [1850/2000], Loss: 0.9096\n","Epoch [1852/2000], Loss: 0.7105\n","Epoch [1854/2000], Loss: 0.6873\n","Epoch [1856/2000], Loss: 1.0179\n","Epoch [1858/2000], Loss: 0.7649\n","Epoch [1860/2000], Loss: 0.9854\n","Epoch [1862/2000], Loss: 1.0193\n","Epoch [1864/2000], Loss: 0.7901\n","Epoch [1866/2000], Loss: 0.7298\n","Epoch [1868/2000], Loss: 0.7139\n","Epoch [1870/2000], Loss: 0.8557\n","Epoch [1872/2000], Loss: 0.8541\n","Epoch [1874/2000], Loss: 0.9292\n","Epoch [1876/2000], Loss: 0.6905\n","Epoch [1878/2000], Loss: 0.8368\n","Epoch [1880/2000], Loss: 0.8079\n","Epoch [1882/2000], Loss: 0.8158\n","Epoch [1884/2000], Loss: 0.7334\n","Epoch [1886/2000], Loss: 1.0915\n","Epoch [1888/2000], Loss: 0.9555\n","Epoch [1890/2000], Loss: 0.8441\n","Epoch [1892/2000], Loss: 0.9511\n","Epoch [1894/2000], Loss: 1.0426\n","Epoch [1896/2000], Loss: 0.9193\n","Epoch [1898/2000], Loss: 1.0134\n","Epoch [1900/2000], Loss: 0.7615\n","Epoch [1902/2000], Loss: 0.6268\n","Epoch [1904/2000], Loss: 0.8225\n","Epoch [1906/2000], Loss: 0.9310\n","Epoch [1908/2000], Loss: 1.0078\n","Epoch [1910/2000], Loss: 0.9126\n","Epoch [1912/2000], Loss: 0.6792\n","Epoch [1914/2000], Loss: 0.7471\n","Epoch [1916/2000], Loss: 0.8286\n","Epoch [1918/2000], Loss: 0.6101\n","Epoch [1920/2000], Loss: 0.8283\n","Epoch [1922/2000], Loss: 0.7441\n","Epoch [1924/2000], Loss: 0.8751\n","Epoch [1926/2000], Loss: 0.7007\n","Epoch [1928/2000], Loss: 0.8518\n","Epoch [1930/2000], Loss: 0.7762\n","Epoch [1932/2000], Loss: 0.7764\n","Epoch [1934/2000], Loss: 0.7401\n","Epoch [1936/2000], Loss: 0.8077\n","Epoch [1938/2000], Loss: 0.7769\n","Epoch [1940/2000], Loss: 0.8272\n","Epoch [1942/2000], Loss: 0.8436\n","Epoch [1944/2000], Loss: 0.7298\n","Epoch [1946/2000], Loss: 0.6880\n","Epoch [1948/2000], Loss: 0.9364\n","Epoch [1950/2000], Loss: 0.9950\n","Epoch [1952/2000], Loss: 0.8306\n","Epoch [1954/2000], Loss: 1.0773\n","Epoch [1956/2000], Loss: 0.8539\n","Epoch [1958/2000], Loss: 0.8126\n","Epoch [1960/2000], Loss: 0.7108\n","Epoch [1962/2000], Loss: 1.0049\n","Epoch [1964/2000], Loss: 0.9022\n","Epoch [1966/2000], Loss: 0.7853\n","Epoch [1968/2000], Loss: 0.8023\n","Epoch [1970/2000], Loss: 0.6539\n","Epoch [1972/2000], Loss: 0.5816\n","Epoch [1974/2000], Loss: 0.5578\n","Epoch [1976/2000], Loss: 0.9138\n","Epoch [1978/2000], Loss: 0.8278\n","Epoch [1980/2000], Loss: 0.9492\n","Epoch [1982/2000], Loss: 0.6526\n","Epoch [1984/2000], Loss: 0.9091\n","Epoch [1986/2000], Loss: 0.9621\n","Epoch [1988/2000], Loss: 0.7194\n","Epoch [1990/2000], Loss: 0.8433\n","Epoch [1992/2000], Loss: 0.9215\n","Epoch [1994/2000], Loss: 0.8928\n","Epoch [1996/2000], Loss: 0.7324\n","Epoch [1998/2000], Loss: 0.8046\n","Epoch [2000/2000], Loss: 1.0025\n","Epoch [2/2000], Loss: 1.0673\n","Epoch [4/2000], Loss: 1.0062\n","Epoch [6/2000], Loss: 1.0689\n","Epoch [8/2000], Loss: 0.9733\n","Epoch [10/2000], Loss: 0.9334\n","Epoch [12/2000], Loss: 1.0567\n","Epoch [14/2000], Loss: 0.8094\n","Epoch [16/2000], Loss: 0.8747\n","Epoch [18/2000], Loss: 1.2220\n","Epoch [20/2000], Loss: 1.3139\n","Epoch [22/2000], Loss: 1.1802\n","Epoch [24/2000], Loss: 1.3727\n","Epoch [26/2000], Loss: 1.1071\n","Epoch [28/2000], Loss: 1.1558\n","Epoch [30/2000], Loss: 1.0295\n","Epoch [32/2000], Loss: 0.9962\n","Epoch [34/2000], Loss: 0.8694\n","Epoch [36/2000], Loss: 1.1227\n","Epoch [38/2000], Loss: 1.0459\n","Epoch [40/2000], Loss: 0.9824\n","Epoch [42/2000], Loss: 1.2332\n","patience exceeded, loading best model\n","Epoch [2/2000], Loss: 1.0326\n","Epoch [4/2000], Loss: 0.8315\n","Epoch [6/2000], Loss: 1.1783\n","Epoch [8/2000], Loss: 1.0970\n","Epoch [10/2000], Loss: 1.1880\n","Epoch [12/2000], Loss: 1.0682\n","Epoch [14/2000], Loss: 1.2120\n","Epoch [16/2000], Loss: 0.9096\n","Epoch [18/2000], Loss: 1.1159\n","Epoch [20/2000], Loss: 1.0869\n","Epoch [22/2000], Loss: 0.9861\n","Epoch [24/2000], Loss: 1.0860\n","Epoch [26/2000], Loss: 1.3199\n","Epoch [28/2000], Loss: 0.7331\n","Epoch [30/2000], Loss: 1.0060\n","Epoch [32/2000], Loss: 0.8363\n","Epoch [34/2000], Loss: 0.9938\n","Epoch [36/2000], Loss: 1.0614\n","Epoch [38/2000], Loss: 1.2154\n","Epoch [40/2000], Loss: 1.1834\n","Epoch [42/2000], Loss: 1.2240\n","Epoch [44/2000], Loss: 1.2478\n","Epoch [46/2000], Loss: 1.4020\n","Epoch [48/2000], Loss: 1.2133\n","Epoch [50/2000], Loss: 1.0357\n","Epoch [52/2000], Loss: 1.2717\n","Epoch [54/2000], Loss: 0.9225\n","Epoch [56/2000], Loss: 1.0142\n","Epoch [58/2000], Loss: 0.9987\n","Epoch [60/2000], Loss: 0.7920\n","Epoch [62/2000], Loss: 0.8010\n","Epoch [64/2000], Loss: 1.2165\n","Epoch [66/2000], Loss: 1.1987\n","Epoch [68/2000], Loss: 1.1125\n","Epoch [70/2000], Loss: 0.8994\n","Epoch [72/2000], Loss: 1.2896\n","Epoch [74/2000], Loss: 1.1098\n","Epoch [76/2000], Loss: 1.0074\n","Epoch [78/2000], Loss: 1.0656\n","Epoch [80/2000], Loss: 0.9016\n","Epoch [82/2000], Loss: 1.1992\n","Epoch [84/2000], Loss: 0.8666\n","Epoch [86/2000], Loss: 0.9500\n","Epoch [88/2000], Loss: 0.8399\n","Epoch [90/2000], Loss: 0.9160\n","Epoch [92/2000], Loss: 1.0920\n","Epoch [94/2000], Loss: 1.0367\n","Epoch [96/2000], Loss: 0.9275\n","Epoch [98/2000], Loss: 1.2302\n","Epoch [100/2000], Loss: 1.2576\n","Epoch [102/2000], Loss: 1.1960\n","Epoch [104/2000], Loss: 0.9660\n","Epoch [106/2000], Loss: 0.8158\n","Epoch [108/2000], Loss: 1.0288\n","Epoch [110/2000], Loss: 1.0549\n","Epoch [112/2000], Loss: 0.8452\n","Epoch [114/2000], Loss: 1.2256\n","Epoch [116/2000], Loss: 0.8683\n","Epoch [118/2000], Loss: 0.9846\n","Epoch [120/2000], Loss: 0.8884\n","Epoch [122/2000], Loss: 1.1372\n","Epoch [124/2000], Loss: 0.9899\n","Epoch [126/2000], Loss: 1.1041\n","Epoch [128/2000], Loss: 0.8682\n","Epoch [130/2000], Loss: 0.9919\n","Epoch [132/2000], Loss: 1.0466\n","Epoch [134/2000], Loss: 1.1248\n","Epoch [136/2000], Loss: 1.1234\n","Epoch [138/2000], Loss: 0.7875\n","Epoch [140/2000], Loss: 1.1451\n","Epoch [142/2000], Loss: 1.0610\n","Epoch [144/2000], Loss: 1.0282\n","Epoch [146/2000], Loss: 0.7972\n","Epoch [148/2000], Loss: 0.8565\n","Epoch [150/2000], Loss: 0.9882\n","Epoch [152/2000], Loss: 1.0635\n","Epoch [154/2000], Loss: 1.0381\n","Epoch [156/2000], Loss: 1.0041\n","Epoch [158/2000], Loss: 0.9184\n","Epoch [160/2000], Loss: 1.3273\n","Epoch [162/2000], Loss: 1.2739\n","Epoch [164/2000], Loss: 0.8256\n","Epoch [166/2000], Loss: 0.9531\n","Epoch [168/2000], Loss: 1.0074\n","Epoch [170/2000], Loss: 1.0992\n","Epoch [172/2000], Loss: 0.8571\n","Epoch [174/2000], Loss: 0.9273\n","Epoch [176/2000], Loss: 0.7780\n","Epoch [178/2000], Loss: 0.9372\n","Epoch [180/2000], Loss: 0.8738\n","patience exceeded, loading best model\n","Average accuracy: 0.99592906\n","Average top_k_average_accuracies 1.0465546\n","KNN accuracy: 1.0793874\n"]}],"source":["best_accuracies = []\n","accuracies = []\n","top_k_average_accuracies = []\n","knn_accuracies = []\n","PATH = os.path.join(folder_name, f'checkpoints/regression_{dataset_name}.h5')\n","cfg.PATH = PATH\n","k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n","\n","\n","for train_index, test_index in k_fold.split(Xs):\n","  # Get training and testing data\n","  X_train, X_test = Xs[train_index], Xs[test_index]\n","  y_train, y_test = ys[train_index], ys[test_index]\n","\n","  knn = KNeighborsRegressor(n_neighbors=cfg.top_k)\n","  knn.fit(X_train, y_train)\n","  knn_accuracies.append(mean_squared_error(knn.predict(X_test), y_test))\n","\n","  best_accuracy, accuracy, top_k_average_accuracy, model= train_reg(X_train, y_train, X_test, y_test, cfg)\n","  best_accuracies.append(best_accuracy)\n","  accuracies.append(accuracy)\n","  top_k_average_accuracies.append(top_k_average_accuracy)\n","  # break\n","\n","print(\"Average accuracy:\", np.mean([acc.detach().numpy() for acc in accuracies]))\n","print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n","print(\"KNN accuracy:\", np.mean(knn_accuracies))"]},{"cell_type":"code","source":["print(\"Average accuracy:\", np.mean([acc.detach().numpy() for acc in accuracies]))\n","print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n","print(\"KNN accuracy:\", np.mean(knn_accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9m_Yhd0SOmiV","executionInfo":{"status":"ok","timestamp":1719034973070,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"4151e1cf-e430-4e7d-e8cb-6d4f38f7910a"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy: 0.99592906\n","Average top_k_average_accuracies 1.0465546\n","KNN accuracy: 1.0793874\n"]}]},{"cell_type":"markdown","metadata":{"id":"aQMTPXLwaBVq"},"source":["# Results Interpretation"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"sf23mP1UaIvx","executionInfo":{"status":"ok","timestamp":1719034973070,"user_tz":-480,"elapsed":3,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["def print_model_features(input_model):\n","  for n, p in model.named_parameters():\n","    print(n)\n","    print(p.data)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"MRO3tUPEbJUs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719034975004,"user_tz":-480,"elapsed":1936,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"df793dc6-6ac6-41bc-f58e-5411db38a985"},"outputs":[{"output_type":"stream","name":"stdout","text":["fa_layer.f1weight\n","tensor([1.0646, 0.9778, 0.9870, 0.9357, 1.0788, 1.0091, 0.9982, 1.1459, 1.0635,\n","        1.0747, 1.1307, 1.0958, 0.9828, 1.1748, 1.0513, 1.1137, 1.0952, 1.1366,\n","        1.0851, 1.1076, 1.0556, 1.0433, 1.1043, 1.1563, 1.0348, 1.0374, 0.9015,\n","        1.1102, 1.1140, 0.9573, 1.0733, 1.1348, 1.1363, 1.0109, 1.0304, 1.1578,\n","        1.1615, 1.0784, 1.1400, 1.1086, 0.9542, 1.0675, 1.1521, 1.0411, 1.0000,\n","        1.0167, 1.0000, 1.1669, 1.1556, 1.1200, 1.0148, 1.1048, 0.9978, 0.9632,\n","        1.1496, 1.0648, 0.9445, 1.0137, 1.0515, 1.1217, 1.0101, 1.1168, 0.9894,\n","        1.1180, 1.1682, 0.9748, 1.0030, 1.1053, 1.0726, 1.1410, 1.1338, 1.0042,\n","        0.9760, 1.0885, 0.9990, 0.9267, 0.9480, 0.9499, 1.0616, 1.0000, 1.0046,\n","        1.0000, 1.0028, 0.9786, 1.1039, 1.0028, 1.0764, 1.1090, 1.0560, 1.0780,\n","        1.0275, 1.0775, 1.0058, 1.0164, 0.9759, 0.9627, 1.0177, 0.9365, 1.0740,\n","        0.9542, 1.1459, 1.1218, 1.0299, 1.0387, 1.0700, 1.1087, 1.0000, 0.9476,\n","        1.0000, 1.0256, 1.0123, 0.9877, 0.9925, 1.0963, 1.0975, 0.9301, 1.0203,\n","        1.0690, 0.9708, 0.9951, 1.0458, 1.1095, 0.9458, 1.0949, 1.1370, 1.2063,\n","        1.0000, 0.9850, 1.0503, 1.0726, 1.0736, 1.0456, 1.0582, 1.0135, 1.1634,\n","        1.1826, 1.0325, 0.9981, 0.9976, 1.1025, 1.1422, 1.0729, 0.9350, 1.0000,\n","        1.0171, 0.9438, 1.0213, 1.0244, 1.0561, 0.9997, 1.0852, 1.0569, 0.9727,\n","        1.0128, 1.1193, 1.1852, 0.9409], device='cuda:0')\n","ca_layer.fa_weight\n","tensor([1.0646, 0.9778, 0.9870, 0.9357, 1.0788, 1.0091, 0.9982, 1.1459, 1.0635,\n","        1.0746, 1.1307, 1.0958, 0.9828, 1.1748, 1.0513, 1.1137, 1.0952, 1.1366,\n","        1.0851, 1.1076, 1.0556, 1.0433, 1.1042, 1.1563, 1.0348, 1.0374, 0.9015,\n","        1.1101, 1.1140, 0.9573, 1.0733, 1.1348, 1.1363, 1.0109, 1.0304, 1.1578,\n","        1.1615, 1.0784, 1.1399, 1.1086, 0.9542, 1.0675, 1.1521, 1.0411, 1.0000,\n","        1.0166, 1.0000, 1.1669, 1.1556, 1.1200, 1.0148, 1.1048, 0.9978, 0.9632,\n","        1.1496, 1.0648, 0.9445, 1.0137, 1.0515, 1.1217, 1.0101, 1.1168, 0.9894,\n","        1.1180, 1.1682, 0.9748, 1.0030, 1.1053, 1.0726, 1.1409, 1.1338, 1.0041,\n","        0.9760, 1.0885, 0.9990, 0.9267, 0.9480, 0.9499, 1.0616, 1.0000, 1.0046,\n","        1.0000, 1.0028, 0.9786, 1.1039, 1.0028, 1.0764, 1.1090, 1.0560, 1.0780,\n","        1.0275, 1.0775, 1.0058, 1.0164, 0.9759, 0.9627, 1.0177, 0.9366, 1.0740,\n","        0.9542, 1.1459, 1.1218, 1.0299, 1.0387, 1.0700, 1.1087, 1.0000, 0.9476,\n","        1.0000, 1.0256, 1.0123, 0.9877, 0.9925, 1.0963, 1.0975, 0.9301, 1.0203,\n","        1.0690, 0.9708, 0.9951, 1.0458, 1.1095, 0.9459, 1.0949, 1.1370, 1.2063,\n","        1.0000, 0.9850, 1.0503, 1.0726, 1.0736, 1.0456, 1.0581, 1.0135, 1.1634,\n","        1.1826, 1.0325, 0.9981, 0.9976, 1.1025, 1.1422, 1.0729, 0.9350, 1.0000,\n","        1.0171, 0.9438, 1.0213, 1.0244, 1.0561, 0.9997, 1.0851, 1.0569, 0.9728,\n","        1.0128, 1.1193, 1.1852, 0.9409], device='cuda:0')\n","ca_layer.bias\n","tensor([4.0126, 3.8785, 3.9396, 3.9220, 3.9437, 3.9914, 3.9203, 3.9539, 3.9408,\n","        3.9869, 3.9780, 3.9268, 3.8438, 3.9307, 3.8674, 3.9883, 3.8794, 3.8606,\n","        4.0368, 3.8509, 3.9470, 3.9070, 3.9529, 3.9759, 3.8962, 3.9301, 3.9320,\n","        3.9131, 3.9549, 3.8331, 3.9540, 3.9106, 3.8573, 3.9766, 3.8622, 3.8900,\n","        3.8915, 3.9910, 3.8800, 3.9682, 3.9760, 3.9859, 3.8867, 3.8422, 3.9479,\n","        4.0362, 3.8933, 3.9992, 3.8990, 3.8684, 3.8781, 3.9870, 3.8983, 3.8670,\n","        3.9496, 3.8746, 3.9004, 3.8709, 3.9636, 3.9050, 3.8582, 3.9203, 3.9144,\n","        3.8459, 3.9596, 4.0219, 3.8866, 3.8679, 3.8997, 3.8810, 3.8795, 3.9108,\n","        3.9472, 3.8732, 3.9250, 3.8736, 3.8712, 3.8962, 3.9134, 3.9780, 3.8476,\n","        3.8552, 3.8609, 3.9962, 3.9413, 3.9049, 3.9661, 3.8616, 3.9310, 3.9267,\n","        3.9184, 3.9333, 3.8797, 3.8812, 3.9055, 3.9281, 3.8967, 3.9628, 3.9921,\n","        3.9879, 3.8980, 3.9384, 3.8581, 3.9139, 3.8665, 3.8803, 3.9807, 3.9927,\n","        3.8483, 3.8960, 3.9426, 3.8849, 3.9674, 3.8445, 3.9634, 3.9582, 3.9955,\n","        3.8508, 4.0050, 3.8243, 3.8984, 3.9662, 3.9963, 3.9940, 3.9755, 3.9908,\n","        3.8659, 3.9250, 3.8926, 3.9250, 3.8731, 3.8864, 3.9490, 3.9250, 3.9340,\n","        3.9602, 3.9734, 3.8967, 3.9702, 3.9107, 3.8986, 3.9301, 3.8532, 3.8862,\n","        3.8623, 4.0029, 3.8889, 3.9082, 3.8876, 3.9598, 3.9840, 3.9362, 3.9069,\n","        3.9699, 3.9560, 3.8714, 3.9627, 3.8661, 3.8857, 3.8444, 4.0033, 3.9071,\n","        3.9301, 3.8720, 3.8755, 3.8451, 4.0114, 3.9723, 3.9250, 3.9133, 3.9915,\n","        3.9980, 3.8949, 3.9244, 3.9989, 3.9302, 3.9761, 3.9835, 3.9655, 3.9391,\n","        3.9796, 3.8824, 3.8696, 3.8696, 3.9851, 3.9670, 3.9449, 3.9833, 3.9277,\n","        3.9063, 3.8981, 3.9806, 3.9091, 3.9296, 3.8943, 3.8430, 3.8576, 3.8853,\n","        3.9032, 3.9193, 3.9940, 3.9959, 3.9380, 3.8988, 3.8455, 3.9624, 3.8737,\n","        3.9119, 3.9000, 3.9190, 3.9187, 3.9610, 3.9030, 3.9393, 3.8971, 3.9250,\n","        3.8744, 3.9027, 3.9952, 3.9945, 3.9327, 3.8894, 3.9285, 3.9584, 3.9440,\n","        3.9878, 3.8691, 3.9864, 3.8746, 3.9601, 3.9649, 3.8873, 3.8691, 3.9189,\n","        3.8681, 3.9250, 3.9788, 3.8837, 3.9692, 3.8786, 3.9925, 3.8736, 3.8656,\n","        3.8803, 3.8665, 3.8669, 3.8791, 3.9049, 3.9754, 3.8740, 3.8888, 3.9427,\n","        3.8743, 3.9860, 3.9810, 3.9433, 3.8622, 3.9044, 4.0059, 3.9201, 3.9163,\n","        3.8628, 3.9646, 3.8984, 3.8972, 3.9275, 3.9565, 3.8835, 3.9808, 3.9773,\n","        3.9792, 3.9347, 3.9466, 3.8702, 3.8707, 3.8568, 3.9638, 3.8987, 3.9038,\n","        3.9419, 3.9303, 3.9303, 4.0029, 3.9424, 3.9269, 3.8725, 3.9239, 3.9622,\n","        3.9508, 3.8644, 3.9290, 3.8605, 3.9690, 3.8704, 3.9641, 3.9704, 3.8748,\n","        3.9454, 3.9789, 3.9814, 3.9250, 3.8856, 3.9387, 3.9082, 3.9717, 3.8560,\n","        3.8767, 3.8868, 3.8753, 3.9787, 3.8749, 3.9096, 3.9248, 3.8966, 3.8711,\n","        3.9112, 3.9426, 3.9356, 3.9730, 3.8737, 3.9720, 3.9309, 3.9729, 3.8701,\n","        3.9277, 3.8731, 3.8626, 3.9371, 3.9124, 3.8891, 3.9669, 3.8559, 3.9484,\n","        3.9217, 3.9310, 4.0001, 3.9194, 3.9966, 4.0381, 3.9612, 3.9121, 3.9536,\n","        3.9536, 3.8608, 3.8991, 3.9377, 3.9765, 3.8697, 3.9608, 3.9411, 3.9913,\n","        3.9570, 3.9781, 3.9916, 3.9449, 3.9465, 3.9140, 3.8828, 3.9000, 3.9772,\n","        3.9169, 3.8943, 3.8704, 4.0030, 3.8936, 3.9905, 3.8864, 3.9231, 3.8621,\n","        3.9070, 3.8447, 3.9389, 3.8979, 3.8693, 3.8854, 4.0038, 3.8427, 3.8417,\n","        3.9598, 3.9776, 3.8615, 3.9979, 3.9927, 3.8831, 3.8823, 3.8924, 3.9179,\n","        3.8733, 3.9736, 3.9888, 3.8876, 3.9448, 3.9139, 3.9891, 3.9741, 3.8846,\n","        3.9250, 3.9190, 3.9705, 3.9543, 3.8346, 3.9786, 3.8471, 3.8982, 3.9936,\n","        3.8349, 3.8440, 3.8528, 3.8904, 3.9787, 3.8821, 3.9044, 3.9369, 3.8613],\n","       device='cuda:0')\n"]}],"source":["print_model_features(model)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"PkxAqSoldjML","executionInfo":{"status":"ok","timestamp":1719034975004,"user_tz":-480,"elapsed":8,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# for regression only. for classification is different\n","#feature_activations, case_activations, predicted_number\n","model.eval()\n","feature_activations, case_activations, output, predicted_class = model(X_test.to(device))"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"PtwGRyMjeXtp","executionInfo":{"status":"ok","timestamp":1719034975004,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4ba125a-ae39-47f3-e48d-64381ac5975a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 4.1465e-02,  5.3894e-02, -2.0531e-02, -8.6567e-02, -8.3351e-02,\n","         1.7569e-09, -8.1551e-02,  4.3990e-02, -4.9926e-03,  2.4224e-01,\n","        -2.6790e-02,  4.4673e-01, -8.4768e-02,  1.2664e-01,  8.0771e-02,\n","         6.5137e-02, -6.7750e-02, -1.1325e-02, -8.2301e-02, -7.8184e-02,\n","         1.0249e-01, -5.7211e-02,  7.6599e-02, -1.1510e-01, -2.7233e-02,\n","        -7.1776e-02, -8.7807e-02, -7.6512e-02,  1.8108e-01, -1.5044e-01,\n","        -7.1940e-01, -3.0080e-01,  6.7821e-02, -8.6528e-02, -6.2181e-02,\n","         1.3165e-01, -2.3978e-01, -2.1318e-02,  3.3745e-02,  2.5853e-01,\n","        -8.5523e-02,  1.6500e-01, -2.5566e-02, -1.1269e-02, -3.5682e-02,\n","        -6.0301e-03], device='cuda:0', grad_fn=<MvBackward0>)"]},"metadata":{},"execution_count":38}],"source":["predicted_class"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"EvY5_WGkeaxg","executionInfo":{"status":"ok","timestamp":1719034975005,"user_tz":-480,"elapsed":8,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fa5ab32-d6ee-4230-8b5b-26c13f3a4ef4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.4467,  0.4467, -1.3402, -1.3402,  0.4467,  0.4467, -0.4467, -1.3402,\n","        -1.3402, -1.3402, -0.4467, -1.3402,  0.4467,  1.3402,  0.4467, -0.4467,\n","        -0.4467,  1.3402,  1.3402, -0.4467,  0.4467, -1.3402,  1.3402,  1.3402,\n","         1.3402, -1.3402,  0.4467,  0.4467, -0.4467, -1.3402,  1.3402,  0.4467,\n","        -0.4467, -0.4467,  1.3402,  0.4467,  1.3402,  1.3402, -1.3402, -0.4467,\n","        -1.3402,  0.4467,  0.4467, -0.4467,  0.4467,  0.4467])"]},"metadata":{},"execution_count":39}],"source":["y_test"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"Eb7wyntlXft_","executionInfo":{"status":"error","timestamp":1719034975005,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"f5e911db-c1dc-46c5-e043-2a13b97cacc4"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-70ff8e18b811>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: accuracy comparing predicted_class and y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}],"source":["# prompt: accuracy comparing predicted_class and y_test\n","\n","accuracy = accuracy_score(y_test.numpy(), predicted_class.cpu().numpy())\n","print(\"Accuracy:\", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjaUAY7Bkjyr","executionInfo":{"status":"aborted","timestamp":1719034975005,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["#inspecting the case activations\n","top_case_indices = torch.topk(case_activations, 5, dim=1)[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfi-PKhokmev","executionInfo":{"status":"aborted","timestamp":1719034975005,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["X_test[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMxrFCywYIaQ","executionInfo":{"status":"aborted","timestamp":1719034975005,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_test[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73NQbeJ_kpO_","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["X_train[top_case_indices[0][0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9_6zvszYKPw","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_train[top_case_indices[0][0]]"]},{"cell_type":"markdown","metadata":{"id":"xMePSTR1lXbb"},"source":["By comparing the following two blocks' outputs, you can see we are retrieving a good neighbor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0izuKF6okrsc","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["#sum abs of X_test[0] and the top activated case\n","sum(abs(X_test[0] - X_train[top_case_indices[0][0]]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faYClHOVktuW","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["# prompt: average sum abs of X_test[0] and X_train data\n","print(np.mean([sum(abs(X_test[0] - X_train[i])) for i in range(len(X_train))]))"]},{"cell_type":"markdown","metadata":{"id":"zkYKvaP-lz_0"},"source":["TODO:: A better way is to show the distribution of ``X_test[0] - X_train[i]``"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrwLY3gXmCLd","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_train[top_case_indices[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"es5Kv85CmHMt","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["knn.predict(X_test)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxOcKMzHmJkS","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["indices = knn.kneighbors(X_test)[1][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4vuKW0rmNVi","executionInfo":{"status":"aborted","timestamp":1719034975006,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"outputs":[],"source":["y_train[indices]"]},{"cell_type":"markdown","source":["# Sanity Check\n"],"metadata":{"id":"uZsxNJZDQ8RQ"}},{"cell_type":"markdown","source":["## Classification Neural Network"],"metadata":{"id":"ZhHAd1y-Q_C2"}},{"cell_type":"code","source":["# Hyperparameters\n","input_size = X_train.shape[1]\n","hidden_size = 1024\n","num_classes = torch.unique(ys).shape[0]\n","learning_rate = 1e-5\n","batch_size = 16\n","epochs = 2000"],"metadata":{"id":"pXu1CDqXRR_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define the neural network architecture for classification\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.nn = nn.Sequential(\n","            nn.Linear(input_size, hidden_size ),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size , hidden_size // 2),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 2, hidden_size // 4),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 4, num_classes)\n","            )\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.fill_(0)\n","\n","    def forward(self, x):\n","        return self.nn(x)\n"],"metadata":{"id":"CBu0Gj9vRKYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_accuracies = []\n","for train_index, test_index in k_fold.split(Xs):\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n","  patience_counter = 0\n","  best_model = None\n","  best_accuracy = None\n","  # Initialize the model, loss function, and optimizer\n","  model = NeuralNet(input_size, hidden_size, num_classes)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # Training loop\n","  for epoch in range(epochs):\n","    epoch_msg = True\n","    training_total_acc = 0.0\n","    training_total_loss = 0.0\n","    num_of_batches = len(train_loader)\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      # Forward pass\n","      outputs = model(X_train_batch)\n","      loss = criterion(outputs, y_train_batch)\n","\n","      # Backward and optimize\n","      _, predicted = torch.max(outputs, 1)\n","      training_total_acc += torch.sum(predicted == y_train_batch).item()\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      training_total_loss += loss.item()\n","      # if (i + 1) % 5 == 0\n","    if epoch == 0 or (epoch + 1) % 100 == 0:\n","      print(f\"Epoch: {epoch + 1}, Training Loss: {training_total_loss/num_of_batches:.2f} Acc: {training_total_acc/num_of_batches:.2f}\")\n","    # Testing the model\n","    model.eval()\n","    with torch.no_grad():\n","      outputs = model(X_test)\n","      loss = criterion(outputs, y_test)\n","      _, predicted = torch.max(outputs, 1)\n","      accuracy = torch.sum(predicted == y_test).item() / len(y_test)\n","      print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n","      if best_accuracy is None or accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_model = model\n","        patience_counter = 0\n","      else:\n","        patience_counter += 1\n","      if epoch_msg and (epoch + 1) % 100 == 0:\n","        epoch_msg = False\n","        print(f'Epoch [{epoch + 1}/{epoch}], Test Loss: {loss.item()}')\n","    if patience_counter >= cfg.patience:\n","      print(\"Best acc achieved: \", best_accuracy)\n","      break\n","  best_accuracies.append(best_accuracy)\n","  # break\n","print(\"Average accuracy:\", np.mean(best_accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcwG0kIDRnfg","executionInfo":{"status":"ok","timestamp":1718894814418,"user_tz":-480,"elapsed":16854,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}},"outputId":"e472fa40-b629-4295-83e6-f1fb09a7143a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 1.13 Acc: 4.00\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Best acc achieved:  0.5833333333333334\n","Epoch: 1, Training Loss: 1.11 Acc: 5.00\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Best acc achieved:  0.4166666666666667\n","Epoch: 1, Training Loss: 1.12 Acc: 5.14\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Best acc achieved:  0.5833333333333334\n","Epoch: 1, Training Loss: 1.11 Acc: 5.43\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Best acc achieved:  0.5\n","Epoch: 1, Training Loss: 1.10 Acc: 5.57\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 8.33%\n","Accuracy on the test set: 8.33%\n","Accuracy on the test set: 8.33%\n","Accuracy on the test set: 8.33%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Best acc achieved:  0.5\n","Epoch: 1, Training Loss: 1.11 Acc: 3.43\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Epoch: 100, Training Loss: 0.52 Acc: 14.71\n","Accuracy on the test set: 58.33%\n","Epoch [100/99], Test Loss: 0.9404299855232239\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Best acc achieved:  0.5833333333333334\n","Epoch: 1, Training Loss: 1.12 Acc: 5.14\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Accuracy on the test set: 58.33%\n","Best acc achieved:  0.5833333333333334\n","Epoch: 1, Training Loss: 1.11 Acc: 5.57\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Accuracy on the test set: 50.00%\n","Best acc achieved:  0.5\n","Epoch: 1, Training Loss: 1.12 Acc: 5.00\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Best acc achieved:  0.4166666666666667\n","Epoch: 1, Training Loss: 1.10 Acc: 5.57\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 16.67%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 25.00%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 33.33%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Accuracy on the test set: 41.67%\n","Best acc achieved:  0.4166666666666667\n","Average accuracy: 0.5083333333333334\n"]}]},{"cell_type":"markdown","source":["## Regression Neural Network"],"metadata":{"id":"wF89uiJWRIcf"}},{"cell_type":"code","source":["# Hyperparameters\n","input_size = X_train.shape[1]\n","hidden_size = 100\n","# num_classes = torch.unique(ys).shape[0]\n","learning_rate = 1e-5\n","batch_size = 16\n","epochs = 2000"],"metadata":{"id":"SUqIgNipWaOk","executionInfo":{"status":"aborted","timestamp":1719020210401,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: a standard neural network with 3 fully connected layers for regression\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RegressionNet(nn.Module):\n","    def __init__(self, input_size):\n","        super(RegressionNet, self).__init__()\n","        self.nn = nn.Sequential(\n","            nn.Linear(input_size, hidden_size ),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size , hidden_size // 2),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 2, hidden_size // 4),\n","            # nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size // 4, 1)\n","            )\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.fill_(0)\n","\n","    def forward(self, x):\n","        return self.nn(x).squeeze()"],"metadata":{"id":"BLy5JoRbWSI0","executionInfo":{"status":"aborted","timestamp":1719020210401,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_accuracies = []\n","for train_index, test_index in k_fold.split(Xs):\n","  # Get training and testing data\n","  X_train, X_test = Xs[train_index], Xs[test_index]\n","  y_train, y_test = ys[train_index], ys[test_index]\n","  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n","  patience_counter = 0\n","  best_model = None\n","  best_accuracy = None\n","  model = RegressionNet(Xs.shape[1])\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  for epoch in range(epochs):\n","    epoch_msg = True\n","    training_total_loss = 0.0\n","    num_of_batches = len(train_loader)\n","    for X_train_batch, y_train_batch in train_loader:\n","      model.train()\n","      # Forward pass\n","      outputs = model(X_train_batch)\n","      loss = criterion(outputs, y_train_batch)\n","      training_total_loss += loss.item()\n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","    if epoch == 0 or (epoch + 1) % 3 == 0:\n","      print(f'Epoch: {epoch + 1}, Training Loss: {training_total_loss/num_of_batches:.2f}')\n","\n","    # Testing the model\n","    model.eval()\n","    with torch.no_grad():\n","      outputs = model(X_test)\n","      loss = criterion(outputs, y_test)\n","      if best_accuracy is None or loss.item() < best_accuracy:\n","        best_accuracy = loss.item()\n","        best_model = model\n","        patience_counter = 0\n","      else:\n","        patience_counter += 1\n","      if epoch_msg and (epoch + 1) % 100 == 0:\n","        epoch_msg = False\n","        print(f'Epoch [{epoch + 1}/{epochs}], Test Loss: {loss.item()}')\n","        # print(f'Loss on the test set: {loss.item()}')\n","    if patience_counter >= cfg.patience:\n","      print(\"Best loss achieved: \", best_accuracy)\n","      break\n","  best_accuracies.append(best_accuracy)\n","print(\"Average accuracy:\", np.mean(best_accuracies))\n"],"metadata":{"id":"55RGrtO-XZuG","executionInfo":{"status":"aborted","timestamp":1719020210401,"user_tz":-480,"elapsed":11,"user":{"displayName":"Xiaomeng Ye","userId":"13514710516313163849"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"L4","toc_visible":true,"collapsed_sections":["rxoHWNZEmG4g","oaKz8Ns3mG4q","46TjfSz-mG4q"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}