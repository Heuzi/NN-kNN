{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file = OmegaConf.load('./config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import cls_small_data as Cdata\n",
    "import model.cls_model as Cmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported small dataset for classification:  \n",
    "'zebra',\n",
    "'zebra_special',\n",
    "'bal',\n",
    "'digits',\n",
    "'iris',\n",
    "'wine',\n",
    "'breast_cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'iris'\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = conf_file['dataset'][dataset_name]\n",
    "Xs, ys = Cdata.Cls_small_data(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with NNKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is used to reload the imported module. For example, if you made any changes in the model.cls_model, you should run importlib.reload(Cmodel) as long as you set import model.cls_model as Cmodel.\n",
    "# import importlib\n",
    "# importlib.reload(Cmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(X_train,y_train, X_test, y_test, cfg:DictConfig):\n",
    "  X_train = X_train.to(device)\n",
    "  y_train = y_train.to(device)\n",
    "  X_test = X_test.to(device)\n",
    "\n",
    "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "  # Train model\n",
    "  model = Cmodel.NN_k_NN(X_train,\n",
    "                         y_train,\n",
    "                         cfg.ca_weight_sharing,\n",
    "                         cfg.top_case_enabled,\n",
    "                         cfg.top_k,\n",
    "                         cfg.discount,\n",
    "                         device=device)\n",
    "  \n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate) #, weight_decay=1e-5)\n",
    "\n",
    "  patience_counter = 0\n",
    "  for epoch in range(cfg.training_epochs):\n",
    "    epoch_msg = True\n",
    "    \n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "      model.train()\n",
    "      _, _, output, predicted_class = model(X_train_batch)\n",
    "      loss = criterion(output, y_train_batch)\n",
    "\n",
    "      # Backward and optimize\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if epoch_msg and (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{cfg.training_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_msg = False\n",
    "      # print(\"evaluating\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      _, _, output, predicted_class = model(X_test)\n",
    "\n",
    "      # Calculate accuracy\n",
    "      accuracy_temp = accuracy_score(y_test.numpy(), predicted_class.cpu().numpy())\n",
    "    if epoch == 0:\n",
    "      best_accuracy = accuracy_temp\n",
    "      torch.save(model.state_dict(), cfg.PATH)\n",
    "      \n",
    "    elif accuracy_temp > best_accuracy:\n",
    "      #memorize best model\n",
    "      torch.save(model.state_dict(), cfg.PATH)\n",
    "      best_accuracy = accuracy_temp\n",
    "      patience_counter = 0\n",
    "\n",
    "    elif patience_counter > cfg.patience:\n",
    "      model.eval()\n",
    "      print(\"patience exceeded, loading best model\")\n",
    "      break\n",
    "    else:\n",
    "      patience_counter += 1\n",
    "\n",
    "  X_train = X_train.cpu()\n",
    "  y_train = y_train.cpu()\n",
    "  X_test = X_test.cpu()\n",
    "  ##compare with a normal k-nn\n",
    "  knn =  KNeighborsClassifier(n_neighbors=cfg.top_k)\n",
    "  knn.fit(X_train, y_train)\n",
    "  knn_acc  = accuracy_score(knn.predict(X_test), y_test)\n",
    "  return best_accuracy, knn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000], Loss: 0.3380\n",
      "Epoch [4/1000], Loss: 0.2282\n",
      "Epoch [6/1000], Loss: 0.1642\n",
      "Epoch [8/1000], Loss: 0.2054\n",
      "Epoch [10/1000], Loss: 0.1512\n",
      "Epoch [12/1000], Loss: 0.1841\n",
      "Epoch [14/1000], Loss: 0.1446\n",
      "Epoch [16/1000], Loss: 0.1176\n",
      "Epoch [18/1000], Loss: 0.1426\n",
      "Epoch [20/1000], Loss: 0.0232\n",
      "Epoch [22/1000], Loss: 0.1193\n",
      "Epoch [24/1000], Loss: 0.1217\n",
      "Epoch [26/1000], Loss: 0.1078\n",
      "Epoch [28/1000], Loss: 0.1094\n",
      "Epoch [30/1000], Loss: 0.1156\n",
      "Epoch [32/1000], Loss: 0.0281\n",
      "Epoch [34/1000], Loss: 0.1048\n",
      "Epoch [36/1000], Loss: 0.1053\n",
      "Epoch [38/1000], Loss: 0.0950\n",
      "Epoch [40/1000], Loss: 0.0510\n",
      "Epoch [42/1000], Loss: 0.0247\n",
      "Epoch [44/1000], Loss: 0.0649\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.3440\n",
      "Epoch [4/1000], Loss: 0.2220\n",
      "Epoch [6/1000], Loss: 0.2854\n",
      "Epoch [8/1000], Loss: 0.1732\n",
      "Epoch [10/1000], Loss: 0.2715\n",
      "Epoch [12/1000], Loss: 0.1484\n",
      "Epoch [14/1000], Loss: 0.0790\n",
      "Epoch [16/1000], Loss: 0.1540\n",
      "Epoch [18/1000], Loss: 0.1062\n",
      "Epoch [20/1000], Loss: 0.1557\n",
      "Epoch [22/1000], Loss: 0.1258\n",
      "Epoch [24/1000], Loss: 0.1037\n",
      "Epoch [26/1000], Loss: 0.1705\n",
      "Epoch [28/1000], Loss: 0.2055\n",
      "Epoch [30/1000], Loss: 0.1935\n",
      "Epoch [32/1000], Loss: 0.1930\n",
      "Epoch [34/1000], Loss: 0.1172\n",
      "Epoch [36/1000], Loss: 0.1307\n",
      "Epoch [38/1000], Loss: 0.1105\n",
      "Epoch [40/1000], Loss: 0.1353\n",
      "Epoch [42/1000], Loss: 0.1237\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.2607\n",
      "Epoch [4/1000], Loss: 0.2811\n",
      "Epoch [6/1000], Loss: 0.2506\n",
      "Epoch [8/1000], Loss: 0.1483\n",
      "Epoch [10/1000], Loss: 0.1963\n",
      "Epoch [12/1000], Loss: 0.2093\n",
      "Epoch [14/1000], Loss: 0.2142\n",
      "Epoch [16/1000], Loss: 0.2069\n",
      "Epoch [18/1000], Loss: 0.1282\n",
      "Epoch [20/1000], Loss: 0.1780\n",
      "Epoch [22/1000], Loss: 0.1002\n",
      "Epoch [24/1000], Loss: 0.1819\n",
      "Epoch [26/1000], Loss: 0.1791\n",
      "Epoch [28/1000], Loss: 0.1762\n",
      "Epoch [30/1000], Loss: 0.0874\n",
      "Epoch [32/1000], Loss: 0.0904\n",
      "Epoch [34/1000], Loss: 0.0897\n",
      "Epoch [36/1000], Loss: 0.0980\n",
      "Epoch [38/1000], Loss: 0.0893\n",
      "Epoch [40/1000], Loss: 0.1495\n",
      "Epoch [42/1000], Loss: 0.0870\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.2837\n",
      "Epoch [4/1000], Loss: 0.2189\n",
      "Epoch [6/1000], Loss: 0.3329\n",
      "Epoch [8/1000], Loss: 0.2463\n",
      "Epoch [10/1000], Loss: 0.2978\n",
      "Epoch [12/1000], Loss: 0.2198\n",
      "Epoch [14/1000], Loss: 0.2640\n",
      "Epoch [16/1000], Loss: 0.2633\n",
      "Epoch [18/1000], Loss: 0.1839\n",
      "Epoch [20/1000], Loss: 0.1576\n",
      "Epoch [22/1000], Loss: 0.1477\n",
      "Epoch [24/1000], Loss: 0.2272\n",
      "Epoch [26/1000], Loss: 0.1865\n",
      "Epoch [28/1000], Loss: 0.1825\n",
      "Epoch [30/1000], Loss: 0.1720\n",
      "Epoch [32/1000], Loss: 0.1065\n",
      "Epoch [34/1000], Loss: 0.1433\n",
      "Epoch [36/1000], Loss: 0.1343\n",
      "Epoch [38/1000], Loss: 0.1290\n",
      "Epoch [40/1000], Loss: 0.1871\n",
      "Epoch [42/1000], Loss: 0.1545\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.4366\n",
      "Epoch [4/1000], Loss: 0.1977\n",
      "Epoch [6/1000], Loss: 0.1905\n",
      "Epoch [8/1000], Loss: 0.0689\n",
      "Epoch [10/1000], Loss: 0.2787\n",
      "Epoch [12/1000], Loss: 0.0435\n",
      "Epoch [14/1000], Loss: 0.2278\n",
      "Epoch [16/1000], Loss: 0.1161\n",
      "Epoch [18/1000], Loss: 0.2373\n",
      "Epoch [20/1000], Loss: 0.0933\n",
      "Epoch [22/1000], Loss: 0.2028\n",
      "Epoch [24/1000], Loss: 0.1133\n",
      "Epoch [26/1000], Loss: 0.1053\n",
      "Epoch [28/1000], Loss: 0.1801\n",
      "Epoch [30/1000], Loss: 0.0802\n",
      "Epoch [32/1000], Loss: 0.1508\n",
      "Epoch [34/1000], Loss: 0.0987\n",
      "Epoch [36/1000], Loss: 0.1519\n",
      "Epoch [38/1000], Loss: 0.0678\n",
      "Epoch [40/1000], Loss: 0.1180\n",
      "Epoch [42/1000], Loss: 0.1189\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.3975\n",
      "Epoch [4/1000], Loss: 0.2966\n",
      "Epoch [6/1000], Loss: 0.2813\n",
      "Epoch [8/1000], Loss: 0.2564\n",
      "Epoch [10/1000], Loss: 0.2112\n",
      "Epoch [12/1000], Loss: 0.2203\n",
      "Epoch [14/1000], Loss: 0.1662\n",
      "Epoch [16/1000], Loss: 0.0826\n",
      "Epoch [18/1000], Loss: 0.2381\n",
      "Epoch [20/1000], Loss: 0.2403\n",
      "Epoch [22/1000], Loss: 0.2353\n",
      "Epoch [24/1000], Loss: 0.1625\n",
      "Epoch [26/1000], Loss: 0.1261\n",
      "Epoch [28/1000], Loss: 0.1891\n",
      "Epoch [30/1000], Loss: 0.1859\n",
      "Epoch [32/1000], Loss: 0.1559\n",
      "Epoch [34/1000], Loss: 0.1604\n",
      "Epoch [36/1000], Loss: 0.2387\n",
      "Epoch [38/1000], Loss: 0.1513\n",
      "Epoch [40/1000], Loss: 0.1915\n",
      "Epoch [42/1000], Loss: 0.2053\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.3146\n",
      "Epoch [4/1000], Loss: 0.3142\n",
      "Epoch [6/1000], Loss: 0.1310\n",
      "Epoch [8/1000], Loss: 0.0795\n",
      "Epoch [10/1000], Loss: 0.2899\n",
      "Epoch [12/1000], Loss: 0.2583\n",
      "Epoch [14/1000], Loss: 0.1651\n",
      "Epoch [16/1000], Loss: 0.0945\n",
      "Epoch [18/1000], Loss: 0.2478\n",
      "Epoch [20/1000], Loss: 0.1407\n",
      "Epoch [22/1000], Loss: 0.1835\n",
      "Epoch [24/1000], Loss: 0.1775\n",
      "Epoch [26/1000], Loss: 0.2125\n",
      "Epoch [28/1000], Loss: 0.0830\n",
      "Epoch [30/1000], Loss: 0.1455\n",
      "Epoch [32/1000], Loss: 0.1741\n",
      "Epoch [34/1000], Loss: 0.1125\n",
      "Epoch [36/1000], Loss: 0.1638\n",
      "Epoch [38/1000], Loss: 0.1463\n",
      "Epoch [40/1000], Loss: 0.1423\n",
      "Epoch [42/1000], Loss: 0.1135\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.3675\n",
      "Epoch [4/1000], Loss: 0.2443\n",
      "Epoch [6/1000], Loss: 0.3127\n",
      "Epoch [8/1000], Loss: 0.2796\n",
      "Epoch [10/1000], Loss: 0.2598\n",
      "Epoch [12/1000], Loss: 0.1613\n",
      "Epoch [14/1000], Loss: 0.0644\n",
      "Epoch [16/1000], Loss: 0.1569\n",
      "Epoch [18/1000], Loss: 0.1831\n",
      "Epoch [20/1000], Loss: 0.1510\n",
      "Epoch [22/1000], Loss: 0.1901\n",
      "Epoch [24/1000], Loss: 0.1394\n",
      "Epoch [26/1000], Loss: 0.1934\n",
      "Epoch [28/1000], Loss: 0.2046\n",
      "Epoch [30/1000], Loss: 0.0933\n",
      "Epoch [32/1000], Loss: 0.1750\n",
      "Epoch [34/1000], Loss: 0.1381\n",
      "Epoch [36/1000], Loss: 0.1135\n",
      "Epoch [38/1000], Loss: 0.1520\n",
      "Epoch [40/1000], Loss: 0.1420\n",
      "Epoch [42/1000], Loss: 0.1178\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.3659\n",
      "Epoch [4/1000], Loss: 0.3024\n",
      "Epoch [6/1000], Loss: 0.2668\n",
      "Epoch [8/1000], Loss: 0.2653\n",
      "Epoch [10/1000], Loss: 0.1589\n",
      "Epoch [12/1000], Loss: 0.2173\n",
      "Epoch [14/1000], Loss: 0.2774\n",
      "Epoch [16/1000], Loss: 0.1534\n",
      "Epoch [18/1000], Loss: 0.2153\n",
      "Epoch [20/1000], Loss: 0.2448\n",
      "Epoch [22/1000], Loss: 0.1403\n",
      "Epoch [24/1000], Loss: 0.1946\n",
      "Epoch [26/1000], Loss: 0.1881\n",
      "Epoch [28/1000], Loss: 0.0777\n",
      "Epoch [30/1000], Loss: 0.1470\n",
      "Epoch [32/1000], Loss: 0.1195\n",
      "Epoch [34/1000], Loss: 0.1469\n",
      "Epoch [36/1000], Loss: 0.1076\n",
      "Epoch [38/1000], Loss: 0.1174\n",
      "Epoch [40/1000], Loss: 0.0774\n",
      "Epoch [42/1000], Loss: 0.1132\n",
      "patience exceeded, loading best model\n",
      "Epoch [2/1000], Loss: 0.2911\n",
      "Epoch [4/1000], Loss: 0.1998\n",
      "Epoch [6/1000], Loss: 0.1149\n",
      "Epoch [8/1000], Loss: 0.1188\n",
      "Epoch [10/1000], Loss: 0.1482\n",
      "Epoch [12/1000], Loss: 0.1033\n",
      "Epoch [14/1000], Loss: 0.1604\n",
      "Epoch [16/1000], Loss: 0.1916\n",
      "Epoch [18/1000], Loss: 0.1615\n",
      "Epoch [20/1000], Loss: 0.0977\n",
      "Epoch [22/1000], Loss: 0.1219\n",
      "Epoch [24/1000], Loss: 0.1405\n",
      "Epoch [26/1000], Loss: 0.1031\n",
      "Epoch [28/1000], Loss: 0.0903\n",
      "Epoch [30/1000], Loss: 0.1173\n",
      "Epoch [32/1000], Loss: 0.0813\n",
      "Epoch [34/1000], Loss: 0.1088\n",
      "Epoch [36/1000], Loss: 0.0974\n",
      "Epoch [38/1000], Loss: 0.0967\n",
      "Epoch [40/1000], Loss: 0.0891\n",
      "Epoch [42/1000], Loss: 0.0731\n",
      "patience exceeded, loading best model\n",
      "Average accuracy:0.973\n",
      "KNN accuracy:0.973\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "knn_accuracies = []\n",
    "PATH = f'checkpoints/classifier_{dataset_name}.h5'\n",
    "cfg.PATH = PATH\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state = None)\n",
    "\n",
    "for train_index, test_index in k_fold.split(Xs):\n",
    "  # Get training and testing data\n",
    "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
    "  y_train, y_test = ys[train_index], ys[test_index]\n",
    "  best_accuracy, knn_acc = train_cls(X_train,y_train, X_test, y_test, cfg)\n",
    "  accuracies.append(best_accuracy)\n",
    "  knn_accuracies.append(knn_acc)\n",
    "\n",
    "print(f\"Average accuracy:{np.mean(accuracies):.3f}\")\n",
    "print(f\"KNN accuracy:{np.mean(knn_accuracies):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
