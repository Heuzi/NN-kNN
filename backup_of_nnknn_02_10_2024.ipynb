{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heuzi/NN-kNN/blob/main/backup_of_nnknn_02_10_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to use this file\n",
        "\n",
        "Run everything in \"Module definitions\" section. This will set up all the classes needed.\n",
        "\n",
        "Run the section \"pydml\" to access knn using other distance metric learning algorithms.\n",
        "\n",
        "Run a subsection in \"Data Sets\" section to load the data set you want.\n",
        "\n",
        "If the data set is a regression data set, run \"Standardize Regression Data sets\"\n",
        "\n",
        "Run either \"classification\" or \"Regression\".\n",
        "Notice that at the beginning of these two sections, there are settings you can tune for NN-kNN\n",
        "\n",
        "Run \"Result look up\" section to check results.\n",
        "\n",
        "Good luck, future Ye or whoever reads this.\n",
        "\n",
        "--- Past Ye\n",
        "\n",
        "This is a backup file, that should work for all experiments described in the paper: \"Combining Interpretability and Neural Network Power: A Neural Network based K-Nearest Neighbor Algorithm\""
      ],
      "metadata": {
        "id": "OJDLyd_l9gP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Important settings for best performance"
      ],
      "metadata": {
        "id": "ReXmju_QUAEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularizer like L2 REGULARIZATION WILL LOWER THE ACCURACY OF NNKNN!"
      ],
      "metadata": {
        "id": "SSmWpl7f9m7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "default case activation layer's bias needs to be carefully set.\n",
        "\n",
        "```\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/2 )\n",
        "    self.bias = torch.nn.Parameter(torch.ones(num_cases) )\n",
        "```\n",
        "These two choices depends on the number of features you have. If you have too many features and the feature distances are big, they might reduce your case activation to 0 and therefore no learning can occur. In that case, use the first one.\n"
      ],
      "metadata": {
        "id": "UBH03nbnUDQ9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8091XltONjJx"
      },
      "source": [
        "#Module definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2l83dT3OnGc"
      },
      "source": [
        "Boring imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5zwSxqSOkaM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxlGKTJXHTpq"
      },
      "source": [
        "##New Feature Act Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.5"
      ],
      "metadata": {
        "id": "iw7AZFbHmpCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxfVJTwmHSd6"
      },
      "outputs": [],
      "source": [
        "class FeatureActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the feature distance/activation between query and all cases\n",
        "  '''\n",
        "  def __init__(self, num_features, num_cases, cases, hidden_layers = False):\n",
        "    super().__init__()\n",
        "    #we assume feature weight sharing between segments\n",
        "    self.feature_matrix = cases\n",
        "    self.f1weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "    # self.activation_func = torch.relu\n",
        "\n",
        "    self.hidden_layers = hidden_layers\n",
        "    hidden_dim = num_features * 2\n",
        "    extract_dim = num_features\n",
        "    self.layer1 = nn.Linear(num_features, hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "    self.layer2 = nn.Linear(hidden_dim,extract_dim)\n",
        "\n",
        "    # self.layer3 = nn.Linear(num_features*3, num_features*2)\n",
        "    # self.layer4 = nn.Linear(num_features*2, extract_dim)\n",
        "  def forward(self, query, cases):\n",
        "    '''\n",
        "      input: (m+1) cases * n features; where the first case is the query\n",
        "      output: m* n feature activations\n",
        "    '''\n",
        "    if self.hidden_layers:\n",
        "      query = self.layer1(query)\n",
        "      query = self.relu(query)\n",
        "      query = self.dropout(query)\n",
        "      query = self.layer2(query)\n",
        "      query = self.relu(query)\n",
        "      # query = self.layer3(query)\n",
        "      # query = self.relu(query)\n",
        "      # query = self.layer4(query)\n",
        "      # query = self.relu(query)\n",
        "\n",
        "      cases = self.layer1(cases)\n",
        "      cases = self.relu(cases)\n",
        "      cases = self.dropout(cases)\n",
        "      cases = self.layer2(cases)\n",
        "      cases = self.relu(cases)\n",
        "      # cases = self.layer3(cases)\n",
        "      # cases = self.relu(cases)\n",
        "      # cases = self.layer4(cases)\n",
        "      # cases = self.relu(cases)\n",
        "\n",
        "    query = query.unsqueeze(1)\n",
        "\n",
        "    # print(\"cool\")\n",
        "    # print(query.shape)\n",
        "    # print(query)\n",
        "    # print((query*self.f1weight).shape)\n",
        "    # print((query*self.f1weight))\n",
        "    # print(self.f1weight.shape)\n",
        "    # print(self.f1weight)\n",
        "    # print((cases*self.f1weight).shape)\n",
        "    # print((cases*self.f1weight))\n",
        "    # print((query*self.f1weight - cases*self.f1weight).shape)\n",
        "    # print((query*self.f1weight - cases*self.f1weight))\n",
        "    #the following utilize pytorch tensor broadcasting functionality\n",
        "    #if the below line does not make sense, uncomment lines above to debug and check values\n",
        "    return (query*self.f1weight - cases*self.f1weight)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2gojHNfJacB"
      },
      "source": [
        "###Old Feature Activation Layer\n",
        "Deprecated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPaUn0-Kf0ts"
      },
      "outputs": [],
      "source": [
        "class OldFeatureActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the feature distance/activation between query and all cases\n",
        "  '''\n",
        "  def __init__(self, num_features, num_cases, weight_sharing_within_segment=False, weight_sharing_between_segment= False, hidden_layers = False):\n",
        "    super().__init__()\n",
        "    self.weight_sharing_between_segment = weight_sharing_between_segment\n",
        "    if self.weight_sharing_between_segment:\n",
        "      self.segments = torch.nn.ModuleList([OldFeatureActivationSegment(num_features, weight_sharing_within_segment, hidden_layers)])\n",
        "    else:\n",
        "      self.segments = torch.nn.ModuleList([OldFeatureActivationSegment(num_features, weight_sharing_within_segment, hidden_layers) for _ in range(num_cases)])\n",
        "    # feature activation layer should not have its own activation function,\n",
        "    # instead, it should just reuse feature activation segments' activation function, which is called by OldFeatureActivationSegment\n",
        "    # self.activation_func = torch.nn.Identity()\n",
        "  def set_f_weight_for_all_segments(self, f):\n",
        "    for segment in self.segments:\n",
        "      segment.set_f1(f)\n",
        "      segment.set_f2(f)\n",
        "  def freeze_f_weight_for_all_segments(self):\n",
        "    for segment in self.segments:\n",
        "      segment.freeze_f1()\n",
        "      segment.freeze_f2()\n",
        "  def forward(self, query, cases):\n",
        "    '''\n",
        "      input: (m+1) cases * n features; where the first case is the query\n",
        "      output: m* n feature activations\n",
        "    '''\n",
        "    if self.weight_sharing_between_segment:\n",
        "      #only uses the first segment through out.\n",
        "      return torch.stack([self.segments[0](query, case) for case in cases], dim=1)\n",
        "    return torch.stack([segment(query, case) for segment, case in zip(self.segments, cases)], dim=1) ###OMG, colab is a great programmer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJvuUic5JPoP"
      },
      "source": [
        "###Old Feature Activation Segment\n",
        "\n",
        "Deprecated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAMlrLdaZK5p"
      },
      "source": [
        "assume there are m cases, n features for each case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VJRT4bxOqPr"
      },
      "outputs": [],
      "source": [
        "class OldFeatureActivationSegment(torch.nn.Module):\n",
        "  '''\n",
        "    measures the feature distance/activation between two cases\n",
        "\n",
        "    two features connect to one feature activation node\n",
        "\n",
        "    multiple feature activation layer only connects to one case node\n",
        "\n",
        "    num_features: number of features that f1 or f2 contains. f1 is a vector of features for one case.\n",
        "  '''\n",
        "  def __init__(self, num_features, weight_sharing=False, hidden_layers = False):\n",
        "    super().__init__()\n",
        "    self.weight_sharing = weight_sharing\n",
        "    ## IMPORTANT DESIGN DECISION:: start weights and biases with ones instead of randoms.\n",
        "    self.f1weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "    self.f2weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "    if weight_sharing:\n",
        "      self.f2weight = self.f1weight\n",
        "    #it does not make sense that the bias is non-zero.\n",
        "    # self.bias = torch.nn.Parameter(torch.zeros(num_features))\n",
        "    # self.activation_func = torch.nn.Identity()\n",
        "    self.activation_func = torch.relu\n",
        "\n",
        "    self.hidden_layers = hidden_layers\n",
        "    hidden_dim = num_features * 2\n",
        "    extract_dim = num_features\n",
        "    self.layer1 = nn.Linear(num_features, hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer2 = nn.Linear(hidden_dim, extract_dim)\n",
        "\n",
        "  # maybe not needed\n",
        "  # def set_activation_func(self, activation_function):\n",
        "  #   self.activation_func = activation_function\n",
        "\n",
        "  def set_f1(self, f1):\n",
        "    self.f1weight = f1\n",
        "\n",
        "  def set_f2(self, f2):\n",
        "    self.f2weight = f2\n",
        "\n",
        "  def freeze_f1(self):\n",
        "    self.f1weight.requires_grad = False\n",
        "\n",
        "  def freeze_f2(self):\n",
        "    self.f2weight.requires_grad = False\n",
        "\n",
        "  def forward(self, f1, f2):\n",
        "    '''\n",
        "      input: f1 and f2, are both feature vectors of a case, shape n.\n",
        "      output: feature activation, shape n.\n",
        "    '''\n",
        "    if self.hidden_layers:\n",
        "      f1 = self.layer1(f1)\n",
        "      f1 = self.relu(f1)\n",
        "      f1 = self.layer2(f1)\n",
        "      f1 = self.relu(f1)\n",
        "\n",
        "      f2 = self.layer1(f2)\n",
        "      f2 = self.relu(f2)\n",
        "      f2 = self.layer2(f2)\n",
        "      f2 = self.relu(f2)\n",
        "\n",
        "\n",
        "    # return self.activation_func(self.f1weight * f1 + self.f2weight * f2 + self.bias)\n",
        "    # IMPORTANT DESIGN DECISION:: minus makes more sense here as in standard k-NN.\n",
        "    # feature_distance = self.f1weight * f1 - self.f2weight * f2\n",
        "    # feature_distance = (self.f1weight * f1 - self.f2weight * f2) ** 2\n",
        "    # feature_distance = torch.abs(self.f1weight * f1 - self.f2weight * f2)\n",
        "    feature_distance = (self.f1weight * f1 - self.f2weight * f2) ** 2\n",
        "    # feature_distance = torch.abs(self.f1weight * f1 - self.f2weight * f2)\n",
        "    # return self.activation_func(feature_distance + self.bias)\n",
        "    return self.activation_func(feature_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvaZeqxwJeJ5"
      },
      "source": [
        "##Case Activation Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT.\n",
        "```\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/2 )\n",
        "    self.bias = torch.nn.Parameter(torch.ones(num_cases) )\n",
        "```\n",
        "These two choices depends on the number of features you have. If you have too many features, may use the first one.\n"
      ],
      "metadata": {
        "id": "Cs_eTUGYRNnt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OtiusY8HV8v"
      },
      "outputs": [],
      "source": [
        "class CaseActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a case given some feature activations\n",
        "\n",
        "    input:\n",
        "      m* n feature activations\n",
        "    output:\n",
        "      m case activations\n",
        "  '''\n",
        "  def __init__(self, num_features, num_cases, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.weight_sharing = weight_sharing\n",
        "    self.fa_weight = torch.nn.Parameter(torch.ones((num_cases, num_features)))\n",
        "    if weight_sharing:\n",
        "      self.fa_weight = torch.nn.Parameter(torch.ones(num_features))\n",
        "\n",
        "    # self.bias = torch.nn.Parameter(torch.ones(num_cases) * num_features/2 )\n",
        "    self.bias = torch.nn.Parameter(torch.ones(num_cases) )\n",
        "  def forward(self,input):\n",
        "    '''\n",
        "      input: m*n feature activations\n",
        "      output: m case activations\n",
        "    '''\n",
        "    input = - torch.sum(input * F.leaky_relu(self.fa_weight), dim=2) + self.bias\n",
        "    ## we don't want negative values in activations\n",
        "    ## they will mess up our top case selections because if the tops are negatives\n",
        "    ## then the filled 0s are actually bigger.\n",
        "    input = F.sigmoid(input)\n",
        "    # input = torch.relu(input)\n",
        "    return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkPmMySVi3jN"
      },
      "source": [
        "##Top Case Selection Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne1tBxp6jEp9"
      },
      "outputs": [],
      "source": [
        "# prompt: define a class named TopCaseLayer inherited from torch.nn.Module. This class is used to select the top k activations of an input tensor (m case activations), and output a tensor of the same shape but only keep the top k activations, and other tensors zeroed out\n",
        "\n",
        "class TopCaseLayer(torch.nn.Module):\n",
        "  def __init__(self, k):\n",
        "    super().__init__()\n",
        "    self.k = k\n",
        "    self.training = True\n",
        "\n",
        "  def forward(self, input):\n",
        "    ##no behavior during training because we want to train for all.\n",
        "    if self.training:\n",
        "      return input\n",
        "    '''\n",
        "      input: m case activations\n",
        "      output: m case activations, the top k activations are kept and others are zeroed out\n",
        "    '''\n",
        "    # print(input)\n",
        "    vals, idx = torch.topk(input, self.k)\n",
        "\n",
        "    #if any of the vals is zero, it will mess things up, reverse them so that they becomes positive.\n",
        "\n",
        "    # print(input.shape)\n",
        "    # print(idx.shape)\n",
        "    # print(vals.shape)\n",
        "    # print(\"huh??\")\n",
        "    output = torch.zeros_like(input).scatter_(1,idx, vals)\n",
        "    #replace output values such that the top k is kept\n",
        "\n",
        "\n",
        "    # output[:, idx] = vals\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESrlZ3sTJhm4"
      },
      "source": [
        "##Class Activation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vg6cXb3rQi1"
      },
      "outputs": [],
      "source": [
        "# class ClassActivationLayer(torch.nn.Module):\n",
        "#   '''\n",
        "#     measures the activation of a class given some case activations\n",
        "\n",
        "#     input:\n",
        "#       m case_activations\n",
        "#     output:\n",
        "#       l class_activations\n",
        "#   '''\n",
        "#   def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "#     super().__init__()\n",
        "#     self.constraints = []\n",
        "#     self.case_labels = case_labels\n",
        "#     self.num_classes = torch.unique(case_labels).shape[0]\n",
        "#     # self.weight_sharing = weight_sharing\n",
        "#     #weight sharing doesn't make sense here, some cases shouldn't contribute to a class while some should\n",
        "#     # they should not share weights.\n",
        "#     self.ca_weight = torch.nn.Parameter(torch.ones((num_cases, self.num_classes))) ## should I use randn here?\n",
        "#     # self.ca_weight = torch.nn.Parameter(torch.rand((num_cases, self.num_classes)))\n",
        "#     # if weight_sharing:\n",
        "#     #   self.ca_weight = torch.nn.Parameter(torch.randn(num_cases))\n",
        "#     # self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "#     self.bias = torch.nn.Parameter(torch.zeros(self.num_classes))\n",
        "\n",
        "#     print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "#   ##NOTE:: the following three constrain uses for loop, may not be the most efficient\n",
        "#   def case_class_constrain_v1(self):\n",
        "#     '''\n",
        "#       ensures that each case only positively activates their correct class label.\n",
        "#     '''\n",
        "#     self.constraints.append(\"case_pos_class\")\n",
        "\n",
        "\n",
        "#   ##NOTE:: if used, this should be called before the constrain_v1()\n",
        "#   def cases_share_weight_on_same_class(self):\n",
        "#     '''\n",
        "#       all cases of the same class share one weight for that class.\n",
        "#     '''\n",
        "#     self.constraints.append(\"case_share_weight_on_same_class\")\n",
        "\n",
        "\n",
        "#   def case_class_constrain_v2(self):\n",
        "#     '''\n",
        "#       ensures that each case does not contribute to incorrect classes.\n",
        "#     '''\n",
        "#     self.constraints.append(\"case_no_contribute_to_wrong_class\")\n",
        "#   def case_class_constrain_v3(self):\n",
        "#     '''\n",
        "#       ensures that each case only negatively contribute to incorrect classes.\n",
        "#     '''\n",
        "#     #TODO, needs revamping\n",
        "#     #The weight manipulation should only happen during forward()\n",
        "#     raise ValueError(\"error, needs revamping\")\n",
        "#     for i in range(self.ca_weight.shape[0]):\n",
        "#       for j in range(self.ca_weight.shape[1]):\n",
        "#         if j != case_labels[i]:\n",
        "#           self.ca_weight[i][j] = 0 - torch.relu(self.ca_weight[i][j])\n",
        "#   def forward(self, ca):\n",
        "#     constrained_weight = self.ca_weight.clone().detach()\n",
        "#     for i in range(constrained_weight.shape[0]):\n",
        "#       for j in range(constrained_weight.shape[1]):\n",
        "#         if(j == self.case_labels[i]):\n",
        "#           if(\"case_pos_class\" in self.constraints):\n",
        "#             constrained_weight[i][j] = torch.relu(self.ca_weight[i][j])\n",
        "#         else: # j != case_labels[i]\n",
        "#           if(\"case_no_contribute_to_wrong_class\" in self.constraints):\n",
        "#             constrained_weight[i][j] = torch.zeros(1)\n",
        "#     return torch.matmul(ca, constrained_weight) + self.bias #matrix multiplication will handle 1 dimension vector property"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Il5qc6sq-3n"
      },
      "source": [
        "The below code is another implementation of ClassActivationLayer which use mask to reduce the amount of computations during the forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So4vm5Z_qwY0"
      },
      "outputs": [],
      "source": [
        "class ClassActivationLayer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      l class_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.case_labels = case_labels\n",
        "    self.num_classes = torch.unique(case_labels).shape[0]\n",
        "    # self.weight_sharing = weight_sharing\n",
        "    #weight sharing doesn't make sense here, some cases shouldn't contribute to a class while some should\n",
        "    # they should not share weights.\n",
        "    self.ca_weight = torch.nn.Parameter(torch.ones((num_cases, self.num_classes))) ## should I use randn here?\n",
        "    # self.ca_weight = torch.nn.Parameter(torch.rand((num_cases, self.num_classes)))\n",
        "    # if weight_sharing:\n",
        "    #   self.ca_weight = torch.nn.Parameter(torch.randn(num_cases))\n",
        "    # self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    self.bias = torch.nn.Parameter(torch.ones(self.num_classes))\n",
        "\n",
        "    # create a mask\n",
        "    self.mask = torch.zeros_like(self.ca_weight)\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "\n",
        "  def update_mask(self):\n",
        "    for i in range(self.mask.shape[0]):\n",
        "      for j in range(self.mask.shape[1]):\n",
        "        if(j == self.case_labels[i]):\n",
        "          if(\"case_pos_class\" in self.constraints):\n",
        "            self.mask[i][j] = torch.ones(1)\n",
        "        else: # j != case_labels[i]\n",
        "          if(\"case_no_contribute_to_wrong_class\" in self.constraints):\n",
        "            self.mask[i][j] = torch.zeros(1)\n",
        "\n",
        "\n",
        "  ##NOTE:: the following three constrain uses for loop, may not be the most efficient\n",
        "  def case_class_constrain_v1(self):\n",
        "    '''\n",
        "      ensures that each case only positively activates their correct class label.\n",
        "    '''\n",
        "    self.constraints.append(\"case_pos_class\")\n",
        "    self.update_mask()\n",
        "\n",
        "\n",
        "  ##NOTE:: if used, this should be called before the constrain_v1()\n",
        "  def cases_share_weight_on_same_class(self):\n",
        "    '''\n",
        "      all cases of the same class share one weight for that class.\n",
        "    '''\n",
        "    self.constraints.append(\"case_share_weight_on_same_class\")\n",
        "\n",
        "\n",
        "  def case_class_constrain_v2(self):\n",
        "    '''\n",
        "      ensures that each case does not contribute to incorrect classes.\n",
        "    '''\n",
        "    self.constraints.append(\"case_no_contribute_to_wrong_class\")\n",
        "    self.update_mask()\n",
        "\n",
        "  def case_class_constrain_v3(self):\n",
        "    '''\n",
        "      ensures that each case only negatively contribute to incorrect classes.\n",
        "    '''\n",
        "    #TODO, needs revamping\n",
        "    #The weight manipulation should only happen during forward()\n",
        "    raise ValueError(\"error, needs revamping\")\n",
        "    for i in range(self.ca_weight.shape[0]):\n",
        "      for j in range(self.ca_weight.shape[1]):\n",
        "        if j != case_labels[i]:\n",
        "          self.ca_weight[i][j] = 0 - torch.relu(self.ca_weight[i][j])\n",
        "  def forward(self, ca):\n",
        "    constrained_weight = self.mask * torch.relu(self.ca_weight)\n",
        "    return torch.matmul(ca, constrained_weight) + self.bias #matrix multiplication will handle 1 dimension vector property"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgd-UBbdbU5N"
      },
      "source": [
        "##Regression Activation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDmkcY7-rfHQ"
      },
      "outputs": [],
      "source": [
        "class CustomSoftmaxLayer(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomSoftmaxLayer, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    total = torch.sum(x, dim=1, keepdim=True)\n",
        "    softmax_output = x / (total + 1e-10)\n",
        "\n",
        "    ##IMPORTANT, cannot use the softmax version below, because of dividing by 0 problem.\n",
        "\n",
        "    # # Apply the exponential function to the input tensor\n",
        "    # exp_x = torch.exp(x)\n",
        "\n",
        "    # # Apply a mask to set the output to 0 where the input is 0\n",
        "    # mask = (x != 0).float()  # Create a binary mask (1 where x is not 0, 0 where x is 0)\n",
        "\n",
        "    # # Apply the mask to the output\n",
        "    # softmax_output = exp_x * mask\n",
        "\n",
        "    # # Normalize the output by dividing by the sum along the same axis\n",
        "    # softmax_output = softmax_output / softmax_output.sum(dim=1, keepdim=True)\n",
        "\n",
        "    return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbNCIgy0aqvW"
      },
      "outputs": [],
      "source": [
        "# prompt: Build a regression activation layer just like the class activation layer above, but for the purpose of regression now\n",
        "## right now this is essentially a linear layer, but somehow it's a lot slower than a linear layer,\n",
        "class RegressionActivation_1_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "    self.ca_weight = torch.nn.Parameter(torch.randn((num_cases, self.num_classes))) ## should I use randn here?\n",
        "    self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "  def forward(self, ca):\n",
        "    ##IMPORTANT, newly added \"* case_labels\" here, so that case labels is now considered in\n",
        "    ##the last layer of NN-k-NN regressor.\n",
        "    # print(\"debugg msaafasdf\")\n",
        "    # print(ca.shape)\n",
        "    # print(self.ca_weight.shape)\n",
        "    # print(self.case_labels.shape)\n",
        "    # print((torch.matmul(ca * self.case_labels, self.ca_weight)).shape)\n",
        "    return torch.matmul(ca, self.ca_weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDbeAaCorwYr"
      },
      "outputs": [],
      "source": [
        "class RegressionActivation_2_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "    self.ca_weight = torch.nn.Parameter(torch.ones((num_cases, self.num_classes))) ## should I use randn here?\n",
        "    self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "  def forward(self, ca):\n",
        "    return torch.matmul(ca* self.case_labels, self.ca_weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkoZwOlQwMlS"
      },
      "outputs": [],
      "source": [
        "class RegressionActivation_3_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "  def forward(self, ca):\n",
        "    return torch.matmul(ca, self.case_labels) #+ self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0LLPPXBsGFj"
      },
      "outputs": [],
      "source": [
        "ca = torch.Tensor([1 , 0.5, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfvItiQqsQdQ",
        "outputId": "02245170-f60f-4cee-b3bd-fa0b4e130103"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "ca.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDvdr2TIsSMK"
      },
      "outputs": [],
      "source": [
        "case_labels = torch.Tensor([0,2,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clJvDPlLsfsq",
        "outputId": "5d8c5dde-943e-4b70-ffa1-8875c49e03ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6000)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "torch.matmul(ca, case_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo1F5P4s8Ed3"
      },
      "outputs": [],
      "source": [
        "class RegressionActivation_4_Layer(torch.nn.Module):\n",
        "  '''\n",
        "    measures the activation of a class given some case activations\n",
        "\n",
        "    input:\n",
        "      m case_activations\n",
        "    output:\n",
        "      1 regression_activations\n",
        "  '''\n",
        "  def __init__(self, num_cases, case_labels, weight_sharing=False):\n",
        "    super().__init__()\n",
        "    self.constraints = []\n",
        "    self.num_classes = 1\n",
        "    self.case_labels = case_labels\n",
        "    self.ca_weight = torch.nn.Parameter(torch.tensor(case_labels).reshape(num_cases, self.num_classes)) ## should I use randn here?\n",
        "    self.bias = torch.nn.Parameter(torch.randn(self.num_classes))\n",
        "    print(\"self.ca_weight.shape\", self.ca_weight.shape)\n",
        "  def forward(self, ca):\n",
        "    ##IMPORTANT, newly added \"* case_labels\" here, so that case labels is now considered in\n",
        "    ##the last layer of NN-k-NN regressor.\n",
        "    # print(\"debugg msaafasdf\")\n",
        "    # print(ca.shape)\n",
        "    # print(self.ca_weight.shape)\n",
        "    # print(self.case_labels.shape)\n",
        "    # print((torch.matmul(ca * self.case_labels, self.ca_weight)).shape)\n",
        "    return torch.matmul(ca, self.ca_weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQgug4eSyKDv"
      },
      "outputs": [],
      "source": [
        "a = torch.ones((100, 8))\n",
        "b = torch.ones((1548, 8))\n",
        "c = torch.ones((1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwaQh8NUNDLv"
      },
      "outputs": [],
      "source": [
        "# a-b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3afere_KfLO"
      },
      "outputs": [],
      "source": [
        "# (b*b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5qqmJxXyQsd"
      },
      "outputs": [],
      "source": [
        "# (a*b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bITJLDhAySo8"
      },
      "outputs": [],
      "source": [
        "# torch.matmul(a, b).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvvVhcBW241Z"
      },
      "outputs": [],
      "source": [
        "# torch.matmul(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcM3JXmz26Cs"
      },
      "outputs": [],
      "source": [
        "# torch.matmul(a, b) + c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmhxrGP2Jj6T"
      },
      "source": [
        "## NN-k-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9HNVfwjPV8E"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class NN_k_NN(torch.nn.Module):\n",
        "  def __init__(self, cases, case_labels,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing, hidden_layers= False):\n",
        "    super().__init__()\n",
        "    self.cases = cases\n",
        "    num_cases = cases.shape[0]\n",
        "    num_features = cases.shape[1]\n",
        "    # num_classes = torch.unique(case_labels).shape[0] # len(set(case_labels))\n",
        "    ##IMPORTANT:: CHECK THE WEIGHT_SHARING PARAMETERS!!\n",
        "    # self.fa_layer = FeatureActivationLayer(num_features, num_cases,\n",
        "    #                                        weight_sharing_within_segment= fa_weight_sharing_within_segment,\n",
        "    #                                        weight_sharing_between_segment= fa_weight_sharing_between_segment,\n",
        "    #                                        hidden_layers = hidden_layers)\n",
        "    self.fa_layer = FeatureActivationLayer(num_features, num_cases, self.cases, hidden_layers = hidden_layers)\n",
        "\n",
        "    self.ca_layer = CaseActivationLayer(num_features, num_cases,\n",
        "                                        weight_sharing= ca_weight_sharing)\n",
        "    self.top_case_enabled = top_case_enabled\n",
        "    self.selection_layer = TopCaseLayer(top_k)\n",
        "    self.class_layer = ClassActivationLayer(num_cases, case_labels,\n",
        "                                            weight_sharing=class_weight_sharing)\n",
        "    ## more preprocessing here needed to tie each case to their correct class\n",
        "    ## only one is turned on right now for testing purpose.\n",
        "    self.class_layer.case_class_constrain_v1()\n",
        "    self.class_layer.case_class_constrain_v2()\n",
        "\n",
        "    self.ca_dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, query):\n",
        "    #repeat query so that it has the same shape as self.cases\n",
        "    # query = query.repeat(self.cases.shape[0], 1)\n",
        "    #no need to repeat the query, fa_layer's forward will handle it.\n",
        "    feature_activations = self.fa_layer(query, self.cases)\n",
        "    # print(\"feature_activations.shape\", feature_activations.shape)\n",
        "    ##tie the fa_segments with corresponding case in ca layer\n",
        "    #each segment should only tie to one case in ca_layer\n",
        "    case_activations = self.ca_layer(feature_activations)\n",
        "    # print(\"case_activations\" , case_activations.shape)\n",
        "    #uncomment to enable topk\n",
        "    if self.top_case_enabled:\n",
        "      case_activations = self.selection_layer(case_activations)\n",
        "\n",
        "    # if self.training:\n",
        "    #   case_activations = self.ca_dropout(case_activations)\n",
        "      # # print(\"dropout\")\n",
        "      #==========#\n",
        "      # expect the case_activations has the shape of (query_number, case_numer)\n",
        "      # expect the query has the shape of (batch_size, feature_size)\n",
        "      # expect the cases has the shape of (case_number, feature_size)\n",
        "      # dquery = query.unsqueeze(1)\n",
        "      # dcases = self.cases.unsqueeze(0)\n",
        "      # difference = torch.sum(dquery - dcases, dim=-1)\n",
        "      # masks = torch.where(difference == 0, 0., 1.)\n",
        "      # # print(\"masks.shape\", masks.shape)\n",
        "      # # print(\"case_activations.shape\", case_activations.shape)\n",
        "      # case_activations = case_activations * masks\n",
        "      # # print(\"case_activations.shape\", case_activations.shape)\n",
        "\n",
        "\n",
        "    # print(\"case_activations\" , case_activations.shape)\n",
        "    class_ativations = self.class_layer(case_activations)\n",
        "    # print(\"class_ativations.shape\", class_ativations.shape)\n",
        "\n",
        "    # Softmax activation for classification\n",
        "    # output = F.softmax(class_ativations, dim=1)\n",
        "    output = class_ativations\n",
        "\n",
        "    # Get the class predictions (class with the highest probability)\n",
        "    ## TODO:: need debug here, am I using the right dim?\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "    return case_activations, output, predicted_class\n",
        "    # return self.class_layer(ca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukjmB70eHr7a"
      },
      "source": [
        "##NN-k-NN regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD1WpUjVHV58"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "##IMPORTANT, cannot use top case selection layer here like we do in for classification.\n",
        "## it has to be either enabled or disabled the whole time.\n",
        "class NN_k_NN_regression(torch.nn.Module):\n",
        "  def __init__(self, cases, case_labels,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing, hidden_layers= False):\n",
        "    super().__init__()\n",
        "    self.cases = cases\n",
        "    num_cases = cases.shape[0]\n",
        "    num_features = cases.shape[1]\n",
        "    # num_classes = torch.unique(case_labels).shape[0] # len(set(case_labels))\n",
        "    ##IMPORTANT:: CHECK THE WEIGHT_SHARING PARAMETERS!!\n",
        "    # self.fa_layer = FeatureActivationLayer(num_features, num_cases,\n",
        "    #                                        weight_sharing_within_segment= fa_weight_sharing_within_segment,\n",
        "    #                                        weight_sharing_between_segment= fa_weight_sharing_between_segment,\n",
        "    #                                        hidden_layers = hidden_layers)\n",
        "    self.fa_layer = FeatureActivationLayer(num_features, num_cases, self.cases, hidden_layers = hidden_layers)\n",
        "    self.ca_layer = CaseActivationLayer(num_features, num_cases,\n",
        "                                        weight_sharing= ca_weight_sharing)\n",
        "    self.top_case_enabled = top_case_enabled\n",
        "    self.selection_layer = TopCaseLayer(top_k)\n",
        "\n",
        "    self.ca_dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    self.softmax = CustomSoftmaxLayer()\n",
        "    # self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    self.class_layer = RegressionActivation_3_Layer(num_cases, case_labels,weight_sharing=class_weight_sharing)\n",
        "    # self.class_layer = torch.nn.Linear(num_cases, 1)\n",
        "\n",
        "\n",
        "  def forward(self, query):\n",
        "    feature_activations = self.fa_layer(query, self.cases)\n",
        "    # print(\"feature_activations.shape\", feature_activations.shape)\n",
        "    ##tie the fa_segments with corresponding case in ca layer\n",
        "    #each segment should only tie to one case in ca_layer\n",
        "    case_activations = self.ca_layer(feature_activations)\n",
        "    # print(\"case_activations\" , case_activations.shape)\n",
        "    #uncomment to enable topk\n",
        "    if self.top_case_enabled:\n",
        "      case_activations = self.selection_layer(case_activations)\n",
        "    # print(\"case_activations1\" , torch.sum(case_activations, dim= 1))\n",
        "\n",
        "    # if self.training:\n",
        "    #   # case_activations = self.ca_dropout(case_activations)\n",
        "    #   # print(\"dropout\")\n",
        "    #   # ==========#\n",
        "    #   # expect the case_activations has the shape of (query_number, case_numer)\n",
        "    #   # expect the query has the shape of (batch_size, feature_size)\n",
        "    #   # expect the cases has the shape of (case_number, feature_size)\n",
        "    #   dquery = query.unsqueeze(1)\n",
        "    #   dcases = self.cases.unsqueeze(0)\n",
        "    #   difference = torch.sum(dquery - dcases, dim=-1)\n",
        "    #   masks = torch.where(difference == 0, 0., 1.)\n",
        "    #   case_activations = case_activations * masks\n",
        "\n",
        "    case_activations = self.softmax(case_activations)\n",
        "\n",
        "\n",
        "    # print(\"case_activations\" , torch.sum(case_activations, dim= 1))\n",
        "    predicted_number = self.class_layer(case_activations)\n",
        "\n",
        "    # # Softmax activation for classification\n",
        "    # output = F.softmax(class_ativations, dim=1)\n",
        "    # # output = class_ativations\n",
        "    # _, predicted_class = torch.max(output, 1)\n",
        "    # print(\"predicted_class\", predicted_class)\n",
        "    # predicted_class = case_labels[predicted_class]\n",
        "    # print(\"predicted_class after label lookup\", predicted_class)\n",
        "    ##added feature activations here in output because they might be used for adaptation\n",
        "    ####VERY VERY IMPORTANT, the output format is different!!\n",
        "    return feature_activations, case_activations, predicted_number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXaL3PmlBwg6"
      },
      "outputs": [],
      "source": [
        "# # prompt: a standard neural network with fully connected layers that has the same architecture as NN_k_NN_regression\n",
        "\n",
        "# class nn_regression(torch.nn.Module):\n",
        "#   def __init__(self, cases, case_labels,\n",
        "#                fa_weight_sharing_within_segment,\n",
        "#                fa_weight_sharing_between_segment,\n",
        "#                ca_weight_sharing,\n",
        "#                top_case_enabled, top_k,\n",
        "#                class_weight_sharing):\n",
        "#     super().__init__()\n",
        "#     ##WIP\n",
        "#     self.cases = torch.nn.parameters(torch.randn_like(cases))\n",
        "#     num_cases = cases.shape[0]\n",
        "#     num_features = cases.shape[1]\n",
        "#     # num_classes = torch.unique(case_labels).shape[0] # len(set(case_labels))\n",
        "#     ##IMPORTANT:: CHECK THE WEIGHT_SHARING PARAMETERS!!\n",
        "#     self.fa_layer = FeatureActivationLayer(num_features, num_cases, weight_sharing_within_segment= fa_weight_sharing_within_segment,\n",
        "#                                            weight_sharing_between_segment= fa_weight_sharing_between_segment)\n",
        "#     self.ca_layer = CaseActivationLayer(num_features, num_cases,\n",
        "#                                         weight_sharing= ca_weight_sharing)\n",
        "#     self.top_case_enabled = top_case_enabled\n",
        "#     self.selection_layer = TopCaseLayer(top_k)\n",
        "#     self.class_layer = RegressionActivationLayer(num_cases,weight_sharing=class_weight_sharing)\n",
        "\n",
        "\n",
        "#   def forward(self, query):\n",
        "#     feature_activations = self.fa_layer(query, self.cases)\n",
        "#     ##tie the fa_segments with corresponding case in ca layer\n",
        "#     #each segment should only tie to one case in ca_layer\n",
        "#     case_activations = self.ca_layer(feature_activations)\n",
        "\n",
        "#     #uncomment to enable topk\n",
        "#     if self.top_case_enabled:\n",
        "#       case_activations = self.selection_layer(case_activations)\n",
        "\n",
        "#     predicted_number = self.class_layer(case_activations)\n",
        "\n",
        "#     # # Softmax activation for classification\n",
        "#     # output = F.softmax(class_ativations, dim=1)\n",
        "#     # # output = class_ativations\n",
        "#     # _, predicted_class = torch.max(output, 1)\n",
        "#     # print(\"predicted_class\", predicted_class)\n",
        "#     # predicted_class = case_labels[predicted_class]\n",
        "#     # print(\"predicted_class after label lookup\", predicted_class)\n",
        "#     ##added feature activations here in output because they might be used for adaptation\n",
        "#     return feature_activations, case_activations, predicted_number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdFOP30kurhF"
      },
      "outputs": [],
      "source": [
        "def print_model_features(input_model):\n",
        "  for n, p in model.named_parameters():\n",
        "    print(n)\n",
        "    print(p.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJz8UivW3nYt"
      },
      "outputs": [],
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "input = torch.randn(2, 3)\n",
        "output = m(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pydml"
      ],
      "metadata": {
        "id": "aIg-EoJydTrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install metric-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzEt9iuYZvIy",
        "outputId": "2d096876-f97b-4de8-9e23-011c13659ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metric-learn\n",
            "  Downloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/67.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from metric-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->metric-learn) (3.3.0)\n",
            "Installing collected packages: metric-learn\n",
            "Successfully installed metric-learn-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import metric_learn"
      ],
      "metadata": {
        "id": "UpdWR_eh3c1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_0mocIXjT0"
      },
      "source": [
        "#Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIDEqsvbfUMf"
      },
      "source": [
        "##Classification, Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2dEUwwWbL2M"
      },
      "outputs": [],
      "source": [
        "# prompt: load fashion mnist data set into Xs and ys\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load training set and test set\n",
        "trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW2UFYznfndb"
      },
      "outputs": [],
      "source": [
        "trainset[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnZSBxEWf_LH"
      },
      "outputs": [],
      "source": [
        "trainset[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJiR0Ti-v2gG"
      },
      "source": [
        "##Classification, data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8NANW0CwW5X"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "# sklearn.datasets.fetch_olivetti_faces\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "iris = fetch_olivetti_faces()\n",
        "Xs = iris['data']\n",
        "ys = iris['target']\n",
        "# names = iris['target_names']\n",
        "# feature_names = iris['feature_names']\n",
        "\n",
        "## fit Xs in a scaler transform, transform Xs into 0-1\n",
        "# scaler = MinMaxScaler()\n",
        "# Xs = scaler.fit_transform(Xs)\n",
        "if Xs.shape[1] == 64: #for breast cancer\n",
        "  scaler = MinMaxScaler()\n",
        "  Xs = scaler.fit_transform(Xs)\n",
        "\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rLv9czNqfnU",
        "outputId": "5cb13802-1d95-4c6e-930d-01829f6616eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydE6eHnvb4Z6",
        "outputId": "fd90642b-828d-48dd-a289-c0feca3b2747"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3099, 0.3678, 0.4174,  ..., 0.1529, 0.1612, 0.1570])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUGLcsqvC-TP",
        "outputId": "ffec3330-c4f7-4524-afa6-7426fc77ab5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "ys[0].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UFFbmbxFoYa"
      },
      "source": [
        "## Sanity check: A normal neural network classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0kqnMz5Ehs3"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.1, random_state=42, stratify=ys)\n",
        "\n",
        "# # Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Convert data to PyTorch tensors\n",
        "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# # Define the neural network architecture for classification\n",
        "# class NeuralNet(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_classes):\n",
        "#         super(NeuralNet, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# # Hyperparameters\n",
        "# input_size = X_train.shape[1]\n",
        "# hidden_size = 64\n",
        "# num_classes = torch.unique(ys).shape[0]\n",
        "# learning_rate = 0.001\n",
        "# batch_size = 16\n",
        "# epochs = 50\n",
        "\n",
        "# # Create DataLoader for training\n",
        "# train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Initialize the model, loss function, and optimizer\n",
        "# model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(epochs):\n",
        "#     for i, (inputs, labels) in enumerate(train_loader):\n",
        "#         # Forward pass\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backward and optimize\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if (i + 1) % 5 == 0:\n",
        "#             print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "# # Testing the model\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(X_test_tensor)\n",
        "#     _, predicted = torch.max(outputs, 1)\n",
        "#     accuracy = torch.sum(predicted == y_test_tensor).item() / len(y_test_tensor)\n",
        "#     print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "pK6bzvrPNSt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.1, random_state=42, stratify=ys)\n",
        "\n",
        "# Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = X_train\n",
        "y_train_tensor = y_train\n",
        "X_test_tensor = X_test\n",
        "y_test_tensor = y_test\n",
        "\n",
        "# Define the neural network architecture for classification\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size // 4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size // 4, hidden_size // 2),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 2),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "            )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 1024\n",
        "num_classes = torch.unique(ys).shape[0]\n",
        "learning_rate = 1e-5\n",
        "batch_size = 16\n",
        "epochs = 2000\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    accuracy = 0.0\n",
        "    total_num = 0\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        batch_size = len(labels)\n",
        "        total_num += batch_size\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy += torch.sum(predicted == labels).item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # if (i + 1) % 5 == 0\n",
        "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {total_loss/(i+1):.2f} Acc: {accuracy/total_num :.2f}\")\n",
        "    # Testing the model\n",
        "    # model.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     outputs = model(X_test_tensor)\n",
        "    #     _, predicted = torch.max(outputs, 1)\n",
        "    #     accuracy = torch.sum(predicted == y_test_tensor).item() / len(y_test_tensor)\n",
        "    #     print(f'Accuracy on the test set: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "-Anm2zhINjLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN_3_0zd1WD3"
      },
      "source": [
        "##Classification, Zebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-w9R0eu1Ys-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "33b7a573-fe1c-4db1-9426-11517b3efc21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of points per class\n",
        "num_points = 110\n",
        "\n",
        "# Generate alternating classes along the X-axis\n",
        "x = np.linspace(0, 100, (int) (num_points/10))\n",
        "#repeat X 10 times\n",
        "x = np.repeat(x, 10)\n",
        "y = np.random.rand(num_points)*100\n",
        "labels = np.zeros(num_points)\n",
        "\n",
        "# Assign alternating classes\n",
        "labels[x%20 == 0] = 0\n",
        "labels[x%20 != 0] = 1\n",
        "\n",
        "# Plot the generated data\n",
        "plt.scatter(x, y, c=labels, cmap='viridis', marker='.')\n",
        "plt.title('Synthetic Zebra-Style Classification Dataset')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y (not relevant)')\n",
        "# plt.colorbar(ticks=[0, 1], label='Class')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJhXEd7TAoGT"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(np.column_stack((x, y)), dtype=torch.float32)\n",
        "ys = torch.tensor(labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0haNFtPWBILt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b99d177-ca3e-46ca-a8a7-9fdd5cfa7613"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([110, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl3ZyEY0CZY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48599567-3141-4128-984e-cf071508fdd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECZsy8w5EChV",
        "outputId": "c497e490-0bba-4f72-c009-dce2ed6ff77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lDS9m8xpXXt"
      },
      "source": [
        "##Classification, Special Zebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "icfMjXivpkLI",
        "outputId": "4738a6af-6cea-4357-b62f-b7af92f0fccd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of points per class\n",
        "num_points = 110\n",
        "\n",
        "# Generate alternating classes along the X-axis\n",
        "x1 = np.linspace(0, 100, (int) (num_points/10))\n",
        "#repeat X 10 times\n",
        "x1 = np.repeat(x1, 10)\n",
        "y1 = np.random.rand(num_points)*100\n",
        "labels1 = np.zeros(num_points)\n",
        "\n",
        "# Assign alternating classes\n",
        "labels1[x1%20 == 0] = 0\n",
        "labels1[x1%20 != 0] = 1\n",
        "\n",
        "# Generate alternating classes along the X-axis\n",
        "y2 = np.linspace(0, 100, (int) (num_points/10))\n",
        "#repeat X 10 times\n",
        "y2 = np.repeat(y2, 10)\n",
        "x2 = 100 + np.random.rand(num_points)*100\n",
        "labels2 = np.zeros(num_points)\n",
        "\n",
        "# Assign alternating classes\n",
        "labels2[y2%20 == 0] = 0\n",
        "labels2[y2%20 != 0] = 1\n",
        "\n",
        "x = np.concatenate((x1,x2))\n",
        "y = np.concatenate((y1,y2))\n",
        "labels = np.concatenate((labels1,labels2))\n",
        "\n",
        "\n",
        "# Plot the generated data\n",
        "plt.scatter(x, y, c=labels, cmap='viridis', marker='.')\n",
        "plt.title('Synthetic Zebra-Style Classification Dataset')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "# plt.colorbar(ticks=[0, 1], label='Class')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNjOT7AqpnTT"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(np.column_stack((x, y)), dtype=torch.float32)\n",
        "ys = torch.tensor(labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cZECSDXY7V"
      },
      "source": [
        "##Classification, Breast Cancer Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwIvazPxXYr0"
      },
      "outputs": [],
      "source": [
        "# prompt: load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "Xs = cancer.data\n",
        "ys = cancer.target\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powiAg8RL5L7"
      },
      "source": [
        "##Classification, BAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmIR8VjJL8ck"
      },
      "source": [
        "Ziwei: Download the file below from https://archive.ics.uci.edu/dataset/12/balance+scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hgdQkbqL7Wt"
      },
      "outputs": [],
      "source": [
        "with open(\"balance-scale.data\",\"r\") as filef:\n",
        "    bal_file = filef.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ39n1GOMElm"
      },
      "outputs": [],
      "source": [
        "Xs = []\n",
        "ys = []\n",
        "for line in bal_file:\n",
        "    Xs.append([int(line[2]),int(line[4]),int(line[6]),int(line[8])])\n",
        "    if line[0] == 'L':\n",
        "        ys.append(0)\n",
        "    elif line[0] == 'B':\n",
        "        ys.append(1)\n",
        "    elif line[0] == 'R':\n",
        "        ys.append(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_1K8YFPMGWt"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(Xs).float()\n",
        "ys = torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6BE0VHTogJo"
      },
      "source": [
        "##Regression, California Housing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMOxxo_MKAci"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california_housing = fetch_california_housing()\n",
        "Xs = california_housing.data\n",
        "ys = california_housing.target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iix84gG6KLYU",
        "outputId": "100a2d55-1138-471e-9e3b-b42608632b35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   8.3252    ,   41.        ,    6.98412698,    1.02380952,\n",
              "        322.        ,    2.55555556,   37.88      , -122.23      ])"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN2dWEpzKM33",
        "outputId": "3c084f1e-b42e-441a-b501-ae6b49567c70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.526"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzGjqqSSV9Fu",
        "outputId": "b78f5db7-e0ee-468f-e6a3-f1a372977319"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.526  , 3.585  , 3.521  , 3.413  , 3.422  , 2.697  , 2.992  ,\n",
              "       2.414  , 2.267  , 2.611  , 2.815  , 2.418  , 2.135  , 1.913  ,\n",
              "       1.592  , 1.4    , 1.525  , 1.555  , 1.587  , 1.629  , 1.475  ,\n",
              "       1.598  , 1.139  , 0.997  , 1.326  , 1.075  , 0.938  , 1.055  ,\n",
              "       1.089  , 1.32   , 1.223  , 1.152  , 1.104  , 1.049  , 1.097  ,\n",
              "       0.972  , 1.045  , 1.039  , 1.914  , 1.76   , 1.554  , 1.5    ,\n",
              "       1.188  , 1.888  , 1.844  , 1.823  , 1.425  , 1.375  , 1.875  ,\n",
              "       1.125  , 1.719  , 0.938  , 0.975  , 1.042  , 0.875  , 0.831  ,\n",
              "       0.875  , 0.853  , 0.803  , 0.6    , 0.757  , 0.75   , 0.861  ,\n",
              "       0.761  , 0.735  , 0.784  , 0.844  , 0.813  , 0.85   , 1.292  ,\n",
              "       0.825  , 0.952  , 0.75   , 0.675  , 1.375  , 1.775  , 1.021  ,\n",
              "       1.083  , 1.125  , 1.313  , 1.625  , 1.125  , 1.125  , 1.375  ,\n",
              "       1.188  , 0.982  , 1.188  , 1.625  , 1.375  , 5.00001, 1.625  ,\n",
              "       1.375  , 1.625  , 1.875  , 1.792  , 1.3    , 1.838  , 1.25   ,\n",
              "       1.7    , 1.931  ])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ],
      "source": [
        "ys[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFuta_wnUjCE"
      },
      "outputs": [],
      "source": [
        "# prompt: Convert Xs and ys to tensors for pytorch\n",
        "\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJWpLsWXVZZg",
        "outputId": "4e749741-abaf-4b32-cfcf-2e2a9b98d49a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20640, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poEqTR7bZuRf"
      },
      "outputs": [],
      "source": [
        "# prompt: downsize the data set to 10%\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "_, Xs, _, ys = train_test_split(Xs, ys, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB_cOi7sZ3rB",
        "outputId": "f0cdbdda-d336-4c49-910b-03ff370b8b92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2064, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdMkoZr_KgZX"
      },
      "source": [
        "##Regression, Abalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTi9rxq1Kfl5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from urllib import request\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "\n",
        "# Download the Abalone dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
        "response = request.urlopen(url)\n",
        "abalone_data = response.read().decode(\"utf-8\").splitlines()\n",
        "\n",
        "# Process and convert the data to PyTorch tensors\n",
        "data = [line.strip().split(',') for line in abalone_data]\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# categories = ['M', 'F', 'I']\n",
        "# label_encoder = OneHotEncoder(categories=[categories])\n",
        "# label_encoder.fit(data)\n",
        "\n",
        "def encode_sex(sex):\n",
        "    if sex == 'M':\n",
        "        return [1, 0, 0]\n",
        "    elif sex == 'F':\n",
        "        return [0, 1, 0]\n",
        "    elif sex == 'I':\n",
        "        return [0, 0, 1]\n",
        "\n",
        "for row in data:\n",
        "    # One-hot encode the 'Sex' feature\n",
        "    # sex_encoded = label_encoder.transform([[row[0]]])[0]\n",
        "    sex_encoded = encode_sex(row[0])\n",
        "\n",
        "    # Convert the row to float and extract the target variable ('Rings')\n",
        "    X.append(sex_encoded + list(map(float, row[1:-1])))\n",
        "    y.append(float(row[-1]))\n",
        "\n",
        "    # # Encode the categorical 'Sex' feature\n",
        "    # row[0] = label_encoder.transform([row[0]])[0]\n",
        "    # # Convert the row to float and extract the target variable ('Rings')\n",
        "    # X.append(list(map(float, row[:-1])))\n",
        "    # y.append(float(row[-1]))\n",
        "\n",
        "Xs = torch.tensor(X, dtype=torch.float32)\n",
        "ys = torch.tensor(y, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vxYpkSzeWw_"
      },
      "source": [
        "##Regression, Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP127Lw6gNFk"
      },
      "outputs": [],
      "source": [
        "# prompt: load the diabetes dataset from sklearn\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "Xs = diabetes.data\n",
        "ys = diabetes.target\n",
        "# prompt: convert Xs and ys to float tensor\n",
        "\n",
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZdQOsd0gdG1",
        "outputId": "80fc627d-11ec-48ac-f236-e4eabde75e9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n",
              "         0.0199, -0.0176])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk8Iw3eL6SF2",
        "outputId": "ce75f36e-a3a9-4e4d-abcb-8992ba387f1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([0.1107, 0.0507, 0.1706, 0.1320, 0.1539, 0.1988, 0.1812, 0.1852, 0.1336,\n",
              "        0.1356]),\n",
              "indices=tensor([204,   0, 367, 340, 230, 123,  58, 123,  23,  23]))"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "torch.max(Xs,dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTuguLpxgeVW",
        "outputId": "b6cad194-46d1-49ef-8213-47a01abdd21e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(151.)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fprm-3uOgfbm",
        "outputId": "b4954894-eaa8-40c2-81c6-62f911e78a30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([442, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "Xs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyOuSplUiUY"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ta_zinTqY48"
      },
      "source": [
        "##Regression, Patchy classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_4xNhFTfQP_"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZshHTL0qcnZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C29ELkyUUzEf"
      },
      "source": [
        "##Regression, Body Fat\n",
        "\n",
        "Tried this data set on multiple settings. Our NN-kNN consistently perform poorly on this when compared to other methods.\n",
        "\n",
        "https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset?resource=download\n",
        "\n",
        "https://www.kaggle.com/code/casper6290/bodyfat-prediction#1-|-Importing-Libraries-and-Loading-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmlUIHa3cFLy",
        "outputId": "c6fc75df-df48-4b07-f790-172158637b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "umDa4rqucbgk",
        "outputId": "45a50d9f-015c-4941-b203-3e43a12e6698"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Density  BodyFat  Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  \\\n",
              "0   1.0708     12.3   23  154.25   67.75  36.2   93.1     85.2   94.5   59.0   \n",
              "1   1.0853      6.1   22  173.25   72.25  38.5   93.6     83.0   98.7   58.7   \n",
              "2   1.0414     25.3   22  154.00   66.25  34.0   95.8     87.9   99.2   59.6   \n",
              "3   1.0751     10.4   26  184.75   72.25  37.4  101.8     86.4  101.2   60.1   \n",
              "4   1.0340     28.7   24  184.25   71.25  34.4   97.3    100.0  101.9   63.2   \n",
              "\n",
              "   Knee  Ankle  Biceps  Forearm  Wrist  \n",
              "0  37.3   21.9    32.0     27.4   17.1  \n",
              "1  37.3   23.4    30.5     28.9   18.2  \n",
              "2  38.9   24.0    28.8     25.2   16.6  \n",
              "3  37.3   22.8    32.4     29.4   18.2  \n",
              "4  42.2   24.0    32.2     27.7   17.7  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bda82faf-eb0e-49d0-a727-9b4c6f16f4b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Density</th>\n",
              "      <th>BodyFat</th>\n",
              "      <th>Age</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Height</th>\n",
              "      <th>Neck</th>\n",
              "      <th>Chest</th>\n",
              "      <th>Abdomen</th>\n",
              "      <th>Hip</th>\n",
              "      <th>Thigh</th>\n",
              "      <th>Knee</th>\n",
              "      <th>Ankle</th>\n",
              "      <th>Biceps</th>\n",
              "      <th>Forearm</th>\n",
              "      <th>Wrist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0708</td>\n",
              "      <td>12.3</td>\n",
              "      <td>23</td>\n",
              "      <td>154.25</td>\n",
              "      <td>67.75</td>\n",
              "      <td>36.2</td>\n",
              "      <td>93.1</td>\n",
              "      <td>85.2</td>\n",
              "      <td>94.5</td>\n",
              "      <td>59.0</td>\n",
              "      <td>37.3</td>\n",
              "      <td>21.9</td>\n",
              "      <td>32.0</td>\n",
              "      <td>27.4</td>\n",
              "      <td>17.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0853</td>\n",
              "      <td>6.1</td>\n",
              "      <td>22</td>\n",
              "      <td>173.25</td>\n",
              "      <td>72.25</td>\n",
              "      <td>38.5</td>\n",
              "      <td>93.6</td>\n",
              "      <td>83.0</td>\n",
              "      <td>98.7</td>\n",
              "      <td>58.7</td>\n",
              "      <td>37.3</td>\n",
              "      <td>23.4</td>\n",
              "      <td>30.5</td>\n",
              "      <td>28.9</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0414</td>\n",
              "      <td>25.3</td>\n",
              "      <td>22</td>\n",
              "      <td>154.00</td>\n",
              "      <td>66.25</td>\n",
              "      <td>34.0</td>\n",
              "      <td>95.8</td>\n",
              "      <td>87.9</td>\n",
              "      <td>99.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>38.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>28.8</td>\n",
              "      <td>25.2</td>\n",
              "      <td>16.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0751</td>\n",
              "      <td>10.4</td>\n",
              "      <td>26</td>\n",
              "      <td>184.75</td>\n",
              "      <td>72.25</td>\n",
              "      <td>37.4</td>\n",
              "      <td>101.8</td>\n",
              "      <td>86.4</td>\n",
              "      <td>101.2</td>\n",
              "      <td>60.1</td>\n",
              "      <td>37.3</td>\n",
              "      <td>22.8</td>\n",
              "      <td>32.4</td>\n",
              "      <td>29.4</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0340</td>\n",
              "      <td>28.7</td>\n",
              "      <td>24</td>\n",
              "      <td>184.25</td>\n",
              "      <td>71.25</td>\n",
              "      <td>34.4</td>\n",
              "      <td>97.3</td>\n",
              "      <td>100.0</td>\n",
              "      <td>101.9</td>\n",
              "      <td>63.2</td>\n",
              "      <td>42.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>27.7</td>\n",
              "      <td>17.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bda82faf-eb0e-49d0-a727-9b4c6f16f4b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bda82faf-eb0e-49d0-a727-9b4c6f16f4b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bda82faf-eb0e-49d0-a727-9b4c6f16f4b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11648cd0-1140-4c44-a931-19b98a88084d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11648cd0-1140-4c44-a931-19b98a88084d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11648cd0-1140-4c44-a931-19b98a88084d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "#\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2023 research/NN-kNN/bodyfat.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqFrJNY2d4uL",
        "outputId": "1a2f1698-fcd1-4de5-a318-986d3fb0fdff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_j5YekSd_3A"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['BodyFat','Density'],axis=1)\n",
        "y = df['Density']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "E7BzcLAveA9Q",
        "outputId": "d309aec9-ee7f-496f-e074-631af7038c9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  Knee  Ankle  \\\n",
              "0   23  154.25   67.75  36.2   93.1     85.2   94.5   59.0  37.3   21.9   \n",
              "1   22  173.25   72.25  38.5   93.6     83.0   98.7   58.7  37.3   23.4   \n",
              "2   22  154.00   66.25  34.0   95.8     87.9   99.2   59.6  38.9   24.0   \n",
              "3   26  184.75   72.25  37.4  101.8     86.4  101.2   60.1  37.3   22.8   \n",
              "4   24  184.25   71.25  34.4   97.3    100.0  101.9   63.2  42.2   24.0   \n",
              "\n",
              "   Biceps  Forearm  Wrist        Bmi  \n",
              "0    32.0     27.4   17.1  23.624460  \n",
              "1    30.5     28.9   18.2  23.332048  \n",
              "2    28.8     25.2   16.6  24.666315  \n",
              "3    32.4     29.4   18.2  24.880784  \n",
              "4    32.2     27.7   17.7  25.514854  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5d7710d-4897-4fbb-9df7-45f177d10a39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Height</th>\n",
              "      <th>Neck</th>\n",
              "      <th>Chest</th>\n",
              "      <th>Abdomen</th>\n",
              "      <th>Hip</th>\n",
              "      <th>Thigh</th>\n",
              "      <th>Knee</th>\n",
              "      <th>Ankle</th>\n",
              "      <th>Biceps</th>\n",
              "      <th>Forearm</th>\n",
              "      <th>Wrist</th>\n",
              "      <th>Bmi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "      <td>154.25</td>\n",
              "      <td>67.75</td>\n",
              "      <td>36.2</td>\n",
              "      <td>93.1</td>\n",
              "      <td>85.2</td>\n",
              "      <td>94.5</td>\n",
              "      <td>59.0</td>\n",
              "      <td>37.3</td>\n",
              "      <td>21.9</td>\n",
              "      <td>32.0</td>\n",
              "      <td>27.4</td>\n",
              "      <td>17.1</td>\n",
              "      <td>23.624460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22</td>\n",
              "      <td>173.25</td>\n",
              "      <td>72.25</td>\n",
              "      <td>38.5</td>\n",
              "      <td>93.6</td>\n",
              "      <td>83.0</td>\n",
              "      <td>98.7</td>\n",
              "      <td>58.7</td>\n",
              "      <td>37.3</td>\n",
              "      <td>23.4</td>\n",
              "      <td>30.5</td>\n",
              "      <td>28.9</td>\n",
              "      <td>18.2</td>\n",
              "      <td>23.332048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>154.00</td>\n",
              "      <td>66.25</td>\n",
              "      <td>34.0</td>\n",
              "      <td>95.8</td>\n",
              "      <td>87.9</td>\n",
              "      <td>99.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>38.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>28.8</td>\n",
              "      <td>25.2</td>\n",
              "      <td>16.6</td>\n",
              "      <td>24.666315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>184.75</td>\n",
              "      <td>72.25</td>\n",
              "      <td>37.4</td>\n",
              "      <td>101.8</td>\n",
              "      <td>86.4</td>\n",
              "      <td>101.2</td>\n",
              "      <td>60.1</td>\n",
              "      <td>37.3</td>\n",
              "      <td>22.8</td>\n",
              "      <td>32.4</td>\n",
              "      <td>29.4</td>\n",
              "      <td>18.2</td>\n",
              "      <td>24.880784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>184.25</td>\n",
              "      <td>71.25</td>\n",
              "      <td>34.4</td>\n",
              "      <td>97.3</td>\n",
              "      <td>100.0</td>\n",
              "      <td>101.9</td>\n",
              "      <td>63.2</td>\n",
              "      <td>42.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>27.7</td>\n",
              "      <td>17.7</td>\n",
              "      <td>25.514854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5d7710d-4897-4fbb-9df7-45f177d10a39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5d7710d-4897-4fbb-9df7-45f177d10a39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5d7710d-4897-4fbb-9df7-45f177d10a39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-858cdf66-914c-40c5-bdef-2ea1e7963d66\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-858cdf66-914c-40c5-bdef-2ea1e7963d66')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-858cdf66-914c-40c5-bdef-2ea1e7963d66 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "X['Bmi']=703*X['Weight']/(X['Height']*X['Height'])\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4tizcri8eDvY",
        "outputId": "69125157-63aa-4b64-f683-0ce337f2c564"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Age  Neck  Knee  Ankle  Biceps  Forearm  Wrist        Bmi   ACratio  \\\n",
              "0   23  36.2  37.3   21.9    32.0     27.4   17.1  23.624460  0.915145   \n",
              "1   22  38.5  37.3   23.4    30.5     28.9   18.2  23.332048  0.886752   \n",
              "2   22  34.0  38.9   24.0    28.8     25.2   16.6  24.666315  0.917537   \n",
              "3   26  37.4  37.3   22.8    32.4     29.4   18.2  24.880784  0.848723   \n",
              "4   24  34.4  42.2   24.0    32.2     27.7   17.7  25.514854  1.027749   \n",
              "\n",
              "    HTratio  \n",
              "0  1.601695  \n",
              "1  1.681431  \n",
              "2  1.664430  \n",
              "3  1.683860  \n",
              "4  1.612342  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5390d7fb-e5af-4f8d-844a-d791feeef1d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Neck</th>\n",
              "      <th>Knee</th>\n",
              "      <th>Ankle</th>\n",
              "      <th>Biceps</th>\n",
              "      <th>Forearm</th>\n",
              "      <th>Wrist</th>\n",
              "      <th>Bmi</th>\n",
              "      <th>ACratio</th>\n",
              "      <th>HTratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "      <td>36.2</td>\n",
              "      <td>37.3</td>\n",
              "      <td>21.9</td>\n",
              "      <td>32.0</td>\n",
              "      <td>27.4</td>\n",
              "      <td>17.1</td>\n",
              "      <td>23.624460</td>\n",
              "      <td>0.915145</td>\n",
              "      <td>1.601695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22</td>\n",
              "      <td>38.5</td>\n",
              "      <td>37.3</td>\n",
              "      <td>23.4</td>\n",
              "      <td>30.5</td>\n",
              "      <td>28.9</td>\n",
              "      <td>18.2</td>\n",
              "      <td>23.332048</td>\n",
              "      <td>0.886752</td>\n",
              "      <td>1.681431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>34.0</td>\n",
              "      <td>38.9</td>\n",
              "      <td>24.0</td>\n",
              "      <td>28.8</td>\n",
              "      <td>25.2</td>\n",
              "      <td>16.6</td>\n",
              "      <td>24.666315</td>\n",
              "      <td>0.917537</td>\n",
              "      <td>1.664430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>37.4</td>\n",
              "      <td>37.3</td>\n",
              "      <td>22.8</td>\n",
              "      <td>32.4</td>\n",
              "      <td>29.4</td>\n",
              "      <td>18.2</td>\n",
              "      <td>24.880784</td>\n",
              "      <td>0.848723</td>\n",
              "      <td>1.683860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>34.4</td>\n",
              "      <td>42.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>27.7</td>\n",
              "      <td>17.7</td>\n",
              "      <td>25.514854</td>\n",
              "      <td>1.027749</td>\n",
              "      <td>1.612342</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5390d7fb-e5af-4f8d-844a-d791feeef1d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5390d7fb-e5af-4f8d-844a-d791feeef1d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5390d7fb-e5af-4f8d-844a-d791feeef1d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1c6aef9a-3f74-4d34-bd63-13611bdb92d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1c6aef9a-3f74-4d34-bd63-13611bdb92d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1c6aef9a-3f74-4d34-bd63-13611bdb92d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "X['ACratio'] = X['Abdomen']/X['Chest']\n",
        "X['HTratio'] = X['Hip']/X['Thigh']\n",
        "X.drop(['Weight','Height','Abdomen','Chest','Hip','Thigh'],axis=1,inplace=True)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJUKzpoHeSax",
        "outputId": "76f3539e-d896-48cd-9df5-7619943d062e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(242, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "z = np.abs(stats.zscore(X))\n",
        "\n",
        "#only keep rows in dataframe with all z-scores less than absolute value of 3\n",
        "X_clean = X[(z<3).all(axis=1)]\n",
        "y_clean = y[(z<3).all(axis=1)]\n",
        "#find how many rows are left in the dataframe\n",
        "X_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4haAdPoVeXfc"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor( X_clean.to_numpy(), dtype=torch.float32)\n",
        "ys = torch.tensor( y_clean.to_numpy(), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efTT8qVCeipZ",
        "outputId": "d4b80af8-7494-414b-ed8c-e466e3e471f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([23.0000, 36.2000, 37.3000, 21.9000, 32.0000, 27.4000, 17.1000, 23.6245,\n",
              "         0.9151,  1.6017])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "Xs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hTs_aikeg_p",
        "outputId": "edfc570f-7c47-482d-85d6-7187a8ca59e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0708)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "ys[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swy1EhJD31r6"
      },
      "source": [
        "##Regression, Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GLpgoy_36B9"
      },
      "outputs": [],
      "source": [
        "Xs = np.load(\"part_features.npy\")\n",
        "ys = np.load(\"part_targets.npy\")\n",
        "#These two files are in the nn-Knn folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z6Psjbh4CGD"
      },
      "outputs": [],
      "source": [
        "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
        "ys = torch.tensor(ys, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXPNaMjp4DpP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "_, Xs, _, ys = train_test_split(Xs, ys, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORxZVyTS4FWG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo4BdbmCFC0d"
      },
      "source": [
        "##Sanity Check: A standard NN for regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM_WhRJqD11s"
      },
      "outputs": [],
      "source": [
        "# prompt: a standard neural network with 3 fully connected layers for regression\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class RegressionNet(nn.Module):\n",
        "#   def __init__(self, input_size):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(input_size, 32)\n",
        "#     self.fc2 = nn.Linear(32, 16)\n",
        "#     self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = F.relu(self.fc1(x))\n",
        "#     x = F.relu(self.fc2(x))\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a standard neural network with 3 fully connected layers for regression\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class RegressionNet(nn.Module):\n",
        "#   def __init__(self, input_size):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(input_size, 32)\n",
        "#     self.fc2 = nn.Linear(32, 16)\n",
        "#     self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = F.relu(self.fc1(x))\n",
        "#     x = F.relu(self.fc2(x))\n",
        "#     x = self.fc3(x)\n",
        "#     return x\n",
        "\n",
        "class RegressionNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionNet, self).__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(input_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 1)\n",
        "            )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x).squeeze()"
      ],
      "metadata": {
        "id": "k1g3svUFNrsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cme3KleQEdQA"
      },
      "outputs": [],
      "source": [
        "training_epochs = 100\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "# prompt: Train and test the RegressionNet on Xs and ys\n",
        "\n",
        "k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in k_fold.split(Xs):\n",
        "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "  y_train, y_test = ys[train_index], ys[test_index]\n",
        "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = RegressionNet(Xs.shape[1])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  for epoch in range(training_epochs):\n",
        "    epoch_msg = True\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "      model.train()\n",
        "      # Forward pass\n",
        "      outputs = model(X_train_batch)\n",
        "      loss = criterion(outputs, y_train_batch)\n",
        "\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch_msg and (epoch + 1) % 100 == 0:\n",
        "        epoch_msg = False\n",
        "        print(f'Epoch [{epoch + 1}/{training_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "  # Testing the model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    loss = criterion(outputs, y_test)\n",
        "    print(f'Loss on the test set: {loss.item()}')\n",
        "    accuracies.append(loss.item())\n",
        "print(f'Average loss on the test set: {sum(accuracies)/len(accuracies)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC3uDRqv8T0V"
      },
      "source": [
        "For the vanilla neural network\n",
        "\n",
        "```\n",
        "Average loss on the test set: 1.3484851598739624\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOGHMEAqaoaq"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9_2oHTKXHIR"
      },
      "outputs": [],
      "source": [
        "# prompt: print the number of parameters in model\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lcYtG21v45O"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPlAPD92JC_j"
      },
      "source": [
        "##Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "newly added here."
      ],
      "metadata": {
        "id": "SqrFf1LvSw7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: center and scale to normalize my Xs and ys\n",
        "\n",
        "# def standardize_tensor(input_tensor):\n",
        "#     mean = input_tensor.mean()\n",
        "#     std = input_tensor.std()\n",
        "#     standardized_tensor = (input_tensor - mean) / std\n",
        "#     return standardized_tensor\n",
        "\n",
        "# Xs = standardize_tensor(Xs)"
      ],
      "metadata": {
        "id": "vlnbKhrYSpNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28HWRvIWsz65"
      },
      "outputs": [],
      "source": [
        "training_epochs = 1000\n",
        "learning_rate = 0.01 #0.01 #0.0001\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "hidden_layers = False\n",
        "\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Z8bG1lwYT9",
        "outputId": "f7ed6742-6db1-4f97-8417-c41e095f9f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.36109018325805664\n",
            "Epoch [4/1000], Loss: 0.3528931736946106\n",
            "Epoch [6/1000], Loss: 0.34503039717674255\n",
            "Epoch [8/1000], Loss: 0.3374462425708771\n",
            "Epoch [10/1000], Loss: 0.3300359845161438\n",
            "Epoch [12/1000], Loss: 0.3226627707481384\n",
            "Epoch [14/1000], Loss: 0.31520581245422363\n",
            "Epoch [16/1000], Loss: 0.3076019883155823\n",
            "Epoch [18/1000], Loss: 0.29984423518180847\n",
            "Epoch [20/1000], Loss: 0.2919541895389557\n",
            "Epoch [22/1000], Loss: 0.28395727276802063\n",
            "Epoch [24/1000], Loss: 0.2758707106113434\n",
            "Epoch [26/1000], Loss: 0.26770269870758057\n",
            "Epoch [28/1000], Loss: 0.2594590187072754\n",
            "Epoch [30/1000], Loss: 0.25115248560905457\n",
            "Epoch [32/1000], Loss: 0.24280905723571777\n",
            "Epoch [34/1000], Loss: 0.23446539044380188\n",
            "Epoch [36/1000], Loss: 0.22615747153759003\n",
            "Epoch [38/1000], Loss: 0.21790695190429688\n",
            "Epoch [40/1000], Loss: 0.20971634984016418\n",
            "Epoch [42/1000], Loss: 0.20157894492149353\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.3641025125980377\n",
            "Epoch [4/1000], Loss: 0.3565458059310913\n",
            "Epoch [6/1000], Loss: 0.3492441177368164\n",
            "Epoch [8/1000], Loss: 0.3420189321041107\n",
            "Epoch [10/1000], Loss: 0.33470404148101807\n",
            "Epoch [12/1000], Loss: 0.32727980613708496\n",
            "Epoch [14/1000], Loss: 0.3197961151599884\n",
            "Epoch [16/1000], Loss: 0.3122842311859131\n",
            "Epoch [18/1000], Loss: 0.3047322630882263\n",
            "Epoch [20/1000], Loss: 0.29710033535957336\n",
            "Epoch [22/1000], Loss: 0.2893563508987427\n",
            "Epoch [24/1000], Loss: 0.28149574995040894\n",
            "Epoch [26/1000], Loss: 0.27353593707084656\n",
            "Epoch [28/1000], Loss: 0.2655012309551239\n",
            "Epoch [30/1000], Loss: 0.25741466879844666\n",
            "Epoch [32/1000], Loss: 0.24929693341255188\n",
            "Epoch [34/1000], Loss: 0.241163969039917\n",
            "Epoch [36/1000], Loss: 0.23301945626735687\n",
            "Epoch [38/1000], Loss: 0.2248464822769165\n",
            "Epoch [40/1000], Loss: 0.21661239862442017\n",
            "Epoch [42/1000], Loss: 0.20829205214977264\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.35819312930107117\n",
            "Epoch [4/1000], Loss: 0.3504140079021454\n",
            "Epoch [6/1000], Loss: 0.3429338335990906\n",
            "Epoch [8/1000], Loss: 0.3356233537197113\n",
            "Epoch [10/1000], Loss: 0.3283003866672516\n",
            "Epoch [12/1000], Loss: 0.32085832953453064\n",
            "Epoch [14/1000], Loss: 0.31329914927482605\n",
            "Epoch [16/1000], Loss: 0.30566343665122986\n",
            "Epoch [18/1000], Loss: 0.2979775667190552\n",
            "Epoch [20/1000], Loss: 0.2902383804321289\n",
            "Epoch [22/1000], Loss: 0.2824212312698364\n",
            "Epoch [24/1000], Loss: 0.2745018005371094\n",
            "Epoch [26/1000], Loss: 0.2664757966995239\n",
            "Epoch [28/1000], Loss: 0.2583635449409485\n",
            "Epoch [30/1000], Loss: 0.2502003014087677\n",
            "Epoch [32/1000], Loss: 0.24202221632003784\n",
            "Epoch [34/1000], Loss: 0.23385480046272278\n",
            "Epoch [36/1000], Loss: 0.22570742666721344\n",
            "Epoch [38/1000], Loss: 0.21757657825946808\n",
            "Epoch [40/1000], Loss: 0.20946159958839417\n",
            "Epoch [42/1000], Loss: 0.20138919353485107\n",
            "Epoch [44/1000], Loss: 0.19343025982379913\n",
            "Epoch [46/1000], Loss: 0.18568386137485504\n",
            "Epoch [48/1000], Loss: 0.17822733521461487\n",
            "Epoch [50/1000], Loss: 0.1710672229528427\n",
            "Epoch [52/1000], Loss: 0.1641196310520172\n",
            "Epoch [54/1000], Loss: 0.15721046924591064\n",
            "Epoch [56/1000], Loss: 0.15008644759655\n",
            "Epoch [58/1000], Loss: 0.14244909584522247\n",
            "Epoch [60/1000], Loss: 0.13400942087173462\n",
            "Epoch [62/1000], Loss: 0.12444531172513962\n",
            "Epoch [64/1000], Loss: 0.1131749078631401\n",
            "Epoch [66/1000], Loss: 0.09895980358123779\n",
            "Epoch [68/1000], Loss: 0.08013749122619629\n",
            "Epoch [70/1000], Loss: 0.05779246985912323\n",
            "Epoch [72/1000], Loss: 0.0338616706430912\n",
            "Epoch [74/1000], Loss: 0.01350646186619997\n",
            "Epoch [76/1000], Loss: 0.002122882753610611\n",
            "Epoch [78/1000], Loss: 3.726291470229626e-05\n",
            "Epoch [80/1000], Loss: 5.511277322511887e-06\n",
            "Epoch [82/1000], Loss: 1.660669113334734e-05\n",
            "Epoch [84/1000], Loss: 0.0002144726604456082\n",
            "Epoch [86/1000], Loss: 0.0008307709940709174\n",
            "Epoch [88/1000], Loss: 0.0016213476192206144\n",
            "Epoch [90/1000], Loss: 0.0021706202533096075\n",
            "Epoch [92/1000], Loss: 0.0022838886361569166\n",
            "Epoch [94/1000], Loss: 0.002017444698140025\n",
            "Epoch [96/1000], Loss: 0.0015555218560621142\n",
            "Epoch [98/1000], Loss: 0.001081683672964573\n",
            "Epoch [100/1000], Loss: 0.0006978853489272296\n",
            "Epoch [102/1000], Loss: 0.0004281838773749769\n",
            "Epoch [104/1000], Loss: 0.0002556240069679916\n",
            "Epoch [106/1000], Loss: 0.0001509092398919165\n",
            "Epoch [108/1000], Loss: 8.947180322138593e-05\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.3636360466480255\n",
            "Epoch [4/1000], Loss: 0.35568881034851074\n",
            "Epoch [6/1000], Loss: 0.34808090329170227\n",
            "Epoch [8/1000], Loss: 0.34074661135673523\n",
            "Epoch [10/1000], Loss: 0.3335648477077484\n",
            "Epoch [12/1000], Loss: 0.3263891637325287\n",
            "Epoch [14/1000], Loss: 0.3191118836402893\n",
            "Epoch [16/1000], Loss: 0.31169360876083374\n",
            "Epoch [18/1000], Loss: 0.304139643907547\n",
            "Epoch [20/1000], Loss: 0.29646581411361694\n",
            "Epoch [22/1000], Loss: 0.28867945075035095\n",
            "Epoch [24/1000], Loss: 0.2807755470275879\n",
            "Epoch [26/1000], Loss: 0.27274543046951294\n",
            "Epoch [28/1000], Loss: 0.264589786529541\n",
            "Epoch [30/1000], Loss: 0.25632932782173157\n",
            "Epoch [32/1000], Loss: 0.24800541996955872\n",
            "Epoch [34/1000], Loss: 0.2396673560142517\n",
            "Epoch [36/1000], Loss: 0.23135560750961304\n",
            "Epoch [38/1000], Loss: 0.2230890989303589\n",
            "Epoch [40/1000], Loss: 0.2148684710264206\n",
            "Epoch [42/1000], Loss: 0.20669463276863098\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.36607885360717773\n",
            "Epoch [4/1000], Loss: 0.35872623324394226\n",
            "Epoch [6/1000], Loss: 0.35165655612945557\n",
            "Epoch [8/1000], Loss: 0.3447486162185669\n",
            "Epoch [10/1000], Loss: 0.3378179967403412\n",
            "Epoch [12/1000], Loss: 0.3307305574417114\n",
            "Epoch [14/1000], Loss: 0.32345712184906006\n",
            "Epoch [16/1000], Loss: 0.31601855158805847\n",
            "Epoch [18/1000], Loss: 0.308431476354599\n",
            "Epoch [20/1000], Loss: 0.3006899356842041\n",
            "Epoch [22/1000], Loss: 0.2927693724632263\n",
            "Epoch [24/1000], Loss: 0.28464579582214355\n",
            "Epoch [26/1000], Loss: 0.27631616592407227\n",
            "Epoch [28/1000], Loss: 0.2678084075450897\n",
            "Epoch [30/1000], Loss: 0.25917714834213257\n",
            "Epoch [32/1000], Loss: 0.25049033761024475\n",
            "Epoch [34/1000], Loss: 0.24181301891803741\n",
            "Epoch [36/1000], Loss: 0.23319388926029205\n",
            "Epoch [38/1000], Loss: 0.2246590405702591\n",
            "Epoch [40/1000], Loss: 0.21621915698051453\n",
            "Epoch [42/1000], Loss: 0.20788788795471191\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.3606548607349396\n",
            "Epoch [4/1000], Loss: 0.35244277119636536\n",
            "Epoch [6/1000], Loss: 0.34455886483192444\n",
            "Epoch [8/1000], Loss: 0.3369448184967041\n",
            "Epoch [10/1000], Loss: 0.3294927179813385\n",
            "Epoch [12/1000], Loss: 0.32206064462661743\n",
            "Epoch [14/1000], Loss: 0.31452035903930664\n",
            "Epoch [16/1000], Loss: 0.30680033564567566\n",
            "Epoch [18/1000], Loss: 0.29888707399368286\n",
            "Epoch [20/1000], Loss: 0.2907995283603668\n",
            "Epoch [22/1000], Loss: 0.2825656533241272\n",
            "Epoch [24/1000], Loss: 0.27421313524246216\n",
            "Epoch [26/1000], Loss: 0.2657691538333893\n",
            "Epoch [28/1000], Loss: 0.2572662830352783\n",
            "Epoch [30/1000], Loss: 0.24874797463417053\n",
            "Epoch [32/1000], Loss: 0.24026983976364136\n",
            "Epoch [34/1000], Loss: 0.2318936288356781\n",
            "Epoch [36/1000], Loss: 0.22367532551288605\n",
            "Epoch [38/1000], Loss: 0.21565338969230652\n",
            "Epoch [40/1000], Loss: 0.20784208178520203\n",
            "Epoch [42/1000], Loss: 0.2002335786819458\n",
            "Epoch [44/1000], Loss: 0.19280298054218292\n",
            "Epoch [46/1000], Loss: 0.18550877273082733\n",
            "Epoch [48/1000], Loss: 0.17827960848808289\n",
            "Epoch [50/1000], Loss: 0.1709926426410675\n",
            "Epoch [52/1000], Loss: 0.1634722501039505\n",
            "Epoch [54/1000], Loss: 0.15553805232048035\n",
            "Epoch [56/1000], Loss: 0.14708353579044342\n",
            "Epoch [58/1000], Loss: 0.13809062540531158\n",
            "Epoch [60/1000], Loss: 0.1285017430782318\n",
            "Epoch [62/1000], Loss: 0.11800654977560043\n",
            "Epoch [64/1000], Loss: 0.10596536099910736\n",
            "Epoch [66/1000], Loss: 0.09125396609306335\n",
            "Epoch [68/1000], Loss: 0.07249294966459274\n",
            "Epoch [70/1000], Loss: 0.050950028002262115\n",
            "Epoch [72/1000], Loss: 0.030224455520510674\n",
            "Epoch [74/1000], Loss: 0.012652182020246983\n",
            "Epoch [76/1000], Loss: 0.002289013471454382\n",
            "Epoch [78/1000], Loss: 8.449092274531722e-05\n",
            "Epoch [80/1000], Loss: 7.83877658250276e-06\n",
            "Epoch [82/1000], Loss: 2.1103309336467646e-05\n",
            "Epoch [84/1000], Loss: 0.0002423291007289663\n",
            "Epoch [86/1000], Loss: 0.0007017046445980668\n",
            "Epoch [88/1000], Loss: 0.0012865657918155193\n",
            "Epoch [90/1000], Loss: 0.001701667788438499\n",
            "Epoch [92/1000], Loss: 0.0018501757876947522\n",
            "Epoch [94/1000], Loss: 0.0017812338192015886\n",
            "Epoch [96/1000], Loss: 0.0015987035585567355\n",
            "Epoch [98/1000], Loss: 0.001370740938000381\n",
            "Epoch [100/1000], Loss: 0.0011212335666641593\n",
            "Epoch [102/1000], Loss: 0.0008849637233652174\n",
            "Epoch [104/1000], Loss: 0.0006803666474297643\n",
            "Epoch [106/1000], Loss: 0.0005169027135707438\n",
            "Epoch [108/1000], Loss: 0.0003940752358175814\n",
            "Epoch [110/1000], Loss: 0.0003024613542947918\n",
            "Epoch [112/1000], Loss: 0.00023065702407620847\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.35797128081321716\n",
            "Epoch [4/1000], Loss: 0.35003530979156494\n",
            "Epoch [6/1000], Loss: 0.34244871139526367\n",
            "Epoch [8/1000], Loss: 0.3351465165615082\n",
            "Epoch [10/1000], Loss: 0.32800722122192383\n",
            "Epoch [12/1000], Loss: 0.3208887577056885\n",
            "Epoch [14/1000], Loss: 0.3136955499649048\n",
            "Epoch [16/1000], Loss: 0.3064056634902954\n",
            "Epoch [18/1000], Loss: 0.2990432679653168\n",
            "Epoch [20/1000], Loss: 0.29164260625839233\n",
            "Epoch [22/1000], Loss: 0.284229040145874\n",
            "Epoch [24/1000], Loss: 0.27681487798690796\n",
            "Epoch [26/1000], Loss: 0.26940494775772095\n",
            "Epoch [28/1000], Loss: 0.26200419664382935\n",
            "Epoch [30/1000], Loss: 0.2546245753765106\n",
            "Epoch [32/1000], Loss: 0.247284397482872\n",
            "Epoch [34/1000], Loss: 0.239999920129776\n",
            "Epoch [36/1000], Loss: 0.23277419805526733\n",
            "Epoch [38/1000], Loss: 0.22558701038360596\n",
            "Epoch [40/1000], Loss: 0.2183942347764969\n",
            "Epoch [42/1000], Loss: 0.21113993227481842\n",
            "Epoch [44/1000], Loss: 0.203778937458992\n",
            "Epoch [46/1000], Loss: 0.19629321992397308\n",
            "Epoch [48/1000], Loss: 0.18868012726306915\n",
            "Epoch [50/1000], Loss: 0.18091927468776703\n",
            "Epoch [52/1000], Loss: 0.17295797169208527\n",
            "Epoch [54/1000], Loss: 0.1647360473871231\n",
            "Epoch [56/1000], Loss: 0.15619157254695892\n",
            "Epoch [58/1000], Loss: 0.14716771245002747\n",
            "Epoch [60/1000], Loss: 0.13722069561481476\n",
            "Epoch [62/1000], Loss: 0.12552891671657562\n",
            "Epoch [64/1000], Loss: 0.11132556200027466\n",
            "Epoch [66/1000], Loss: 0.09422176331281662\n",
            "Epoch [68/1000], Loss: 0.07385212928056717\n",
            "Epoch [70/1000], Loss: 0.048747096210718155\n",
            "Epoch [72/1000], Loss: 0.022486820816993713\n",
            "Epoch [74/1000], Loss: 0.005660757422447205\n",
            "Epoch [76/1000], Loss: 0.00016532608424313366\n",
            "Epoch [78/1000], Loss: 8.068751412793063e-06\n",
            "Epoch [80/1000], Loss: 1.4321574781206436e-05\n",
            "Epoch [82/1000], Loss: 0.0001733152021188289\n",
            "Epoch [84/1000], Loss: 0.0011051971232518554\n",
            "Epoch [86/1000], Loss: 0.0022728280164301395\n",
            "Epoch [88/1000], Loss: 0.002925123553723097\n",
            "Epoch [90/1000], Loss: 0.0029356388840824366\n",
            "Epoch [92/1000], Loss: 0.0024377156514674425\n",
            "Epoch [94/1000], Loss: 0.0016994966426864266\n",
            "Epoch [96/1000], Loss: 0.0009894042741507292\n",
            "Epoch [98/1000], Loss: 0.0004971025045961142\n",
            "Epoch [100/1000], Loss: 0.00025578448548913\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.35741308331489563\n",
            "Epoch [4/1000], Loss: 0.3491488993167877\n",
            "Epoch [6/1000], Loss: 0.34121984243392944\n",
            "Epoch [8/1000], Loss: 0.33358266949653625\n",
            "Epoch [10/1000], Loss: 0.32615920901298523\n",
            "Epoch [12/1000], Loss: 0.3188382685184479\n",
            "Epoch [14/1000], Loss: 0.31149572134017944\n",
            "Epoch [16/1000], Loss: 0.3040277063846588\n",
            "Epoch [18/1000], Loss: 0.29637518525123596\n",
            "Epoch [20/1000], Loss: 0.2885238230228424\n",
            "Epoch [22/1000], Loss: 0.2804921269416809\n",
            "Epoch [24/1000], Loss: 0.2723177969455719\n",
            "Epoch [26/1000], Loss: 0.2640511393547058\n",
            "Epoch [28/1000], Loss: 0.25574997067451477\n",
            "Epoch [30/1000], Loss: 0.2474748194217682\n",
            "Epoch [32/1000], Loss: 0.23927773535251617\n",
            "Epoch [34/1000], Loss: 0.23118852078914642\n",
            "Epoch [36/1000], Loss: 0.22320371866226196\n",
            "Epoch [38/1000], Loss: 0.21528907120227814\n",
            "Epoch [40/1000], Loss: 0.20740003883838654\n",
            "Epoch [42/1000], Loss: 0.19951550662517548\n",
            "Epoch [44/1000], Loss: 0.19166328012943268\n",
            "Epoch [46/1000], Loss: 0.18391065299510956\n",
            "Epoch [48/1000], Loss: 0.17631466686725616\n",
            "Epoch [50/1000], Loss: 0.16886888444423676\n",
            "Epoch [52/1000], Loss: 0.16150064766407013\n",
            "Epoch [54/1000], Loss: 0.15411075949668884\n",
            "Epoch [56/1000], Loss: 0.14657483994960785\n",
            "Epoch [58/1000], Loss: 0.13865850865840912\n",
            "Epoch [60/1000], Loss: 0.12991173565387726\n",
            "Epoch [62/1000], Loss: 0.11965944617986679\n",
            "Epoch [64/1000], Loss: 0.1072600781917572\n",
            "Epoch [66/1000], Loss: 0.09210790693759918\n",
            "Epoch [68/1000], Loss: 0.07318995893001556\n",
            "Epoch [70/1000], Loss: 0.050575073808431625\n",
            "Epoch [72/1000], Loss: 0.026864390820264816\n",
            "Epoch [74/1000], Loss: 0.010621557012200356\n",
            "Epoch [76/1000], Loss: 0.001317931804805994\n",
            "Epoch [78/1000], Loss: 1.551828427182045e-05\n",
            "Epoch [80/1000], Loss: 7.192210432549473e-06\n",
            "Epoch [82/1000], Loss: 6.366232264554128e-05\n",
            "Epoch [84/1000], Loss: 0.0005401945090852678\n",
            "Epoch [86/1000], Loss: 0.0014800905482843518\n",
            "Epoch [88/1000], Loss: 0.0023271075915545225\n",
            "Epoch [90/1000], Loss: 0.0026499934028834105\n",
            "Epoch [92/1000], Loss: 0.0024306755512952805\n",
            "Epoch [94/1000], Loss: 0.001879637478850782\n",
            "Epoch [96/1000], Loss: 0.0012637798208743334\n",
            "Epoch [98/1000], Loss: 0.0007657447713427246\n",
            "Epoch [100/1000], Loss: 0.0004398021555971354\n",
            "Epoch [102/1000], Loss: 0.00024967436911538243\n",
            "Epoch [104/1000], Loss: 0.00014251658285502344\n",
            "Epoch [106/1000], Loss: 8.236476423917338e-05\n",
            "Epoch [108/1000], Loss: 4.860035187448375e-05\n",
            "Epoch [110/1000], Loss: 2.9904444090789184e-05\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.35965317487716675\n",
            "Epoch [4/1000], Loss: 0.3510732650756836\n",
            "Epoch [6/1000], Loss: 0.3428741991519928\n",
            "Epoch [8/1000], Loss: 0.3350323438644409\n",
            "Epoch [10/1000], Loss: 0.327499657869339\n",
            "Epoch [12/1000], Loss: 0.320202112197876\n",
            "Epoch [14/1000], Loss: 0.31304389238357544\n",
            "Epoch [16/1000], Loss: 0.30592212080955505\n",
            "Epoch [18/1000], Loss: 0.2987464666366577\n",
            "Epoch [20/1000], Loss: 0.29145482182502747\n",
            "Epoch [22/1000], Loss: 0.2840177118778229\n",
            "Epoch [24/1000], Loss: 0.27643314003944397\n",
            "Epoch [26/1000], Loss: 0.26872169971466064\n",
            "Epoch [28/1000], Loss: 0.2609196603298187\n",
            "Epoch [30/1000], Loss: 0.2530747950077057\n",
            "Epoch [32/1000], Loss: 0.24523897469043732\n",
            "Epoch [34/1000], Loss: 0.2374550849199295\n",
            "Epoch [36/1000], Loss: 0.22974401712417603\n",
            "Epoch [38/1000], Loss: 0.22209882736206055\n",
            "Epoch [40/1000], Loss: 0.21449624001979828\n",
            "Epoch [42/1000], Loss: 0.206924706697464\n",
            "Epoch [44/1000], Loss: 0.19941163063049316\n",
            "Epoch [46/1000], Loss: 0.19201914966106415\n",
            "Epoch [48/1000], Loss: 0.18479393422603607\n",
            "Epoch [50/1000], Loss: 0.17769894003868103\n",
            "Epoch [52/1000], Loss: 0.17058366537094116\n",
            "Epoch [54/1000], Loss: 0.16321036219596863\n",
            "Epoch [56/1000], Loss: 0.15529055893421173\n",
            "Epoch [58/1000], Loss: 0.146501824259758\n",
            "Epoch [60/1000], Loss: 0.13650575280189514\n",
            "Epoch [62/1000], Loss: 0.1249224841594696\n",
            "Epoch [64/1000], Loss: 0.11120188981294632\n",
            "Epoch [66/1000], Loss: 0.09439095854759216\n",
            "Epoch [68/1000], Loss: 0.07370521873235703\n",
            "Epoch [70/1000], Loss: 0.04990164935588837\n",
            "Epoch [72/1000], Loss: 0.023621344938874245\n",
            "Epoch [74/1000], Loss: 0.005494673270732164\n",
            "Epoch [76/1000], Loss: 0.00030761255766265094\n",
            "Epoch [78/1000], Loss: 6.204861620062729e-06\n",
            "Epoch [80/1000], Loss: 8.521572453901172e-06\n",
            "Epoch [82/1000], Loss: 0.0001409976975992322\n",
            "Epoch [84/1000], Loss: 0.0007148082368075848\n",
            "Epoch [86/1000], Loss: 0.001492549548856914\n",
            "Epoch [88/1000], Loss: 0.002139554126188159\n",
            "Epoch [90/1000], Loss: 0.0024432376958429813\n",
            "Epoch [92/1000], Loss: 0.0023627937771379948\n",
            "Epoch [94/1000], Loss: 0.001999923260882497\n",
            "Epoch [96/1000], Loss: 0.0015396439703181386\n",
            "Epoch [98/1000], Loss: 0.0011248384835198522\n",
            "Epoch [100/1000], Loss: 0.0007948107086122036\n",
            "Epoch [102/1000], Loss: 0.0005441289395093918\n",
            "Epoch [104/1000], Loss: 0.00036142938188277185\n",
            "Epoch [106/1000], Loss: 0.00023538607638329268\n",
            "Epoch [108/1000], Loss: 0.00015167065430432558\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "Epoch [2/1000], Loss: 0.3624204099178314\n",
            "Epoch [4/1000], Loss: 0.3541916310787201\n",
            "Epoch [6/1000], Loss: 0.34628376364707947\n",
            "Epoch [8/1000], Loss: 0.33863767981529236\n",
            "Epoch [10/1000], Loss: 0.3311430811882019\n",
            "Epoch [12/1000], Loss: 0.32366302609443665\n",
            "Epoch [14/1000], Loss: 0.3160915672779083\n",
            "Epoch [16/1000], Loss: 0.30838891863822937\n",
            "Epoch [18/1000], Loss: 0.30056601762771606\n",
            "Epoch [20/1000], Loss: 0.29265183210372925\n",
            "Epoch [22/1000], Loss: 0.28466907143592834\n",
            "Epoch [24/1000], Loss: 0.27662500739097595\n",
            "Epoch [26/1000], Loss: 0.26851290464401245\n",
            "Epoch [28/1000], Loss: 0.26032117009162903\n",
            "Epoch [30/1000], Loss: 0.2520436644554138\n",
            "Epoch [32/1000], Loss: 0.24368619918823242\n",
            "Epoch [34/1000], Loss: 0.23526468873023987\n",
            "Epoch [36/1000], Loss: 0.22679591178894043\n",
            "Epoch [38/1000], Loss: 0.2182883620262146\n",
            "Epoch [40/1000], Loss: 0.20974187552928925\n",
            "Epoch [42/1000], Loss: 0.2011609524488449\n",
            "self.ca_weight.shape torch.Size([99, 2])\n",
            "patience exceeded, loading best model\n",
            "Average accuracy: 0.8727272727272727\n"
          ]
        }
      ],
      "source": [
        "# prompt: train and test my model on the Xs and ys in a 10 fold cross validation\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# from metric_learn import LMNN,NCA\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Split data into 10 folds\n",
        "k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n",
        "# Scale data, not enabled right now.\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(Xs)\n",
        "# Xs = scaler.transform(Xs)\n",
        "\n",
        "# Train and test model on each fold\n",
        "best_model = None\n",
        "PATH = 'best_classifier_model.h5'\n",
        "best_accuracy = None\n",
        "best_accuracies = []\n",
        "accuracies = []\n",
        "knn_accuracies = []\n",
        "\n",
        "nca_accuracies = []\n",
        "klmnn_accuracies = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for train_index, test_index in k_fold.split(Xs):\n",
        "  # Get training and testing data\n",
        "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "  y_train, y_test = ys[train_index], ys[test_index]\n",
        "\n",
        "  # https://contrib.scikit-learn.org/metric-learn/supervised.html#lmnn\n",
        "  # lmnn = LMNN(n_neighbors=5, learn_rate=1e-6)\n",
        "  # ##TODO, change here if you need to use a different one\n",
        "  # # lmnn = metric_learn.MLKR()\n",
        "  # # lmnn = metric_learn.NCA(max_iter=1000)\n",
        "  # lmnn.fit(X_train,y_train)\n",
        "  # knn = KNeighborsClassifier(n_neighbors=5,metric=lmnn.get_metric())\n",
        "  # knn.fit(X_train,y_train)\n",
        "  # klmnn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n",
        "\n",
        "  # continue\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Train model\n",
        "  model = NN_k_NN(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing,hidden_layers)\n",
        "  # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #, weight_decay=1e-5)\n",
        "  # print_model_features(model)\n",
        "  #Training loop\n",
        "  # temp = []\n",
        "  patience_counter = 0\n",
        "  for epoch in range(training_epochs):\n",
        "    epoch_msg = True\n",
        "    # model.eval()\n",
        "    # break\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "      model.train()\n",
        "      case_activations, output, predicted_class = model(X_train_batch)\n",
        "      loss = criterion(output, y_train_batch)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if epoch_msg and (epoch + 1) % 2 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{training_epochs}], Loss: {loss.item()}')\n",
        "        #inspecting the case activations\n",
        "        # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "        # print(top_case_indices)\n",
        "        epoch_msg = False\n",
        "      # print(\"evaluating\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      case_activations, output, predicted_class = model(X_test)\n",
        "      # inspecting the case activations\n",
        "      # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "      # print(top_case_indices)\n",
        "\n",
        "      # Calculate accuracy\n",
        "      accuracy_temp = accuracy_score(y_test, predicted_class)\n",
        "    if epoch == 0:\n",
        "      best_accuracy = accuracy_temp\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "    elif accuracy_temp > best_accuracy:\n",
        "      #memorize best model\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "      best_accuracy = accuracy_temp\n",
        "      patience_counter = 0\n",
        "    elif patience_counter > patience:\n",
        "      model = NN_k_NN(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing)\n",
        "      model.load_state_dict(torch.load(PATH))\n",
        "      model.eval()\n",
        "      print(\"patience exceeded, loading best model\")\n",
        "      break\n",
        "    else:\n",
        "      patience_counter += 1\n",
        "\n",
        "  best_accuracies.append(best_accuracy)\n",
        "  # print_model_features(model)\n",
        "  # Test model\n",
        "  with torch.no_grad():\n",
        "    case_activations, output, predicted_class = model(X_test)\n",
        "\n",
        "    #inspecting the case activations\n",
        "    top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "    # print(top_case_indices)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, predicted_class)\n",
        "    # Add accuracy to list\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "  ##compare with a normal k-nn\n",
        "  knn =  KNeighborsClassifier(n_neighbors=top_k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  knn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n",
        "\n",
        "  # count += 1\n",
        "  # if count == 2:\n",
        "  #   break\n",
        "\n",
        "# Print average accuracy\n",
        "print(\"Average accuracy:\", np.mean(accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(klmnn_accuracies))\n",
        "print(np.mean(knn_accuracies))"
      ],
      "metadata": {
        "id": "hp0VlYenhs7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f953d8-07fc-4636-ec4f-c906f190971d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "0.3181818181818182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMgwt-v5c49R"
      },
      "source": [
        "## Classification with train test loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJBxiGuThMsp"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AA83z3Oc8gb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2NnlWpVWqn6"
      },
      "source": [
        "##Standardize Regression Data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6818C09SWst6"
      },
      "outputs": [],
      "source": [
        "# prompt: center and scale to normalize my Xs and ys\n",
        "\n",
        "def standardize_tensor(input_tensor):\n",
        "    mean = input_tensor.mean()\n",
        "    std = input_tensor.std()\n",
        "    standardized_tensor = (input_tensor - mean) / std\n",
        "    return standardized_tensor\n",
        "\n",
        "Xs = standardize_tensor(Xs)\n",
        "ys = standardize_tensor(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_WVmie3JH9c"
      },
      "source": [
        "##Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPpxOXA2JJJe"
      },
      "outputs": [],
      "source": [
        "training_epochs = 1500\n",
        "learning_rate = 0.01 #0.0001 #0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "hidden_layers = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r8m3pd6NMrl"
      },
      "source": [
        "This code below will attempt to use TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LWnNYJn2APf"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "# !pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
        "# # imports pytorch\n",
        "# import torch\n",
        "\n",
        "# # imports the torch_xla package\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "\n",
        "# dev = xm.xla_device()\n",
        "# Xs = Xs.to(dev)\n",
        "# ys = ys.to(dev)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5PloUp6JL3_",
        "outputId": "dd2a1d0b-b7aa-415c-b238-605f0eb3fe44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/1500], Loss: 0.834648609161377\n",
            "Epoch [4/1500], Loss: 0.8302991986274719\n",
            "Epoch [6/1500], Loss: 0.864800214767456\n",
            "Epoch [8/1500], Loss: 0.7961227893829346\n",
            "Epoch [10/1500], Loss: 0.8002479672431946\n",
            "Epoch [12/1500], Loss: 0.6966904997825623\n",
            "Epoch [14/1500], Loss: 0.7492128014564514\n",
            "Epoch [16/1500], Loss: 0.6885046362876892\n",
            "Epoch [18/1500], Loss: 0.7227585315704346\n",
            "Epoch [20/1500], Loss: 0.6080526113510132\n",
            "Epoch [22/1500], Loss: 0.5747911334037781\n",
            "Epoch [24/1500], Loss: 0.5665337443351746\n",
            "Epoch [26/1500], Loss: 0.5434054136276245\n",
            "Epoch [28/1500], Loss: 0.6184493899345398\n",
            "Epoch [30/1500], Loss: 0.4727575182914734\n",
            "Epoch [32/1500], Loss: 0.5719715356826782\n",
            "Epoch [34/1500], Loss: 0.5125253200531006\n",
            "Epoch [36/1500], Loss: 0.5432581901550293\n",
            "Epoch [38/1500], Loss: 0.6167198419570923\n",
            "Epoch [40/1500], Loss: 0.4619654417037964\n",
            "Epoch [42/1500], Loss: 0.40632301568984985\n",
            "Epoch [44/1500], Loss: 0.44540515542030334\n",
            "Epoch [46/1500], Loss: 0.5037679076194763\n",
            "Epoch [48/1500], Loss: 0.45576226711273193\n",
            "Epoch [50/1500], Loss: 0.3738574981689453\n",
            "Epoch [52/1500], Loss: 0.3982585668563843\n",
            "Epoch [54/1500], Loss: 0.44378072023391724\n",
            "Epoch [56/1500], Loss: 0.39001724123954773\n",
            "Epoch [58/1500], Loss: 0.4590095579624176\n",
            "Epoch [60/1500], Loss: 0.40534600615501404\n",
            "Epoch [62/1500], Loss: 0.4153602123260498\n",
            "Epoch [64/1500], Loss: 0.3503572344779968\n",
            "Epoch [66/1500], Loss: 0.47613510489463806\n",
            "Epoch [68/1500], Loss: 0.5073779821395874\n",
            "Epoch [70/1500], Loss: 0.4138995409011841\n",
            "Epoch [72/1500], Loss: 0.4755305349826813\n",
            "Epoch [74/1500], Loss: 0.49072265625\n",
            "Epoch [76/1500], Loss: 0.43674877285957336\n",
            "Epoch [78/1500], Loss: 0.31746625900268555\n",
            "Epoch [80/1500], Loss: 0.37489211559295654\n",
            "Epoch [82/1500], Loss: 0.4159702658653259\n",
            "Epoch [84/1500], Loss: 0.49617403745651245\n",
            "Epoch [86/1500], Loss: 0.49705013632774353\n",
            "Epoch [88/1500], Loss: 0.40084898471832275\n",
            "Epoch [90/1500], Loss: 0.39223530888557434\n",
            "Epoch [92/1500], Loss: 0.36935752630233765\n",
            "Epoch [94/1500], Loss: 0.501367449760437\n",
            "Epoch [96/1500], Loss: 0.3685879409313202\n",
            "Epoch [98/1500], Loss: 0.414093554019928\n",
            "Epoch [100/1500], Loss: 0.4138731360435486\n",
            "Epoch [102/1500], Loss: 0.4438140392303467\n",
            "Epoch [104/1500], Loss: 0.3996642231941223\n",
            "Epoch [106/1500], Loss: 0.3723439872264862\n",
            "Epoch [108/1500], Loss: 0.43093928694725037\n",
            "Epoch [110/1500], Loss: 0.35021868348121643\n",
            "Epoch [112/1500], Loss: 0.3972553014755249\n",
            "Epoch [114/1500], Loss: 0.40058785676956177\n",
            "Epoch [116/1500], Loss: 0.38465118408203125\n",
            "Epoch [118/1500], Loss: 0.43460986018180847\n",
            "Epoch [120/1500], Loss: 0.3202022612094879\n",
            "Epoch [122/1500], Loss: 0.3680645823478699\n",
            "Epoch [124/1500], Loss: 0.44914746284484863\n",
            "Epoch [126/1500], Loss: 0.40687096118927\n",
            "Epoch [128/1500], Loss: 0.4364226460456848\n",
            "Epoch [130/1500], Loss: 0.4023895263671875\n",
            "Epoch [132/1500], Loss: 0.4130723476409912\n",
            "Epoch [134/1500], Loss: 0.4240002930164337\n",
            "Epoch [136/1500], Loss: 0.3573531210422516\n",
            "Epoch [138/1500], Loss: 0.4737664759159088\n",
            "Epoch [140/1500], Loss: 0.3661141097545624\n",
            "Epoch [142/1500], Loss: 0.3977605104446411\n",
            "Epoch [144/1500], Loss: 0.3732489347457886\n",
            "Epoch [146/1500], Loss: 0.3773355185985565\n",
            "Epoch [148/1500], Loss: 0.3640398681163788\n",
            "Epoch [150/1500], Loss: 0.3892645537853241\n",
            "Epoch [152/1500], Loss: 0.40229153633117676\n",
            "Epoch [154/1500], Loss: 0.3937358558177948\n",
            "Epoch [156/1500], Loss: 0.3555501699447632\n",
            "Epoch [158/1500], Loss: 0.40073540806770325\n",
            "Epoch [160/1500], Loss: 0.33662083745002747\n",
            "Epoch [162/1500], Loss: 0.37880656123161316\n",
            "Epoch [164/1500], Loss: 0.36501866579055786\n",
            "Epoch [166/1500], Loss: 0.26152503490448\n",
            "Epoch [168/1500], Loss: 0.3906222879886627\n",
            "Epoch [170/1500], Loss: 0.3718986511230469\n",
            "Epoch [172/1500], Loss: 0.3142915964126587\n",
            "Epoch [174/1500], Loss: 0.4351429343223572\n",
            "Epoch [176/1500], Loss: 0.41152313351631165\n",
            "Epoch [178/1500], Loss: 0.31897231936454773\n",
            "Epoch [180/1500], Loss: 0.36403727531433105\n",
            "Epoch [182/1500], Loss: 0.3412935733795166\n",
            "Epoch [184/1500], Loss: 0.3482292592525482\n",
            "Epoch [186/1500], Loss: 0.3030189275741577\n",
            "Epoch [188/1500], Loss: 0.3281327486038208\n",
            "Epoch [190/1500], Loss: 0.36096426844596863\n",
            "Epoch [192/1500], Loss: 0.32317841053009033\n",
            "Epoch [194/1500], Loss: 0.32852694392204285\n",
            "Epoch [196/1500], Loss: 0.3826565146446228\n",
            "Epoch [198/1500], Loss: 0.3956787586212158\n",
            "Epoch [200/1500], Loss: 0.3098399043083191\n",
            "Epoch [202/1500], Loss: 0.27358052134513855\n",
            "Epoch [204/1500], Loss: 0.2978631258010864\n",
            "Epoch [206/1500], Loss: 0.3561730682849884\n",
            "Epoch [208/1500], Loss: 0.36879295110702515\n",
            "Epoch [210/1500], Loss: 0.36172816157341003\n",
            "Epoch [212/1500], Loss: 0.378398060798645\n",
            "Epoch [214/1500], Loss: 0.2849128842353821\n",
            "Epoch [216/1500], Loss: 0.3351166844367981\n",
            "Epoch [218/1500], Loss: 0.32999950647354126\n",
            "Epoch [220/1500], Loss: 0.3118211627006531\n",
            "Epoch [222/1500], Loss: 0.3093593120574951\n",
            "Epoch [224/1500], Loss: 0.30317872762680054\n",
            "Epoch [226/1500], Loss: 0.2991775870323181\n",
            "Epoch [228/1500], Loss: 0.2998916506767273\n",
            "Epoch [230/1500], Loss: 0.2906856834888458\n",
            "Epoch [232/1500], Loss: 0.2770341634750366\n",
            "Epoch [234/1500], Loss: 0.3193730413913727\n",
            "Epoch [236/1500], Loss: 0.2577875852584839\n",
            "Epoch [238/1500], Loss: 0.3482505679130554\n",
            "Epoch [240/1500], Loss: 0.28762105107307434\n",
            "Epoch [242/1500], Loss: 0.27920427918434143\n",
            "Epoch [244/1500], Loss: 0.2643328905105591\n",
            "Epoch [246/1500], Loss: 0.2871484160423279\n",
            "Epoch [248/1500], Loss: 0.306001216173172\n",
            "Epoch [250/1500], Loss: 0.30334681272506714\n",
            "Epoch [252/1500], Loss: 0.26770299673080444\n",
            "Epoch [254/1500], Loss: 0.31565484404563904\n",
            "Epoch [256/1500], Loss: 0.347451776266098\n",
            "Epoch [258/1500], Loss: 0.28964507579803467\n",
            "Epoch [260/1500], Loss: 0.3024636209011078\n",
            "Epoch [262/1500], Loss: 0.25762081146240234\n",
            "Epoch [264/1500], Loss: 0.3206961750984192\n",
            "Epoch [266/1500], Loss: 0.25668421387672424\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.8923105597496033\n",
            "Epoch [4/1500], Loss: 0.9304499626159668\n",
            "Epoch [6/1500], Loss: 0.8789360523223877\n",
            "Epoch [8/1500], Loss: 0.8866197466850281\n",
            "Epoch [10/1500], Loss: 0.8506085276603699\n",
            "Epoch [12/1500], Loss: 0.8647573590278625\n",
            "Epoch [14/1500], Loss: 0.6504254937171936\n",
            "Epoch [16/1500], Loss: 0.8406209349632263\n",
            "Epoch [18/1500], Loss: 0.6628947257995605\n",
            "Epoch [20/1500], Loss: 0.6869085431098938\n",
            "Epoch [22/1500], Loss: 0.5879179239273071\n",
            "Epoch [24/1500], Loss: 0.6768310070037842\n",
            "Epoch [26/1500], Loss: 0.6445040702819824\n",
            "Epoch [28/1500], Loss: 0.5713337063789368\n",
            "Epoch [30/1500], Loss: 0.6227139830589294\n",
            "Epoch [32/1500], Loss: 0.561932384967804\n",
            "Epoch [34/1500], Loss: 0.5356624126434326\n",
            "Epoch [36/1500], Loss: 0.5702701807022095\n",
            "Epoch [38/1500], Loss: 0.524728000164032\n",
            "Epoch [40/1500], Loss: 0.4636212885379791\n",
            "Epoch [42/1500], Loss: 0.45861801505088806\n",
            "Epoch [44/1500], Loss: 0.48547881841659546\n",
            "Epoch [46/1500], Loss: 0.46467819809913635\n",
            "Epoch [48/1500], Loss: 0.4507942199707031\n",
            "Epoch [50/1500], Loss: 0.40622207522392273\n",
            "Epoch [52/1500], Loss: 0.5321489572525024\n",
            "Epoch [54/1500], Loss: 0.4633938670158386\n",
            "Epoch [56/1500], Loss: 0.4170355498790741\n",
            "Epoch [58/1500], Loss: 0.5028156042098999\n",
            "Epoch [60/1500], Loss: 0.4874137043952942\n",
            "Epoch [62/1500], Loss: 0.5001794099807739\n",
            "Epoch [64/1500], Loss: 0.4994960129261017\n",
            "Epoch [66/1500], Loss: 0.4196612536907196\n",
            "Epoch [68/1500], Loss: 0.41345417499542236\n",
            "Epoch [70/1500], Loss: 0.38567763566970825\n",
            "Epoch [72/1500], Loss: 0.5041983127593994\n",
            "Epoch [74/1500], Loss: 0.4785785973072052\n",
            "Epoch [76/1500], Loss: 0.42241203784942627\n",
            "Epoch [78/1500], Loss: 0.3563683331012726\n",
            "Epoch [80/1500], Loss: 0.42132991552352905\n",
            "Epoch [82/1500], Loss: 0.45736774802207947\n",
            "Epoch [84/1500], Loss: 0.40726298093795776\n",
            "Epoch [86/1500], Loss: 0.47149157524108887\n",
            "Epoch [88/1500], Loss: 0.35548800230026245\n",
            "Epoch [90/1500], Loss: 0.4605681300163269\n",
            "Epoch [92/1500], Loss: 0.4855043888092041\n",
            "Epoch [94/1500], Loss: 0.4676659405231476\n",
            "Epoch [96/1500], Loss: 0.4709031283855438\n",
            "Epoch [98/1500], Loss: 0.3994269073009491\n",
            "Epoch [100/1500], Loss: 0.3993068039417267\n",
            "Epoch [102/1500], Loss: 0.4853499233722687\n",
            "Epoch [104/1500], Loss: 0.4522355794906616\n",
            "Epoch [106/1500], Loss: 0.4341629147529602\n",
            "Epoch [108/1500], Loss: 0.48425015807151794\n",
            "Epoch [110/1500], Loss: 0.37342578172683716\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.7567352056503296\n",
            "Epoch [4/1500], Loss: 0.8248457312583923\n",
            "Epoch [6/1500], Loss: 0.7708290815353394\n",
            "Epoch [8/1500], Loss: 0.6487683653831482\n",
            "Epoch [10/1500], Loss: 0.645256519317627\n",
            "Epoch [12/1500], Loss: 0.7056124806404114\n",
            "Epoch [14/1500], Loss: 0.5912646651268005\n",
            "Epoch [16/1500], Loss: 0.7831165790557861\n",
            "Epoch [18/1500], Loss: 0.6166427731513977\n",
            "Epoch [20/1500], Loss: 0.5925258994102478\n",
            "Epoch [22/1500], Loss: 0.6169109344482422\n",
            "Epoch [24/1500], Loss: 0.6085561513900757\n",
            "Epoch [26/1500], Loss: 0.4878726303577423\n",
            "Epoch [28/1500], Loss: 0.5558038949966431\n",
            "Epoch [30/1500], Loss: 0.6612356305122375\n",
            "Epoch [32/1500], Loss: 0.5642160177230835\n",
            "Epoch [34/1500], Loss: 0.4565659761428833\n",
            "Epoch [36/1500], Loss: 0.5456330180168152\n",
            "Epoch [38/1500], Loss: 0.550751805305481\n",
            "Epoch [40/1500], Loss: 0.5244102478027344\n",
            "Epoch [42/1500], Loss: 0.5032691359519958\n",
            "Epoch [44/1500], Loss: 0.5429701805114746\n",
            "Epoch [46/1500], Loss: 0.4903196394443512\n",
            "Epoch [48/1500], Loss: 0.46191462874412537\n",
            "Epoch [50/1500], Loss: 0.47848832607269287\n",
            "Epoch [52/1500], Loss: 0.5437366962432861\n",
            "Epoch [54/1500], Loss: 0.5281016826629639\n",
            "Epoch [56/1500], Loss: 0.4551043212413788\n",
            "Epoch [58/1500], Loss: 0.4767281711101532\n",
            "Epoch [60/1500], Loss: 0.5207633376121521\n",
            "Epoch [62/1500], Loss: 0.4414539337158203\n",
            "Epoch [64/1500], Loss: 0.4258286654949188\n",
            "Epoch [66/1500], Loss: 0.400546669960022\n",
            "Epoch [68/1500], Loss: 0.3789502680301666\n",
            "Epoch [70/1500], Loss: 0.3972441852092743\n",
            "Epoch [72/1500], Loss: 0.3298313021659851\n",
            "Epoch [74/1500], Loss: 0.41370880603790283\n",
            "Epoch [76/1500], Loss: 0.47587311267852783\n",
            "Epoch [78/1500], Loss: 0.46938273310661316\n",
            "Epoch [80/1500], Loss: 0.42534491419792175\n",
            "Epoch [82/1500], Loss: 0.4481939971446991\n",
            "Epoch [84/1500], Loss: 0.4261679947376251\n",
            "Epoch [86/1500], Loss: 0.5017199516296387\n",
            "Epoch [88/1500], Loss: 0.386646032333374\n",
            "Epoch [90/1500], Loss: 0.43446388840675354\n",
            "Epoch [92/1500], Loss: 0.37321189045906067\n",
            "Epoch [94/1500], Loss: 0.44348782300949097\n",
            "Epoch [96/1500], Loss: 0.38140884041786194\n",
            "Epoch [98/1500], Loss: 0.37507855892181396\n",
            "Epoch [100/1500], Loss: 0.4329521059989929\n",
            "Epoch [102/1500], Loss: 0.44732630252838135\n",
            "Epoch [104/1500], Loss: 0.49544453620910645\n",
            "Epoch [106/1500], Loss: 0.4947513937950134\n",
            "Epoch [108/1500], Loss: 0.3962881565093994\n",
            "Epoch [110/1500], Loss: 0.41584959626197815\n",
            "Epoch [112/1500], Loss: 0.3957815170288086\n",
            "Epoch [114/1500], Loss: 0.4397915005683899\n",
            "Epoch [116/1500], Loss: 0.4146445095539093\n",
            "Epoch [118/1500], Loss: 0.4120447635650635\n",
            "Epoch [120/1500], Loss: 0.3625761866569519\n",
            "Epoch [122/1500], Loss: 0.3574495315551758\n",
            "Epoch [124/1500], Loss: 0.3667924404144287\n",
            "Epoch [126/1500], Loss: 0.3730129897594452\n",
            "Epoch [128/1500], Loss: 0.45123088359832764\n",
            "Epoch [130/1500], Loss: 0.3526836037635803\n",
            "Epoch [132/1500], Loss: 0.359588623046875\n",
            "Epoch [134/1500], Loss: 0.41279032826423645\n",
            "Epoch [136/1500], Loss: 0.39980101585388184\n",
            "Epoch [138/1500], Loss: 0.3695555031299591\n",
            "Epoch [140/1500], Loss: 0.40084972977638245\n",
            "Epoch [142/1500], Loss: 0.3525608479976654\n",
            "Epoch [144/1500], Loss: 0.4596245288848877\n",
            "Epoch [146/1500], Loss: 0.43006446957588196\n",
            "Epoch [148/1500], Loss: 0.3568381071090698\n",
            "Epoch [150/1500], Loss: 0.4211805462837219\n",
            "Epoch [152/1500], Loss: 0.32447579503059387\n",
            "Epoch [154/1500], Loss: 0.3616030216217041\n",
            "Epoch [156/1500], Loss: 0.3914724290370941\n",
            "Epoch [158/1500], Loss: 0.42052069306373596\n",
            "Epoch [160/1500], Loss: 0.3021010458469391\n",
            "Epoch [162/1500], Loss: 0.4059140086174011\n",
            "Epoch [164/1500], Loss: 0.33448830246925354\n",
            "Epoch [166/1500], Loss: 0.34537872672080994\n",
            "Epoch [168/1500], Loss: 0.4060896337032318\n",
            "Epoch [170/1500], Loss: 0.3506157696247101\n",
            "Epoch [172/1500], Loss: 0.41597896814346313\n",
            "Epoch [174/1500], Loss: 0.423168420791626\n",
            "Epoch [176/1500], Loss: 0.3756946623325348\n",
            "Epoch [178/1500], Loss: 0.34384146332740784\n",
            "Epoch [180/1500], Loss: 0.32735562324523926\n",
            "Epoch [182/1500], Loss: 0.43561434745788574\n",
            "Epoch [184/1500], Loss: 0.34715545177459717\n",
            "Epoch [186/1500], Loss: 0.3399280309677124\n",
            "Epoch [188/1500], Loss: 0.3809022009372711\n",
            "Epoch [190/1500], Loss: 0.25308722257614136\n",
            "Epoch [192/1500], Loss: 0.3217514455318451\n",
            "Epoch [194/1500], Loss: 0.33126822113990784\n",
            "Epoch [196/1500], Loss: 0.334105908870697\n",
            "Epoch [198/1500], Loss: 0.3356005549430847\n",
            "Epoch [200/1500], Loss: 0.3758105933666229\n",
            "Epoch [202/1500], Loss: 0.3367213010787964\n",
            "Epoch [204/1500], Loss: 0.37184852361679077\n",
            "Epoch [206/1500], Loss: 0.3619612157344818\n",
            "Epoch [208/1500], Loss: 0.3775329291820526\n",
            "Epoch [210/1500], Loss: 0.3037213385105133\n",
            "Epoch [212/1500], Loss: 0.2600906193256378\n",
            "Epoch [214/1500], Loss: 0.29603609442710876\n",
            "Epoch [216/1500], Loss: 0.3409196734428406\n",
            "Epoch [218/1500], Loss: 0.28406044840812683\n",
            "Epoch [220/1500], Loss: 0.35447466373443604\n",
            "Epoch [222/1500], Loss: 0.3485524654388428\n",
            "Epoch [224/1500], Loss: 0.364500492811203\n",
            "Epoch [226/1500], Loss: 0.3223121166229248\n",
            "Epoch [228/1500], Loss: 0.29405874013900757\n",
            "Epoch [230/1500], Loss: 0.3019047677516937\n",
            "Epoch [232/1500], Loss: 0.3128904104232788\n",
            "Epoch [234/1500], Loss: 0.36564406752586365\n",
            "Epoch [236/1500], Loss: 0.2678844928741455\n",
            "Epoch [238/1500], Loss: 0.3490836024284363\n",
            "Epoch [240/1500], Loss: 0.3106715679168701\n",
            "Epoch [242/1500], Loss: 0.2877328097820282\n",
            "Epoch [244/1500], Loss: 0.25515300035476685\n",
            "Epoch [246/1500], Loss: 0.3048928380012512\n",
            "Epoch [248/1500], Loss: 0.26376357674598694\n",
            "Epoch [250/1500], Loss: 0.3027419149875641\n",
            "Epoch [252/1500], Loss: 0.3576493561267853\n",
            "Epoch [254/1500], Loss: 0.3232327699661255\n",
            "Epoch [256/1500], Loss: 0.317954421043396\n",
            "Epoch [258/1500], Loss: 0.3363363742828369\n",
            "Epoch [260/1500], Loss: 0.26794373989105225\n",
            "Epoch [262/1500], Loss: 0.31282585859298706\n",
            "Epoch [264/1500], Loss: 0.2794528007507324\n",
            "Epoch [266/1500], Loss: 0.2528802752494812\n",
            "Epoch [268/1500], Loss: 0.23114517331123352\n",
            "Epoch [270/1500], Loss: 0.29730725288391113\n",
            "Epoch [272/1500], Loss: 0.30148380994796753\n",
            "Epoch [274/1500], Loss: 0.25534579157829285\n",
            "Epoch [276/1500], Loss: 0.32401344180107117\n",
            "Epoch [278/1500], Loss: 0.2986507713794708\n",
            "Epoch [280/1500], Loss: 0.3246804177761078\n",
            "Epoch [282/1500], Loss: 0.2731987237930298\n",
            "Epoch [284/1500], Loss: 0.31036093831062317\n",
            "Epoch [286/1500], Loss: 0.2897852659225464\n",
            "Epoch [288/1500], Loss: 0.265947163105011\n",
            "Epoch [290/1500], Loss: 0.3254838287830353\n",
            "Epoch [292/1500], Loss: 0.31110426783561707\n",
            "Epoch [294/1500], Loss: 0.3026920557022095\n",
            "Epoch [296/1500], Loss: 0.3117978274822235\n",
            "Epoch [298/1500], Loss: 0.2527397871017456\n",
            "Epoch [300/1500], Loss: 0.2600434124469757\n",
            "Epoch [302/1500], Loss: 0.28497371077537537\n",
            "Epoch [304/1500], Loss: 0.25310081243515015\n",
            "Epoch [306/1500], Loss: 0.31119799613952637\n",
            "Epoch [308/1500], Loss: 0.3059108555316925\n",
            "Epoch [310/1500], Loss: 0.2824722230434418\n",
            "Epoch [312/1500], Loss: 0.2591013014316559\n",
            "Epoch [314/1500], Loss: 0.23811574280261993\n",
            "Epoch [316/1500], Loss: 0.21666742861270905\n",
            "Epoch [318/1500], Loss: 0.25971776247024536\n",
            "Epoch [320/1500], Loss: 0.27288854122161865\n",
            "Epoch [322/1500], Loss: 0.25684794783592224\n",
            "Epoch [324/1500], Loss: 0.21656498312950134\n",
            "Epoch [326/1500], Loss: 0.2547876834869385\n",
            "Epoch [328/1500], Loss: 0.28005772829055786\n",
            "Epoch [330/1500], Loss: 0.21743375062942505\n",
            "Epoch [332/1500], Loss: 0.2565409541130066\n",
            "Epoch [334/1500], Loss: 0.279535174369812\n",
            "Epoch [336/1500], Loss: 0.2727285623550415\n",
            "Epoch [338/1500], Loss: 0.2445564717054367\n",
            "Epoch [340/1500], Loss: 0.25934305787086487\n",
            "Epoch [342/1500], Loss: 0.22408252954483032\n",
            "Epoch [344/1500], Loss: 0.2795509397983551\n",
            "Epoch [346/1500], Loss: 0.274408221244812\n",
            "Epoch [348/1500], Loss: 0.2542004883289337\n",
            "Epoch [350/1500], Loss: 0.20516036450862885\n",
            "Epoch [352/1500], Loss: 0.29235589504241943\n",
            "Epoch [354/1500], Loss: 0.2529313564300537\n",
            "Epoch [356/1500], Loss: 0.20718427002429962\n",
            "Epoch [358/1500], Loss: 0.19472017884254456\n",
            "Epoch [360/1500], Loss: 0.1995530128479004\n",
            "Epoch [362/1500], Loss: 0.22331543266773224\n",
            "Epoch [364/1500], Loss: 0.2284087985754013\n",
            "Epoch [366/1500], Loss: 0.23940898478031158\n",
            "Epoch [368/1500], Loss: 0.2683737277984619\n",
            "Epoch [370/1500], Loss: 0.25370460748672485\n",
            "Epoch [372/1500], Loss: 0.22225558757781982\n",
            "Epoch [374/1500], Loss: 0.2806486189365387\n",
            "Epoch [376/1500], Loss: 0.2467489242553711\n",
            "Epoch [378/1500], Loss: 0.22865256667137146\n",
            "Epoch [380/1500], Loss: 0.2506025433540344\n",
            "Epoch [382/1500], Loss: 0.2421455979347229\n",
            "Epoch [384/1500], Loss: 0.23513410985469818\n",
            "Epoch [386/1500], Loss: 0.22043758630752563\n",
            "Epoch [388/1500], Loss: 0.23946347832679749\n",
            "Epoch [390/1500], Loss: 0.22735019028186798\n",
            "Epoch [392/1500], Loss: 0.26077210903167725\n",
            "Epoch [394/1500], Loss: 0.1853855550289154\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.8672752976417542\n",
            "Epoch [4/1500], Loss: 0.984061598777771\n",
            "Epoch [6/1500], Loss: 0.7354661822319031\n",
            "Epoch [8/1500], Loss: 0.7909544110298157\n",
            "Epoch [10/1500], Loss: 0.7160739898681641\n",
            "Epoch [12/1500], Loss: 0.742534339427948\n",
            "Epoch [14/1500], Loss: 0.6457661390304565\n",
            "Epoch [16/1500], Loss: 0.7300025820732117\n",
            "Epoch [18/1500], Loss: 0.7378523349761963\n",
            "Epoch [20/1500], Loss: 0.6583957076072693\n",
            "Epoch [22/1500], Loss: 0.6259812116622925\n",
            "Epoch [24/1500], Loss: 0.519887387752533\n",
            "Epoch [26/1500], Loss: 0.5568313598632812\n",
            "Epoch [28/1500], Loss: 0.5357215404510498\n",
            "Epoch [30/1500], Loss: 0.6011377573013306\n",
            "Epoch [32/1500], Loss: 0.4858529567718506\n",
            "Epoch [34/1500], Loss: 0.5482227206230164\n",
            "Epoch [36/1500], Loss: 0.5761852264404297\n",
            "Epoch [38/1500], Loss: 0.5903233289718628\n",
            "Epoch [40/1500], Loss: 0.5007994174957275\n",
            "Epoch [42/1500], Loss: 0.48709771037101746\n",
            "Epoch [44/1500], Loss: 0.4540022313594818\n",
            "Epoch [46/1500], Loss: 0.4708605706691742\n",
            "Epoch [48/1500], Loss: 0.4526491165161133\n",
            "Epoch [50/1500], Loss: 0.5005739331245422\n",
            "Epoch [52/1500], Loss: 0.47486305236816406\n",
            "Epoch [54/1500], Loss: 0.5328736305236816\n",
            "Epoch [56/1500], Loss: 0.5063469409942627\n",
            "Epoch [58/1500], Loss: 0.5055737495422363\n",
            "Epoch [60/1500], Loss: 0.48955821990966797\n",
            "Epoch [62/1500], Loss: 0.5271466970443726\n",
            "Epoch [64/1500], Loss: 0.572735607624054\n",
            "Epoch [66/1500], Loss: 0.3932117819786072\n",
            "Epoch [68/1500], Loss: 0.3900080621242523\n",
            "Epoch [70/1500], Loss: 0.44946521520614624\n",
            "Epoch [72/1500], Loss: 0.5151214599609375\n",
            "Epoch [74/1500], Loss: 0.47102829813957214\n",
            "Epoch [76/1500], Loss: 0.42242035269737244\n",
            "Epoch [78/1500], Loss: 0.3633778691291809\n",
            "Epoch [80/1500], Loss: 0.5006898641586304\n",
            "Epoch [82/1500], Loss: 0.43665024638175964\n",
            "Epoch [84/1500], Loss: 0.46619153022766113\n",
            "Epoch [86/1500], Loss: 0.476318895816803\n",
            "Epoch [88/1500], Loss: 0.4078114330768585\n",
            "Epoch [90/1500], Loss: 0.4691022038459778\n",
            "Epoch [92/1500], Loss: 0.5003272891044617\n",
            "Epoch [94/1500], Loss: 0.4365570545196533\n",
            "Epoch [96/1500], Loss: 0.4437520205974579\n",
            "Epoch [98/1500], Loss: 0.42174938321113586\n",
            "Epoch [100/1500], Loss: 0.41596779227256775\n",
            "Epoch [102/1500], Loss: 0.4802893102169037\n",
            "Epoch [104/1500], Loss: 0.43480774760246277\n",
            "Epoch [106/1500], Loss: 0.4459492862224579\n",
            "Epoch [108/1500], Loss: 0.4418584108352661\n",
            "Epoch [110/1500], Loss: 0.4053923487663269\n",
            "Epoch [112/1500], Loss: 0.4177038073539734\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.8331587314605713\n",
            "Epoch [4/1500], Loss: 0.9432176351547241\n",
            "Epoch [6/1500], Loss: 0.8249716758728027\n",
            "Epoch [8/1500], Loss: 0.738695502281189\n",
            "Epoch [10/1500], Loss: 0.7773177623748779\n",
            "Epoch [12/1500], Loss: 0.6995020508766174\n",
            "Epoch [14/1500], Loss: 0.6747597455978394\n",
            "Epoch [16/1500], Loss: 0.7038680911064148\n",
            "Epoch [18/1500], Loss: 0.7599296569824219\n",
            "Epoch [20/1500], Loss: 0.5813825726509094\n",
            "Epoch [22/1500], Loss: 0.7170849442481995\n",
            "Epoch [24/1500], Loss: 0.5285370945930481\n",
            "Epoch [26/1500], Loss: 0.6191771030426025\n",
            "Epoch [28/1500], Loss: 0.6062284111976624\n",
            "Epoch [30/1500], Loss: 0.4975121319293976\n",
            "Epoch [32/1500], Loss: 0.5389804244041443\n",
            "Epoch [34/1500], Loss: 0.6109898090362549\n",
            "Epoch [36/1500], Loss: 0.5096858739852905\n",
            "Epoch [38/1500], Loss: 0.4835609495639801\n",
            "Epoch [40/1500], Loss: 0.5425851941108704\n",
            "Epoch [42/1500], Loss: 0.48213404417037964\n",
            "Epoch [44/1500], Loss: 0.4792501926422119\n",
            "Epoch [46/1500], Loss: 0.49035385251045227\n",
            "Epoch [48/1500], Loss: 0.5084724426269531\n",
            "Epoch [50/1500], Loss: 0.3726803660392761\n",
            "Epoch [52/1500], Loss: 0.4113500118255615\n",
            "Epoch [54/1500], Loss: 0.5187332630157471\n",
            "Epoch [56/1500], Loss: 0.4297485053539276\n",
            "Epoch [58/1500], Loss: 0.5098912119865417\n",
            "Epoch [60/1500], Loss: 0.3256547152996063\n",
            "Epoch [62/1500], Loss: 0.41570156812667847\n",
            "Epoch [64/1500], Loss: 0.4164409339427948\n",
            "Epoch [66/1500], Loss: 0.4077260494232178\n",
            "Epoch [68/1500], Loss: 0.4905529022216797\n",
            "Epoch [70/1500], Loss: 0.5401970744132996\n",
            "Epoch [72/1500], Loss: 0.450479120016098\n",
            "Epoch [74/1500], Loss: 0.41685864329338074\n",
            "Epoch [76/1500], Loss: 0.4600391089916229\n",
            "Epoch [78/1500], Loss: 0.4136470854282379\n",
            "Epoch [80/1500], Loss: 0.37753090262413025\n",
            "Epoch [82/1500], Loss: 0.3843812942504883\n",
            "Epoch [84/1500], Loss: 0.48630186915397644\n",
            "Epoch [86/1500], Loss: 0.4199509918689728\n",
            "Epoch [88/1500], Loss: 0.438740074634552\n",
            "Epoch [90/1500], Loss: 0.431984543800354\n",
            "Epoch [92/1500], Loss: 0.41462135314941406\n",
            "Epoch [94/1500], Loss: 0.42338478565216064\n",
            "Epoch [96/1500], Loss: 0.4504341781139374\n",
            "Epoch [98/1500], Loss: 0.35124000906944275\n",
            "Epoch [100/1500], Loss: 0.4173929989337921\n",
            "Epoch [102/1500], Loss: 0.527035117149353\n",
            "Epoch [104/1500], Loss: 0.37371811270713806\n",
            "Epoch [106/1500], Loss: 0.46341511607170105\n",
            "Epoch [108/1500], Loss: 0.35783377289772034\n",
            "Epoch [110/1500], Loss: 0.41629284620285034\n",
            "Epoch [112/1500], Loss: 0.4243263602256775\n",
            "Epoch [114/1500], Loss: 0.42790523171424866\n",
            "Epoch [116/1500], Loss: 0.4004177153110504\n",
            "Epoch [118/1500], Loss: 0.4618004858493805\n",
            "Epoch [120/1500], Loss: 0.41418522596359253\n",
            "Epoch [122/1500], Loss: 0.4476132094860077\n",
            "Epoch [124/1500], Loss: 0.43108490109443665\n",
            "Epoch [126/1500], Loss: 0.42563584446907043\n",
            "Epoch [128/1500], Loss: 0.48423466086387634\n",
            "Epoch [130/1500], Loss: 0.38101550936698914\n",
            "Epoch [132/1500], Loss: 0.31283608078956604\n",
            "Epoch [134/1500], Loss: 0.3557901084423065\n",
            "Epoch [136/1500], Loss: 0.4108960032463074\n",
            "Epoch [138/1500], Loss: 0.4374096691608429\n",
            "Epoch [140/1500], Loss: 0.333353728055954\n",
            "Epoch [142/1500], Loss: 0.42977240681648254\n",
            "Epoch [144/1500], Loss: 0.3996087610721588\n",
            "Epoch [146/1500], Loss: 0.37854915857315063\n",
            "Epoch [148/1500], Loss: 0.3364983797073364\n",
            "Epoch [150/1500], Loss: 0.35545557737350464\n",
            "Epoch [152/1500], Loss: 0.41327014565467834\n",
            "Epoch [154/1500], Loss: 0.43917474150657654\n",
            "Epoch [156/1500], Loss: 0.3558381199836731\n",
            "Epoch [158/1500], Loss: 0.3305816650390625\n",
            "Epoch [160/1500], Loss: 0.3493136167526245\n",
            "Epoch [162/1500], Loss: 0.3642726242542267\n",
            "Epoch [164/1500], Loss: 0.34858497977256775\n",
            "Epoch [166/1500], Loss: 0.3456883132457733\n",
            "Epoch [168/1500], Loss: 0.34227943420410156\n",
            "Epoch [170/1500], Loss: 0.3860577344894409\n",
            "Epoch [172/1500], Loss: 0.435006707906723\n",
            "Epoch [174/1500], Loss: 0.38111254572868347\n",
            "Epoch [176/1500], Loss: 0.36448153853416443\n",
            "Epoch [178/1500], Loss: 0.3924819529056549\n",
            "Epoch [180/1500], Loss: 0.2994839549064636\n",
            "Epoch [182/1500], Loss: 0.3749994933605194\n",
            "Epoch [184/1500], Loss: 0.39641979336738586\n",
            "Epoch [186/1500], Loss: 0.44074320793151855\n",
            "Epoch [188/1500], Loss: 0.38375845551490784\n",
            "Epoch [190/1500], Loss: 0.2938835024833679\n",
            "Epoch [192/1500], Loss: 0.328827828168869\n",
            "Epoch [194/1500], Loss: 0.34693917632102966\n",
            "Epoch [196/1500], Loss: 0.3392610251903534\n",
            "Epoch [198/1500], Loss: 0.43409934639930725\n",
            "Epoch [200/1500], Loss: 0.3196561932563782\n",
            "Epoch [202/1500], Loss: 0.3639594316482544\n",
            "Epoch [204/1500], Loss: 0.3773841857910156\n",
            "Epoch [206/1500], Loss: 0.417464941740036\n",
            "Epoch [208/1500], Loss: 0.3470771908760071\n",
            "Epoch [210/1500], Loss: 0.3551788330078125\n",
            "Epoch [212/1500], Loss: 0.31643784046173096\n",
            "Epoch [214/1500], Loss: 0.33733561635017395\n",
            "Epoch [216/1500], Loss: 0.3433645963668823\n",
            "Epoch [218/1500], Loss: 0.33016204833984375\n",
            "Epoch [220/1500], Loss: 0.4083482623100281\n",
            "Epoch [222/1500], Loss: 0.3321778476238251\n",
            "Epoch [224/1500], Loss: 0.3813486099243164\n",
            "Epoch [226/1500], Loss: 0.306270569562912\n",
            "Epoch [228/1500], Loss: 0.3459816873073578\n",
            "Epoch [230/1500], Loss: 0.3165853023529053\n",
            "Epoch [232/1500], Loss: 0.3160860240459442\n",
            "Epoch [234/1500], Loss: 0.34004923701286316\n",
            "Epoch [236/1500], Loss: 0.28987428545951843\n",
            "Epoch [238/1500], Loss: 0.3206154704093933\n",
            "Epoch [240/1500], Loss: 0.29965779185295105\n",
            "Epoch [242/1500], Loss: 0.33714282512664795\n",
            "Epoch [244/1500], Loss: 0.28384751081466675\n",
            "Epoch [246/1500], Loss: 0.2711198031902313\n",
            "Epoch [248/1500], Loss: 0.25974246859550476\n",
            "Epoch [250/1500], Loss: 0.2725747525691986\n",
            "Epoch [252/1500], Loss: 0.3405079245567322\n",
            "Epoch [254/1500], Loss: 0.30911996960639954\n",
            "Epoch [256/1500], Loss: 0.355233758687973\n",
            "Epoch [258/1500], Loss: 0.3429408371448517\n",
            "Epoch [260/1500], Loss: 0.30566197633743286\n",
            "Epoch [262/1500], Loss: 0.3638671040534973\n",
            "Epoch [264/1500], Loss: 0.3236226737499237\n",
            "Epoch [266/1500], Loss: 0.3634081184864044\n",
            "Epoch [268/1500], Loss: 0.3207019865512848\n",
            "Epoch [270/1500], Loss: 0.24070222675800323\n",
            "Epoch [272/1500], Loss: 0.28115522861480713\n",
            "Epoch [274/1500], Loss: 0.29266735911369324\n",
            "Epoch [276/1500], Loss: 0.334076464176178\n",
            "Epoch [278/1500], Loss: 0.29629626870155334\n",
            "Epoch [280/1500], Loss: 0.3055998682975769\n",
            "Epoch [282/1500], Loss: 0.27913719415664673\n",
            "Epoch [284/1500], Loss: 0.27957576513290405\n",
            "Epoch [286/1500], Loss: 0.29395779967308044\n",
            "Epoch [288/1500], Loss: 0.25922125577926636\n",
            "Epoch [290/1500], Loss: 0.24185670912265778\n",
            "Epoch [292/1500], Loss: 0.31466421484947205\n",
            "Epoch [294/1500], Loss: 0.2809050977230072\n",
            "Epoch [296/1500], Loss: 0.23903021216392517\n",
            "Epoch [298/1500], Loss: 0.29024672508239746\n",
            "Epoch [300/1500], Loss: 0.2218305617570877\n",
            "Epoch [302/1500], Loss: 0.2714748978614807\n",
            "Epoch [304/1500], Loss: 0.27536770701408386\n",
            "Epoch [306/1500], Loss: 0.23871776461601257\n",
            "Epoch [308/1500], Loss: 0.3091270923614502\n",
            "Epoch [310/1500], Loss: 0.3159707486629486\n",
            "Epoch [312/1500], Loss: 0.292544960975647\n",
            "Epoch [314/1500], Loss: 0.2933899462223053\n",
            "Epoch [316/1500], Loss: 0.2450680136680603\n",
            "Epoch [318/1500], Loss: 0.239377960562706\n",
            "Epoch [320/1500], Loss: 0.26931285858154297\n",
            "Epoch [322/1500], Loss: 0.20100663602352142\n",
            "Epoch [324/1500], Loss: 0.27322617173194885\n",
            "Epoch [326/1500], Loss: 0.2336503267288208\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.7749488353729248\n",
            "Epoch [4/1500], Loss: 1.0173794031143188\n",
            "Epoch [6/1500], Loss: 0.7872134447097778\n",
            "Epoch [8/1500], Loss: 0.9045151472091675\n",
            "Epoch [10/1500], Loss: 0.8249175548553467\n",
            "Epoch [12/1500], Loss: 0.9043949246406555\n",
            "Epoch [14/1500], Loss: 0.7331457734107971\n",
            "Epoch [16/1500], Loss: 0.6544088125228882\n",
            "Epoch [18/1500], Loss: 0.6434977054595947\n",
            "Epoch [20/1500], Loss: 0.6929019093513489\n",
            "Epoch [22/1500], Loss: 0.5947331786155701\n",
            "Epoch [24/1500], Loss: 0.5712921619415283\n",
            "Epoch [26/1500], Loss: 0.6066871881484985\n",
            "Epoch [28/1500], Loss: 0.4921146333217621\n",
            "Epoch [30/1500], Loss: 0.5241754651069641\n",
            "Epoch [32/1500], Loss: 0.4560507535934448\n",
            "Epoch [34/1500], Loss: 0.46547672152519226\n",
            "Epoch [36/1500], Loss: 0.5062578320503235\n",
            "Epoch [38/1500], Loss: 0.4781239330768585\n",
            "Epoch [40/1500], Loss: 0.48692718148231506\n",
            "Epoch [42/1500], Loss: 0.4756301939487457\n",
            "Epoch [44/1500], Loss: 0.45382529497146606\n",
            "Epoch [46/1500], Loss: 0.5055674910545349\n",
            "Epoch [48/1500], Loss: 0.45360347628593445\n",
            "Epoch [50/1500], Loss: 0.4195011556148529\n",
            "Epoch [52/1500], Loss: 0.4255647659301758\n",
            "Epoch [54/1500], Loss: 0.5343244671821594\n",
            "Epoch [56/1500], Loss: 0.4626709222793579\n",
            "Epoch [58/1500], Loss: 0.50405353307724\n",
            "Epoch [60/1500], Loss: 0.5101038217544556\n",
            "Epoch [62/1500], Loss: 0.5264627933502197\n",
            "Epoch [64/1500], Loss: 0.4892139732837677\n",
            "Epoch [66/1500], Loss: 0.5467402935028076\n",
            "Epoch [68/1500], Loss: 0.424728125333786\n",
            "Epoch [70/1500], Loss: 0.4785992503166199\n",
            "Epoch [72/1500], Loss: 0.4603642225265503\n",
            "Epoch [74/1500], Loss: 0.42391303181648254\n",
            "Epoch [76/1500], Loss: 0.49911367893218994\n",
            "Epoch [78/1500], Loss: 0.3737410008907318\n",
            "Epoch [80/1500], Loss: 0.3793475329875946\n",
            "Epoch [82/1500], Loss: 0.4558206796646118\n",
            "Epoch [84/1500], Loss: 0.4036072790622711\n",
            "Epoch [86/1500], Loss: 0.38864609599113464\n",
            "Epoch [88/1500], Loss: 0.4532971680164337\n",
            "Epoch [90/1500], Loss: 0.41530558466911316\n",
            "Epoch [92/1500], Loss: 0.39477741718292236\n",
            "Epoch [94/1500], Loss: 0.3246609568595886\n",
            "Epoch [96/1500], Loss: 0.42927560210227966\n",
            "Epoch [98/1500], Loss: 0.2928914725780487\n",
            "Epoch [100/1500], Loss: 0.3800565302371979\n",
            "Epoch [102/1500], Loss: 0.4300709664821625\n",
            "Epoch [104/1500], Loss: 0.45117369294166565\n",
            "Epoch [106/1500], Loss: 0.4774206876754761\n",
            "Epoch [108/1500], Loss: 0.46843278408050537\n",
            "Epoch [110/1500], Loss: 0.3809508979320526\n",
            "Epoch [112/1500], Loss: 0.38990214467048645\n",
            "Epoch [114/1500], Loss: 0.41995376348495483\n",
            "Epoch [116/1500], Loss: 0.3734007179737091\n",
            "Epoch [118/1500], Loss: 0.41149961948394775\n",
            "Epoch [120/1500], Loss: 0.3800240457057953\n",
            "Epoch [122/1500], Loss: 0.38804885745048523\n",
            "Epoch [124/1500], Loss: 0.3535837233066559\n",
            "Epoch [126/1500], Loss: 0.40373721718788147\n",
            "Epoch [128/1500], Loss: 0.40850451588630676\n",
            "Epoch [130/1500], Loss: 0.3662853240966797\n",
            "Epoch [132/1500], Loss: 0.4376473128795624\n",
            "Epoch [134/1500], Loss: 0.3915521204471588\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.7953036427497864\n",
            "Epoch [4/1500], Loss: 0.7708994150161743\n",
            "Epoch [6/1500], Loss: 0.7575247287750244\n",
            "Epoch [8/1500], Loss: 0.84319007396698\n",
            "Epoch [10/1500], Loss: 0.789152204990387\n",
            "Epoch [12/1500], Loss: 0.5951281189918518\n",
            "Epoch [14/1500], Loss: 0.7575764656066895\n",
            "Epoch [16/1500], Loss: 0.5872007608413696\n",
            "Epoch [18/1500], Loss: 0.7461512684822083\n",
            "Epoch [20/1500], Loss: 0.6393420100212097\n",
            "Epoch [22/1500], Loss: 0.6632903218269348\n",
            "Epoch [24/1500], Loss: 0.5652706027030945\n",
            "Epoch [26/1500], Loss: 0.6499691009521484\n",
            "Epoch [28/1500], Loss: 0.5423426628112793\n",
            "Epoch [30/1500], Loss: 0.5700605511665344\n",
            "Epoch [32/1500], Loss: 0.5428740978240967\n",
            "Epoch [34/1500], Loss: 0.5886366963386536\n",
            "Epoch [36/1500], Loss: 0.48626649379730225\n",
            "Epoch [38/1500], Loss: 0.5835328102111816\n",
            "Epoch [40/1500], Loss: 0.4315677583217621\n",
            "Epoch [42/1500], Loss: 0.41502970457077026\n",
            "Epoch [44/1500], Loss: 0.4243343472480774\n",
            "Epoch [46/1500], Loss: 0.44778263568878174\n",
            "Epoch [48/1500], Loss: 0.4505772888660431\n",
            "Epoch [50/1500], Loss: 0.4546026885509491\n",
            "Epoch [52/1500], Loss: 0.4753028154373169\n",
            "Epoch [54/1500], Loss: 0.5073185563087463\n",
            "Epoch [56/1500], Loss: 0.45875412225723267\n",
            "Epoch [58/1500], Loss: 0.513476550579071\n",
            "Epoch [60/1500], Loss: 0.40044066309928894\n",
            "Epoch [62/1500], Loss: 0.4345703423023224\n",
            "Epoch [64/1500], Loss: 0.500596284866333\n",
            "Epoch [66/1500], Loss: 0.4612077474594116\n",
            "Epoch [68/1500], Loss: 0.5202685594558716\n",
            "Epoch [70/1500], Loss: 0.48167097568511963\n",
            "Epoch [72/1500], Loss: 0.3979617953300476\n",
            "Epoch [74/1500], Loss: 0.38087379932403564\n",
            "Epoch [76/1500], Loss: 0.454604834318161\n",
            "Epoch [78/1500], Loss: 0.4642258584499359\n",
            "Epoch [80/1500], Loss: 0.4282241463661194\n",
            "Epoch [82/1500], Loss: 0.37968528270721436\n",
            "Epoch [84/1500], Loss: 0.3985520303249359\n",
            "Epoch [86/1500], Loss: 0.4885219633579254\n",
            "Epoch [88/1500], Loss: 0.4123763144016266\n",
            "Epoch [90/1500], Loss: 0.48398399353027344\n",
            "Epoch [92/1500], Loss: 0.44690510630607605\n",
            "Epoch [94/1500], Loss: 0.4370892345905304\n",
            "Epoch [96/1500], Loss: 0.4708443582057953\n",
            "Epoch [98/1500], Loss: 0.3987836539745331\n",
            "Epoch [100/1500], Loss: 0.41163963079452515\n",
            "Epoch [102/1500], Loss: 0.45750245451927185\n",
            "Epoch [104/1500], Loss: 0.47319722175598145\n",
            "Epoch [106/1500], Loss: 0.3888914883136749\n",
            "Epoch [108/1500], Loss: 0.39742743968963623\n",
            "Epoch [110/1500], Loss: 0.4342769980430603\n",
            "Epoch [112/1500], Loss: 0.4151010811328888\n",
            "Epoch [114/1500], Loss: 0.3655439019203186\n",
            "Epoch [116/1500], Loss: 0.3557811975479126\n",
            "Epoch [118/1500], Loss: 0.369271844625473\n",
            "Epoch [120/1500], Loss: 0.32551345229148865\n",
            "Epoch [122/1500], Loss: 0.38387900590896606\n",
            "Epoch [124/1500], Loss: 0.3795045018196106\n",
            "Epoch [126/1500], Loss: 0.3291473090648651\n",
            "Epoch [128/1500], Loss: 0.41893136501312256\n",
            "Epoch [130/1500], Loss: 0.39281439781188965\n",
            "Epoch [132/1500], Loss: 0.4236254096031189\n",
            "Epoch [134/1500], Loss: 0.414386510848999\n",
            "Epoch [136/1500], Loss: 0.4158748686313629\n",
            "Epoch [138/1500], Loss: 0.2980634570121765\n",
            "Epoch [140/1500], Loss: 0.3960181474685669\n",
            "Epoch [142/1500], Loss: 0.43590298295021057\n",
            "Epoch [144/1500], Loss: 0.34209853410720825\n",
            "Epoch [146/1500], Loss: 0.40663015842437744\n",
            "Epoch [148/1500], Loss: 0.42851555347442627\n",
            "Epoch [150/1500], Loss: 0.3608931601047516\n",
            "Epoch [152/1500], Loss: 0.38646742701530457\n",
            "Epoch [154/1500], Loss: 0.4011085629463196\n",
            "Epoch [156/1500], Loss: 0.40726593136787415\n",
            "Epoch [158/1500], Loss: 0.35438063740730286\n",
            "Epoch [160/1500], Loss: 0.34892380237579346\n",
            "Epoch [162/1500], Loss: 0.4189085066318512\n",
            "Epoch [164/1500], Loss: 0.3820638358592987\n",
            "Epoch [166/1500], Loss: 0.3275342881679535\n",
            "Epoch [168/1500], Loss: 0.3510911464691162\n",
            "Epoch [170/1500], Loss: 0.3865549564361572\n",
            "Epoch [172/1500], Loss: 0.33824148774147034\n",
            "Epoch [174/1500], Loss: 0.35287511348724365\n",
            "Epoch [176/1500], Loss: 0.4213463068008423\n",
            "Epoch [178/1500], Loss: 0.3997175991535187\n",
            "Epoch [180/1500], Loss: 0.33260512351989746\n",
            "Epoch [182/1500], Loss: 0.3243996798992157\n",
            "Epoch [184/1500], Loss: 0.451790452003479\n",
            "Epoch [186/1500], Loss: 0.42529892921447754\n",
            "Epoch [188/1500], Loss: 0.4257274568080902\n",
            "Epoch [190/1500], Loss: 0.3477064073085785\n",
            "Epoch [192/1500], Loss: 0.3491397202014923\n",
            "Epoch [194/1500], Loss: 0.3430529832839966\n",
            "Epoch [196/1500], Loss: 0.35431504249572754\n",
            "Epoch [198/1500], Loss: 0.3392415940761566\n",
            "Epoch [200/1500], Loss: 0.3209322392940521\n",
            "Epoch [202/1500], Loss: 0.37813690304756165\n",
            "Epoch [204/1500], Loss: 0.3082895278930664\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.6857550740242004\n",
            "Epoch [4/1500], Loss: 0.8611791729927063\n",
            "Epoch [6/1500], Loss: 0.8113773465156555\n",
            "Epoch [8/1500], Loss: 0.8795992136001587\n",
            "Epoch [10/1500], Loss: 0.6814118027687073\n",
            "Epoch [12/1500], Loss: 0.7452302575111389\n",
            "Epoch [14/1500], Loss: 0.6490627527236938\n",
            "Epoch [16/1500], Loss: 0.7206605672836304\n",
            "Epoch [18/1500], Loss: 0.7013305425643921\n",
            "Epoch [20/1500], Loss: 0.5187719464302063\n",
            "Epoch [22/1500], Loss: 0.6076382994651794\n",
            "Epoch [24/1500], Loss: 0.6700882911682129\n",
            "Epoch [26/1500], Loss: 0.6001793742179871\n",
            "Epoch [28/1500], Loss: 0.5603737831115723\n",
            "Epoch [30/1500], Loss: 0.5924317836761475\n",
            "Epoch [32/1500], Loss: 0.6097258925437927\n",
            "Epoch [34/1500], Loss: 0.46562156081199646\n",
            "Epoch [36/1500], Loss: 0.3838573396205902\n",
            "Epoch [38/1500], Loss: 0.4086426794528961\n",
            "Epoch [40/1500], Loss: 0.4939767122268677\n",
            "Epoch [42/1500], Loss: 0.4947768449783325\n",
            "Epoch [44/1500], Loss: 0.4933798611164093\n",
            "Epoch [46/1500], Loss: 0.5293880701065063\n",
            "Epoch [48/1500], Loss: 0.5385587215423584\n",
            "Epoch [50/1500], Loss: 0.38243281841278076\n",
            "Epoch [52/1500], Loss: 0.522213876247406\n",
            "Epoch [54/1500], Loss: 0.44562095403671265\n",
            "Epoch [56/1500], Loss: 0.43928730487823486\n",
            "Epoch [58/1500], Loss: 0.46962761878967285\n",
            "Epoch [60/1500], Loss: 0.41407543420791626\n",
            "Epoch [62/1500], Loss: 0.415012001991272\n",
            "Epoch [64/1500], Loss: 0.4652082920074463\n",
            "Epoch [66/1500], Loss: 0.4725101590156555\n",
            "Epoch [68/1500], Loss: 0.4342389404773712\n",
            "Epoch [70/1500], Loss: 0.4520634114742279\n",
            "Epoch [72/1500], Loss: 0.43907687067985535\n",
            "Epoch [74/1500], Loss: 0.4734329283237457\n",
            "Epoch [76/1500], Loss: 0.49954262375831604\n",
            "Epoch [78/1500], Loss: 0.480629563331604\n",
            "Epoch [80/1500], Loss: 0.42811548709869385\n",
            "Epoch [82/1500], Loss: 0.48892486095428467\n",
            "Epoch [84/1500], Loss: 0.4307084381580353\n",
            "Epoch [86/1500], Loss: 0.4146205484867096\n",
            "Epoch [88/1500], Loss: 0.4668112099170685\n",
            "Epoch [90/1500], Loss: 0.4174804985523224\n",
            "Epoch [92/1500], Loss: 0.4757867753505707\n",
            "Epoch [94/1500], Loss: 0.402082622051239\n",
            "Epoch [96/1500], Loss: 0.36345192790031433\n",
            "Epoch [98/1500], Loss: 0.445047527551651\n",
            "Epoch [100/1500], Loss: 0.44041574001312256\n",
            "Epoch [102/1500], Loss: 0.4204237759113312\n",
            "Epoch [104/1500], Loss: 0.438346266746521\n",
            "Epoch [106/1500], Loss: 0.42214417457580566\n",
            "Epoch [108/1500], Loss: 0.36640530824661255\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.8333954811096191\n",
            "Epoch [4/1500], Loss: 0.8313766717910767\n",
            "Epoch [6/1500], Loss: 0.7235366702079773\n",
            "Epoch [8/1500], Loss: 0.7802886366844177\n",
            "Epoch [10/1500], Loss: 0.7706085443496704\n",
            "Epoch [12/1500], Loss: 0.7813953161239624\n",
            "Epoch [14/1500], Loss: 0.85565185546875\n",
            "Epoch [16/1500], Loss: 0.6947214007377625\n",
            "Epoch [18/1500], Loss: 0.5893188714981079\n",
            "Epoch [20/1500], Loss: 0.7296560406684875\n",
            "Epoch [22/1500], Loss: 0.5724127292633057\n",
            "Epoch [24/1500], Loss: 0.621436595916748\n",
            "Epoch [26/1500], Loss: 0.5311127305030823\n",
            "Epoch [28/1500], Loss: 0.595835268497467\n",
            "Epoch [30/1500], Loss: 0.5111121535301208\n",
            "Epoch [32/1500], Loss: 0.4777880012989044\n",
            "Epoch [34/1500], Loss: 0.5028173327445984\n",
            "Epoch [36/1500], Loss: 0.5557549595832825\n",
            "Epoch [38/1500], Loss: 0.4509546756744385\n",
            "Epoch [40/1500], Loss: 0.5346739888191223\n",
            "Epoch [42/1500], Loss: 0.47477009892463684\n",
            "Epoch [44/1500], Loss: 0.43612340092658997\n",
            "Epoch [46/1500], Loss: 0.41455569863319397\n",
            "Epoch [48/1500], Loss: 0.47284477949142456\n",
            "Epoch [50/1500], Loss: 0.4260999262332916\n",
            "Epoch [52/1500], Loss: 0.501147449016571\n",
            "Epoch [54/1500], Loss: 0.5076960921287537\n",
            "Epoch [56/1500], Loss: 0.5428265929222107\n",
            "Epoch [58/1500], Loss: 0.5117652416229248\n",
            "Epoch [60/1500], Loss: 0.526960015296936\n",
            "Epoch [62/1500], Loss: 0.4604838192462921\n",
            "Epoch [64/1500], Loss: 0.463629812002182\n",
            "Epoch [66/1500], Loss: 0.47479647397994995\n",
            "Epoch [68/1500], Loss: 0.4600723683834076\n",
            "Epoch [70/1500], Loss: 0.40335312485694885\n",
            "Epoch [72/1500], Loss: 0.48255911469459534\n",
            "Epoch [74/1500], Loss: 0.5056395530700684\n",
            "Epoch [76/1500], Loss: 0.46721401810646057\n",
            "Epoch [78/1500], Loss: 0.5152000188827515\n",
            "Epoch [80/1500], Loss: 0.4117370843887329\n",
            "Epoch [82/1500], Loss: 0.46859070658683777\n",
            "Epoch [84/1500], Loss: 0.4436166286468506\n",
            "Epoch [86/1500], Loss: 0.4606326222419739\n",
            "Epoch [88/1500], Loss: 0.518598198890686\n",
            "Epoch [90/1500], Loss: 0.4027349054813385\n",
            "Epoch [92/1500], Loss: 0.4666508436203003\n",
            "Epoch [94/1500], Loss: 0.38214775919914246\n",
            "Epoch [96/1500], Loss: 0.38504794239997864\n",
            "Epoch [98/1500], Loss: 0.32462021708488464\n",
            "Epoch [100/1500], Loss: 0.44966721534729004\n",
            "Epoch [102/1500], Loss: 0.3904150724411011\n",
            "Epoch [104/1500], Loss: 0.37166813015937805\n",
            "Epoch [106/1500], Loss: 0.4266643524169922\n",
            "Epoch [108/1500], Loss: 0.4266012907028198\n",
            "Epoch [110/1500], Loss: 0.43089544773101807\n",
            "Epoch [112/1500], Loss: 0.3838997185230255\n",
            "Epoch [114/1500], Loss: 0.3604731261730194\n",
            "Epoch [116/1500], Loss: 0.4594474732875824\n",
            "Epoch [118/1500], Loss: 0.4130697250366211\n",
            "Epoch [120/1500], Loss: 0.46281859278678894\n",
            "Epoch [122/1500], Loss: 0.34005776047706604\n",
            "Epoch [124/1500], Loss: 0.4212082326412201\n",
            "Epoch [126/1500], Loss: 0.38720497488975525\n",
            "Epoch [128/1500], Loss: 0.3832949697971344\n",
            "Epoch [130/1500], Loss: 0.3404242694377899\n",
            "Epoch [132/1500], Loss: 0.4722788333892822\n",
            "Epoch [134/1500], Loss: 0.3742489516735077\n",
            "Epoch [136/1500], Loss: 0.36346349120140076\n",
            "Epoch [138/1500], Loss: 0.3754643201828003\n",
            "Epoch [140/1500], Loss: 0.38361793756484985\n",
            "Epoch [142/1500], Loss: 0.3827180862426758\n",
            "Epoch [144/1500], Loss: 0.4575950503349304\n",
            "Epoch [146/1500], Loss: 0.3340575695037842\n",
            "Epoch [148/1500], Loss: 0.36299461126327515\n",
            "Epoch [150/1500], Loss: 0.3965859115123749\n",
            "Epoch [152/1500], Loss: 0.39422187209129333\n",
            "Epoch [154/1500], Loss: 0.3424835205078125\n",
            "Epoch [156/1500], Loss: 0.40364310145378113\n",
            "Epoch [158/1500], Loss: 0.36740702390670776\n",
            "Epoch [160/1500], Loss: 0.35813644528388977\n",
            "Epoch [162/1500], Loss: 0.34829315543174744\n",
            "Epoch [164/1500], Loss: 0.3749159574508667\n",
            "Epoch [166/1500], Loss: 0.36432862281799316\n",
            "Epoch [168/1500], Loss: 0.32570311427116394\n",
            "Epoch [170/1500], Loss: 0.3891196846961975\n",
            "Epoch [172/1500], Loss: 0.35531798005104065\n",
            "Epoch [174/1500], Loss: 0.4193011224269867\n",
            "Epoch [176/1500], Loss: 0.35076582431793213\n",
            "Epoch [178/1500], Loss: 0.41162750124931335\n",
            "Epoch [180/1500], Loss: 0.3484415113925934\n",
            "Epoch [182/1500], Loss: 0.32334819436073303\n",
            "Epoch [184/1500], Loss: 0.3822731077671051\n",
            "Epoch [186/1500], Loss: 0.2848856747150421\n",
            "Epoch [188/1500], Loss: 0.313583105802536\n",
            "Epoch [190/1500], Loss: 0.3449415862560272\n",
            "Epoch [192/1500], Loss: 0.2712953984737396\n",
            "Epoch [194/1500], Loss: 0.33803409337997437\n",
            "Epoch [196/1500], Loss: 0.30083850026130676\n",
            "Epoch [198/1500], Loss: 0.3917096257209778\n",
            "Epoch [200/1500], Loss: 0.3207855224609375\n",
            "Epoch [202/1500], Loss: 0.2784532308578491\n",
            "Epoch [204/1500], Loss: 0.31425151228904724\n",
            "Epoch [206/1500], Loss: 0.33477821946144104\n",
            "Epoch [208/1500], Loss: 0.33833274245262146\n",
            "Epoch [210/1500], Loss: 0.36282458901405334\n",
            "Epoch [212/1500], Loss: 0.37091174721717834\n",
            "Epoch [214/1500], Loss: 0.3097135126590729\n",
            "Epoch [216/1500], Loss: 0.30483630299568176\n",
            "Epoch [218/1500], Loss: 0.3422848582267761\n",
            "Epoch [220/1500], Loss: 0.36070895195007324\n",
            "Epoch [222/1500], Loss: 0.3003099262714386\n",
            "Epoch [224/1500], Loss: 0.29724982380867004\n",
            "patience exceeded, loading best model\n",
            "Epoch [2/1500], Loss: 0.8282817006111145\n",
            "Epoch [4/1500], Loss: 0.8336330652236938\n",
            "Epoch [6/1500], Loss: 0.6416457891464233\n",
            "Epoch [8/1500], Loss: 0.7979785203933716\n",
            "Epoch [10/1500], Loss: 0.8318940997123718\n",
            "Epoch [12/1500], Loss: 0.6706416606903076\n",
            "Epoch [14/1500], Loss: 0.7057168483734131\n",
            "Epoch [16/1500], Loss: 0.6551981568336487\n",
            "Epoch [18/1500], Loss: 0.6111273169517517\n",
            "Epoch [20/1500], Loss: 0.5701775550842285\n",
            "Epoch [22/1500], Loss: 0.6046056151390076\n",
            "Epoch [24/1500], Loss: 0.5859977006912231\n",
            "Epoch [26/1500], Loss: 0.5684302449226379\n",
            "Epoch [28/1500], Loss: 0.44046318531036377\n",
            "Epoch [30/1500], Loss: 0.557894766330719\n",
            "Epoch [32/1500], Loss: 0.5010506510734558\n",
            "Epoch [34/1500], Loss: 0.4803852438926697\n",
            "Epoch [36/1500], Loss: 0.4801909923553467\n",
            "Epoch [38/1500], Loss: 0.5620669722557068\n",
            "Epoch [40/1500], Loss: 0.4760023057460785\n",
            "Epoch [42/1500], Loss: 0.5505069494247437\n",
            "Epoch [44/1500], Loss: 0.48547881841659546\n",
            "Epoch [46/1500], Loss: 0.5436273217201233\n",
            "Epoch [48/1500], Loss: 0.42904162406921387\n",
            "Epoch [50/1500], Loss: 0.48361736536026\n",
            "Epoch [52/1500], Loss: 0.44064608216285706\n",
            "Epoch [54/1500], Loss: 0.5224671959877014\n",
            "Epoch [56/1500], Loss: 0.3956543207168579\n",
            "Epoch [58/1500], Loss: 0.4047123193740845\n",
            "Epoch [60/1500], Loss: 0.5485776662826538\n",
            "Epoch [62/1500], Loss: 0.3460739552974701\n",
            "Epoch [64/1500], Loss: 0.38055089116096497\n",
            "Epoch [66/1500], Loss: 0.5287118554115295\n",
            "Epoch [68/1500], Loss: 0.3686645030975342\n",
            "Epoch [70/1500], Loss: 0.43009093403816223\n",
            "Epoch [72/1500], Loss: 0.3579869568347931\n",
            "Epoch [74/1500], Loss: 0.38179001212120056\n",
            "Epoch [76/1500], Loss: 0.37567955255508423\n",
            "Epoch [78/1500], Loss: 0.4035273790359497\n",
            "Epoch [80/1500], Loss: 0.3860913813114166\n",
            "Epoch [82/1500], Loss: 0.39425140619277954\n",
            "Epoch [84/1500], Loss: 0.3693103790283203\n",
            "Epoch [86/1500], Loss: 0.39206135272979736\n",
            "Epoch [88/1500], Loss: 0.3953554630279541\n",
            "Epoch [90/1500], Loss: 0.35582074522972107\n",
            "Epoch [92/1500], Loss: 0.37941455841064453\n",
            "Epoch [94/1500], Loss: 0.41217684745788574\n",
            "Epoch [96/1500], Loss: 0.4173845648765564\n",
            "Epoch [98/1500], Loss: 0.39673489332199097\n",
            "Epoch [100/1500], Loss: 0.4217880368232727\n",
            "Epoch [102/1500], Loss: 0.3859243392944336\n",
            "Epoch [104/1500], Loss: 0.4032183885574341\n",
            "Epoch [106/1500], Loss: 0.39498209953308105\n",
            "Epoch [108/1500], Loss: 0.37149620056152344\n",
            "Epoch [110/1500], Loss: 0.33893322944641113\n",
            "Epoch [112/1500], Loss: 0.3672632575035095\n",
            "Epoch [114/1500], Loss: 0.37270984053611755\n",
            "Epoch [116/1500], Loss: 0.37364161014556885\n",
            "Epoch [118/1500], Loss: 0.3780063986778259\n",
            "Epoch [120/1500], Loss: 0.33595114946365356\n",
            "Epoch [122/1500], Loss: 0.3349083662033081\n",
            "Epoch [124/1500], Loss: 0.44036853313446045\n",
            "Epoch [126/1500], Loss: 0.3252389430999756\n",
            "Epoch [128/1500], Loss: 0.3430568277835846\n",
            "Epoch [130/1500], Loss: 0.3509627878665924\n",
            "Epoch [132/1500], Loss: 0.3808192014694214\n",
            "Epoch [134/1500], Loss: 0.37035292387008667\n",
            "Epoch [136/1500], Loss: 0.25776907801628113\n",
            "Epoch [138/1500], Loss: 0.3930608630180359\n",
            "Epoch [140/1500], Loss: 0.3919026553630829\n",
            "Epoch [142/1500], Loss: 0.38138291239738464\n",
            "Epoch [144/1500], Loss: 0.3092116117477417\n",
            "Epoch [146/1500], Loss: 0.3168659508228302\n",
            "Epoch [148/1500], Loss: 0.3367949426174164\n",
            "Epoch [150/1500], Loss: 0.3878879249095917\n",
            "Epoch [152/1500], Loss: 0.33141380548477173\n",
            "Epoch [154/1500], Loss: 0.37547603249549866\n",
            "Epoch [156/1500], Loss: 0.37923046946525574\n",
            "Epoch [158/1500], Loss: 0.3901672065258026\n",
            "Epoch [160/1500], Loss: 0.3404434323310852\n",
            "patience exceeded, loading best model\n",
            "Average accuracy: 0.43387285\n",
            "Average top_k_average_accuracies 0.99776715\n"
          ]
        }
      ],
      "source": [
        "# prompt: train and test my model on the Xs and ys in a 10 fold cross validation\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split data into 10 folds\n",
        "k_fold = KFold(n_splits=10, shuffle = True,random_state = None)\n",
        "# Scale data, not enabled right now.\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(Xs)\n",
        "# Xs = scaler.transform(Xs)\n",
        "\n",
        "# Train and test model on each fold\n",
        "best_model = None\n",
        "PATH = 'best_regressor_model.h5'\n",
        "best_accuracy = None\n",
        "best_accuracies = []\n",
        "accuracies = []\n",
        "top_k_average_accuracies = []\n",
        "knn_accuracies = []\n",
        "count = 0\n",
        "for train_index, test_index in k_fold.split(Xs):\n",
        "  # Get training and testing data\n",
        "  X_train, X_test = Xs[train_index], Xs[test_index]\n",
        "  y_train, y_test = ys[train_index], ys[test_index]\n",
        "\n",
        "  # lmnn = metric_learn.MLKR()\n",
        "  # lmnn.fit(X_train,y_train)\n",
        "  # knn = KNeighborsRegressor(n_neighbors=5,metric=lmnn.get_metric())\n",
        "  # knn.fit(X_train,y_train)\n",
        "  # knn_accuracies.append( accuracy_score(knn.predict(X_test), y_test))\n",
        "\n",
        "  # continue\n",
        "\n",
        "  #build a train loader with my X_train and X_test\n",
        "  train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Train model\n",
        "  model = NN_k_NN_regression(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing,\n",
        "                             hidden_layers)\n",
        "  # model = model.to(dev)\n",
        "  # print(\"number of parameters in model: \",len(list(model.parameters())))\n",
        "  # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #, weight_decay=1e-5)\n",
        "  # print_model_features(model)\n",
        "  #Training loop\n",
        "  patience_counter = 0\n",
        "  for epoch in range(training_epochs):\n",
        "    # break # no training\n",
        "    epoch_msg = True\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "      # print(\"training\")\n",
        "      ##DO NOT USE train() or eval() mode for regression\n",
        "      ##top k selection will mess up the final layer output\n",
        "      ##unlike in classification, each case's output is meaningful\n",
        "      ##here in regresion, the collective output is meaningful.\n",
        "      ## So if you topK case selection needs to be enabled or disabled the whole time.\n",
        "      model.train()\n",
        "      feature_activations, case_activations, predicted_number = model(X_train_batch)\n",
        "      # break\n",
        "      loss = criterion(predicted_number.squeeze(), y_train_batch)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if epoch_msg and (epoch + 1) % 2 == 0:\n",
        "        epoch_msg = False\n",
        "        print(f'Epoch [{epoch + 1}/{training_epochs}], Loss: {loss.item()}')\n",
        "        #inspecting the case activations\n",
        "        # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "        # print(top_case_indices)\n",
        "    # print(\"evaluating\")\n",
        "    # break\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      feature_activations, case_activations, predicted_number = model(X_test)\n",
        "\n",
        "    # inspecting the case activations\n",
        "    # top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "    # print(top_case_indices)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    # accuracy_temp = mean_squared_error(y_test, predicted_number.squeeze().detach())\n",
        "    accuracy_temp = criterion(y_test, predicted_number.squeeze().detach())\n",
        "    if epoch == 0:\n",
        "      best_accuracy = accuracy_temp\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "    elif accuracy_temp < best_accuracy:\n",
        "      #memorize best model\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "\n",
        "      best_accuracy = accuracy_temp\n",
        "      patience_counter = 0\n",
        "    elif patience_counter > patience:\n",
        "      model = NN_k_NN_regression(X_train, y_train,\n",
        "               fa_weight_sharing_within_segment,\n",
        "               fa_weight_sharing_between_segment,\n",
        "               ca_weight_sharing,\n",
        "               top_case_enabled, top_k,\n",
        "               class_weight_sharing)\n",
        "      model.load_state_dict(torch.load(PATH))\n",
        "      model.eval()\n",
        "      print(\"patience exceeded, loading best model\")\n",
        "      break\n",
        "    else:\n",
        "      patience_counter += 1\n",
        "    # temp.append(accuracy_temp)\n",
        "    # if len(temp) > patience and accuracy_temp > min(temp[-patience:]):\n",
        "    #   ##patience runs out\n",
        "    #   break\n",
        "  # best_accuracies.append(min(temp))\n",
        "  best_accuracies.append(best_accuracy)\n",
        "\n",
        "  # print(\"testing\")\n",
        "  # print_model_features(model)\n",
        "  # Test model\n",
        "  feature_activations, case_activations, predicted_number = model(X_test)\n",
        "\n",
        "  #inspecting the case activations\n",
        "  top_case_indices = torch.topk(case_activations, 5, dim=1)[1]\n",
        "  # print(top_case_indices)\n",
        "\n",
        "  # Using top case solution as final solution\n",
        "  # Calculate accuracy\n",
        "  # accuracy = mean_squared_error(y_test, predicted_number)\n",
        "  accuracy = criterion(y_test, predicted_number.squeeze())\n",
        "  # Add accuracy to list\n",
        "  accuracies.append(accuracy)\n",
        "\n",
        "  top_k_average_accuracies.append(mean_squared_error(torch.mean(y_train[top_case_indices], dim=1),\n",
        "                                                     y_test))\n",
        "\n",
        "  ##compare with a normal k-nn\n",
        "  knn =  KNeighborsRegressor(n_neighbors=top_k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  knn_accuracies.append( mean_squared_error(knn.predict(X_test), y_test))\n",
        "\n",
        "  # break\n",
        "  # count += 1\n",
        "  # if count == 5:\n",
        "  #   break\n",
        "\n",
        "# Print average accuracy\n",
        "# print(\"Average accuracy:\", np.mean(accuracies.detach().numpy()))\n",
        "print(\"Average accuracy:\", np.mean([acc.detach().numpy() for acc in accuracies]))\n",
        "\n",
        "# print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n",
        "print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n",
        "# torch.cat(torch.mean(top_k_average_accuracies)).item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(knn_accuracies))"
      ],
      "metadata": {
        "id": "smeb3Z1O2RRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed38c25b-345f-48a5-c82e-2983bdf16c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.592213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kiHFacefPBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e66109-a465-4279-cca2-ae6cfec94bcf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "X_train[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shTwkGdc2bDV"
      },
      "outputs": [],
      "source": [
        "# top_k_average_accuracies.append(mean_squared_error(torch.mean(y_train[top_case_indices], dim=1),\n",
        "#                                                      y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guA1k-Fd490p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025bfeb9-3769-46c3-e52e-863145a22735"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "predicted_number.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muGHh0IpcSF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ae5869-b2be-4575-8f8d-baf734b5653c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.8590, -0.2511, -0.1026,  1.2135,  1.0687, -0.7840,  1.2388,  1.1695,\n",
              "        -1.0307,  0.4343,  0.3279, -0.3448, -0.2235,  0.1660,  0.0145, -1.1071,\n",
              "         0.8048,  0.9167,  0.2146,  0.0121, -1.4749, -1.1777, -0.2260, -0.0639],\n",
              "       grad_fn=<MvBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "predicted_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1mhapnXSVsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196468b0-3c26-4a6b-b259-c30fe0865d57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ],
      "source": [
        "model.cases.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nBkRvYdcWEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72d4537-a7f5-4e17-f887-633c0e6bbc96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "X_test.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m0Dco_SOHMb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW_Fy4y1WqNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063d5d53-13b5-41ca-b1cc-1eb68a2bc504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.4533),\n",
              " tensor(0.3362),\n",
              " tensor(0.3708),\n",
              " tensor(0.3828),\n",
              " tensor(0.3325),\n",
              " tensor(0.5644),\n",
              " tensor(0.4630),\n",
              " tensor(0.4585),\n",
              " tensor(0.3124),\n",
              " tensor(0.6648)]"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "best_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apW8H37ZmIUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec304b6-6b9f-4497-dc8a-1cf204ffb5fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8717186,\n",
              " 1.2572063,\n",
              " 0.7170766,\n",
              " 1.1177417,\n",
              " 0.5962,\n",
              " 1.3180887,\n",
              " 0.98034114,\n",
              " 1.045724,\n",
              " 0.6749353,\n",
              " 1.3986393]"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "top_k_average_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8DchB3qkqAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb9083c-b014-4229-97f6-b52fc50e1ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average top_k_average_accuracies 0.99776715\n"
          ]
        }
      ],
      "source": [
        "# print(\"Average top_k_average_accuracies\", torch.stack(torch.mean(top_k_average_accuracies)).item())\n",
        "print(\"Average top_k_average_accuracies\", np.mean(top_k_average_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooy_sIumpQEr"
      },
      "outputs": [],
      "source": [
        "tens_1 = torch.Tensor([[10, 20], [30, 40]])\n",
        "tens_2 = torch.Tensor([[2], [4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9nYCnL7pTZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de1c33e-30ff-4b0c-8579-71e48512c8a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 20.,  40.],\n",
              "        [120., 160.]])"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "tens_1 * tens_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lehnuufypYMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1ac9dd-2295-4488-b114-904fe23ec6ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 20.,  40.],\n",
              "        [120., 160.]])"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ],
      "source": [
        "torch.mul(tens_1, tens_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWRu_gbNI5NA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuNUzhWmpa7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3e2a71-2d0e-4f4e-885d-73b15bba395c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[100.],\n",
              "        [220.]])"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "torch.matmul(tens_1, tens_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvDGfi_Rp1mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c79efb-db86-466a-8750-84ce255a5265"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 60., 280.])"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "torch.sum(tens_1 * tens_2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS1hCvnB44zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca85b221-b65d-49a7-c00b-3f696ebcd521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.9435383 , -0.51429   ,  0.57908124,  1.0349183 ,  0.8712851 ,\n",
              "       -0.7321125 ,  1.2527446 ,  0.32300663, -0.9425009 ,  0.5812067 ,\n",
              "        0.56208   , -0.5047266 ,  0.12962064, -0.19870977, -0.36659393,\n",
              "       -0.49197635,  0.30494398,  0.6300849 ,  0.42607412, -0.72042495,\n",
              "       -1.2803947 , -0.53660357, -0.87555873, -0.8341185 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d60j7sj46Ar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ae7c9f-ee18-4cfa-eb20-f4b004b232b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.5524, -0.2965, -0.2593,  0.3729,  0.1232, -0.6524,  0.6226,  1.5258,\n",
              "        -1.0881,  1.2177,  1.1911,  2.2856, -0.2911, -0.1636,  0.1711, -0.7693,\n",
              "        -0.4080,  0.4526, -0.8384,  0.7661, -1.3643, -1.2049,  0.4261,  0.9308])"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pclYFL115U-Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(knn.predict(X_test), y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik_1t2154sZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc3751b-9ea9-4da3-8bbd-47074b09c32c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94716805"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "517-keElDvpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069837a4-f288-43a3-bf9f-b58999dab5ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RegressionActivation_3_Layer()"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ],
      "source": [
        "model.class_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWU8UiHDD_PO"
      },
      "outputs": [],
      "source": [
        "# model.class_layer.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Eu3P0zELJ4"
      },
      "outputs": [],
      "source": [
        "# model.class_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvZmmgJaXd85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ada6d38-6e32-4358-b3f4-b2a34c0579f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2838\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koSz4phP7J0z"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYQCH4N6I8zO"
      },
      "source": [
        "##Result look up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gARxSXiq-_W",
        "outputId": "98bdc9c2-499c-4568-a6f5-c4aa853fc2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fa_layer.f1weight\n",
            "tensor([0.2030, 0.1868])\n",
            "fa_layer.layer1.weight\n",
            "tensor([[-0.0018, -0.0747],\n",
            "        [-0.3963,  0.5785],\n",
            "        [-0.0224, -0.5366],\n",
            "        [-0.0434,  0.5925]])\n",
            "fa_layer.layer1.bias\n",
            "tensor([ 0.4317, -0.2874,  0.3923, -0.2067])\n",
            "fa_layer.layer2.weight\n",
            "tensor([[ 0.4153, -0.3496,  0.2090,  0.3790],\n",
            "        [ 0.3129,  0.2788, -0.4245,  0.0112]])\n",
            "fa_layer.layer2.bias\n",
            "tensor([ 0.0168, -0.2969])\n",
            "ca_layer.fa_weight\n",
            "tensor([[1.2816, 1.2769],\n",
            "        [1.3401, 1.3018],\n",
            "        [1.2851, 0.3914],\n",
            "        [1.3115, 0.2436],\n",
            "        [1.3630, 0.5401],\n",
            "        [1.3629, 0.5403],\n",
            "        [1.3164, 0.4516],\n",
            "        [1.3097, 0.2454],\n",
            "        [1.2775, 0.3264],\n",
            "        [1.3210, 1.3003],\n",
            "        [1.3450, 0.7852],\n",
            "        [1.3189, 0.8448],\n",
            "        [1.4404, 0.0128],\n",
            "        [1.4400, 0.1882],\n",
            "        [1.4362, 0.2318],\n",
            "        [1.4084, 0.4315],\n",
            "        [1.3774, 0.5798],\n",
            "        [1.3440, 0.5473],\n",
            "        [1.4022, 0.4256],\n",
            "        [1.3949, 0.5004],\n",
            "        [1.3484, 0.4078],\n",
            "        [1.3953, 0.4760],\n",
            "        [1.3781, 1.3397],\n",
            "        [1.3447, 1.1057],\n",
            "        [1.3829, 0.4703],\n",
            "        [1.3714, 0.7756],\n",
            "        [1.3238, 0.5083],\n",
            "        [1.3040, 0.6010],\n",
            "        [1.3457, 0.6570],\n",
            "        [1.4423, 0.6498],\n",
            "        [1.3789, 0.4359],\n",
            "        [1.3526, 0.7965],\n",
            "        [1.3730, 0.3331],\n",
            "        [1.3692, 0.3108],\n",
            "        [1.3798, 0.5511],\n",
            "        [1.4222, 1.3351],\n",
            "        [1.3857, 0.2869],\n",
            "        [1.3861, 1.3151],\n",
            "        [1.3669, 0.4239],\n",
            "        [1.3979, 0.3033],\n",
            "        [1.4052, 0.1527],\n",
            "        [1.3914, 0.3454],\n",
            "        [1.3549, 0.8073],\n",
            "        [1.4045, 0.4885],\n",
            "        [1.4173, 0.0911],\n",
            "        [1.3488, 1.3399],\n",
            "        [1.4023, 0.3986],\n",
            "        [1.4283, 0.1338],\n",
            "        [1.4254, 0.5361],\n",
            "        [1.4085, 0.2190],\n",
            "        [1.3706, 0.8415],\n",
            "        [1.3628, 0.1383],\n",
            "        [1.3725, 0.3413],\n",
            "        [1.3815, 0.2980],\n",
            "        [1.3786, 0.0920],\n",
            "        [1.3673, 0.3089],\n",
            "        [1.2998, 1.2817],\n",
            "        [1.4079, 0.4621],\n",
            "        [1.3686, 0.4329],\n",
            "        [1.4246, 0.5028],\n",
            "        [1.3144, 1.3100],\n",
            "        [1.4020, 0.3004],\n",
            "        [1.3329, 0.4726],\n",
            "        [1.3342, 0.4319],\n",
            "        [1.4237, 0.3032],\n",
            "        [1.3727, 0.5162],\n",
            "        [1.3811, 0.4115],\n",
            "        [1.4019, 0.2965],\n",
            "        [1.4007, 0.1145],\n",
            "        [1.3946, 1.3697],\n",
            "        [1.4032, 0.1346],\n",
            "        [1.3748, 0.3574],\n",
            "        [1.3181, 0.4075],\n",
            "        [1.3272, 0.4463],\n",
            "        [1.3920, 1.3810],\n",
            "        [1.3111, 0.5417],\n",
            "        [1.3186, 0.6679],\n",
            "        [1.4109, 0.4810],\n",
            "        [1.3514, 0.4089],\n",
            "        [1.3367, 0.4611],\n",
            "        [1.3610, 1.2930],\n",
            "        [0.9626, 0.4833],\n",
            "        [1.4152, 0.1555],\n",
            "        [1.3933, 0.4278],\n",
            "        [1.3769, 0.4225],\n",
            "        [1.3862, 0.4469],\n",
            "        [1.4126, 0.4340],\n",
            "        [1.3949, 0.4410],\n",
            "        [1.3847, 0.4459],\n",
            "        [1.0761, 0.4447],\n",
            "        [1.5302, 0.8646],\n",
            "        [0.8038, 0.3348],\n",
            "        [1.9874, 1.1438],\n",
            "        [1.9022, 1.1941],\n",
            "        [1.4502, 1.4664],\n",
            "        [1.6433, 0.7915],\n",
            "        [1.4697, 0.6807],\n",
            "        [0.8536, 0.6907],\n",
            "        [1.7330, 1.5867],\n",
            "        [0.4899, 1.3759],\n",
            "        [0.4206, 1.3417],\n",
            "        [0.2214, 1.3989],\n",
            "        [0.3745, 1.3978],\n",
            "        [0.2217, 1.3952],\n",
            "        [0.5167, 1.3778],\n",
            "        [1.3256, 1.3485],\n",
            "        [0.3889, 1.4371],\n",
            "        [0.4526, 1.4383],\n",
            "        [0.4484, 1.4126],\n",
            "        [0.6176, 1.3923],\n",
            "        [0.4925, 1.4209],\n",
            "        [0.4753, 1.4214],\n",
            "        [0.3755, 1.4411],\n",
            "        [1.5758, 1.7477],\n",
            "        [0.5135, 1.4307],\n",
            "        [0.2605, 1.3957],\n",
            "        [0.5726, 1.4083],\n",
            "        [0.6643, 1.4109],\n",
            "        [0.3466, 1.4139],\n",
            "        [0.4555, 1.3756],\n",
            "        [0.3307, 1.4235],\n",
            "        [0.6779, 1.4341],\n",
            "        [1.2630, 1.2551],\n",
            "        [0.0208, 1.4615],\n",
            "        [0.4526, 1.3681],\n",
            "        [1.3839, 1.4574],\n",
            "        [0.2991, 1.4295],\n",
            "        [0.3335, 1.4280],\n",
            "        [0.4507, 1.4161],\n",
            "        [1.1133, 1.9106],\n",
            "        [0.5258, 1.3507],\n",
            "        [0.3355, 1.4291],\n",
            "        [1.1062, 1.7488],\n",
            "        [0.4788, 1.4159],\n",
            "        [0.5204, 1.3579],\n",
            "        [0.3644, 1.4200],\n",
            "        [0.4782, 1.3662],\n",
            "        [1.3411, 1.4512],\n",
            "        [0.4887, 1.3592],\n",
            "        [0.3488, 1.4218],\n",
            "        [0.4441, 1.3660],\n",
            "        [0.3584, 1.4195],\n",
            "        [0.4496, 1.3225],\n",
            "        [0.3880, 1.3671],\n",
            "        [0.2096, 1.4460],\n",
            "        [0.1842, 1.4485],\n",
            "        [0.2478, 1.4014],\n",
            "        [0.4951, 1.4924],\n",
            "        [1.2839, 1.2789],\n",
            "        [0.2297, 1.3752],\n",
            "        [0.5426, 1.4404],\n",
            "        [1.6097, 1.8039],\n",
            "        [0.2535, 1.4116],\n",
            "        [0.4485, 1.3693],\n",
            "        [0.6514, 0.5611],\n",
            "        [0.8011, 1.4180],\n",
            "        [0.5286, 1.3756],\n",
            "        [0.0069, 1.4288],\n",
            "        [0.5154, 1.3519],\n",
            "        [1.2159, 1.4287],\n",
            "        [1.3063, 1.3618],\n",
            "        [0.5243, 1.3650],\n",
            "        [0.4514, 1.4270],\n",
            "        [0.4026, 1.3770],\n",
            "        [0.4140, 1.3794],\n",
            "        [0.4154, 1.3902],\n",
            "        [0.5612, 1.4116],\n",
            "        [0.2301, 1.3871],\n",
            "        [0.2855, 1.3633],\n",
            "        [0.4487, 1.3829],\n",
            "        [0.6004, 1.1683],\n",
            "        [0.5700, 1.3915],\n",
            "        [0.6106, 1.3915],\n",
            "        [0.5522, 1.3957],\n",
            "        [0.7156, 1.3568],\n",
            "        [0.7127, 1.3549],\n",
            "        [0.2961, 1.3557],\n",
            "        [0.5538, 1.3917],\n",
            "        [0.4266, 1.3929],\n",
            "        [0.3201, 1.3177],\n",
            "        [0.5674, 1.3816],\n",
            "        [0.7784, 1.4688],\n",
            "        [0.5049, 1.3623],\n",
            "        [1.3820, 1.3963],\n",
            "        [0.7146, 1.4132],\n",
            "        [0.5702, 1.3909],\n",
            "        [1.2412, 1.3817],\n",
            "        [0.5055, 1.3620],\n",
            "        [0.7341, 1.2522],\n",
            "        [0.3083, 1.3824],\n",
            "        [0.0061, 1.4025],\n",
            "        [0.4529, 1.4371],\n",
            "        [0.2805, 1.3657],\n",
            "        [0.4227, 1.4262],\n",
            "        [0.4791, 1.0484],\n",
            "        [0.1220, 1.3465],\n",
            "        [0.0121, 1.4033],\n",
            "        [0.5784, 1.0221]])\n",
            "ca_layer.bias\n",
            "tensor([1.6158, 1.6027, 1.5917, 1.6049, 1.6798, 1.6798, 1.6307, 1.6083, 1.6238,\n",
            "        1.5979, 1.6316, 1.6351, 1.6459, 1.6174, 1.6096, 1.5449, 1.6760, 1.6765,\n",
            "        1.5445, 1.6379, 1.7165, 1.6312, 1.6108, 1.6130, 1.6870, 1.6358, 1.6140,\n",
            "        1.6011, 1.6226, 1.6463, 1.7139, 1.6377, 1.6709, 1.5848, 1.6063, 1.6181,\n",
            "        1.6541, 1.5549, 1.5817, 1.6191, 1.6224, 1.6371, 1.5784, 1.5849, 1.6565,\n",
            "        1.6288, 1.6247, 1.7383, 1.6279, 1.6890, 1.6298, 1.6489, 1.5398, 1.6634,\n",
            "        1.6434, 1.5457, 1.5878, 1.6515, 1.6337, 1.6395, 1.6186, 1.5840, 1.5825,\n",
            "        1.5797, 1.4893, 1.5194, 1.5600, 1.5841, 1.7008, 1.6353, 1.6942, 1.6523,\n",
            "        1.6145, 1.5552, 1.6219, 1.5253, 1.4617, 1.6231, 1.6324, 1.6290, 1.6017,\n",
            "        1.6380, 1.6048, 1.6306, 1.6379, 1.6072, 1.7049, 1.6250, 1.6013, 1.6470,\n",
            "        1.3772, 1.6381, 1.4223, 1.2955, 1.0033, 1.4993, 1.6453, 1.6076, 1.0439,\n",
            "        1.6080, 1.5725, 1.6182, 1.6262, 1.6025, 1.6079, 1.6123, 1.6113, 1.6430,\n",
            "        1.7272, 1.6980, 1.7143, 1.7279, 1.6269, 1.3650, 1.6789, 1.5737, 1.5740,\n",
            "        1.5883, 1.5743, 1.6013, 1.5423, 1.5405, 1.6131, 1.6934, 1.6033, 1.6286,\n",
            "        1.6024, 1.6824, 1.5635, 1.2758, 1.6223, 1.6967, 1.3492, 1.5885, 1.6565,\n",
            "        1.6108, 1.5417, 1.6123, 1.6552, 1.6037, 1.6526, 1.6133, 1.6683, 1.6397,\n",
            "        1.6108, 1.6093, 1.5243, 1.7482, 1.6345, 1.6958, 1.6391, 1.5473, 1.6050,\n",
            "        1.5425, 1.5061, 1.5834, 1.6552, 1.6776, 1.5040, 1.6066, 1.6002, 1.6667,\n",
            "        1.6166, 1.6640, 1.6721, 1.6663, 1.5977, 1.7131, 1.6626, 1.6355, 1.6436,\n",
            "        1.4246, 1.4296, 1.5787, 1.5677, 1.5687, 1.7071, 1.4256, 1.4741, 1.6472,\n",
            "        1.6426, 1.3609, 1.5354, 0.7665, 1.4213, 1.6420, 0.7924, 1.5346, 1.5275,\n",
            "        1.5384, 1.5847, 1.6152, 1.5829, 1.6427, 1.6135, 1.5790, 1.5791, 1.5476])\n",
            "class_layer.ca_weight\n",
            "tensor([[1.6169, 1.0000],\n",
            "        [1.6139, 1.0000],\n",
            "        [1.5593, 1.0000],\n",
            "        [1.5761, 1.0000],\n",
            "        [1.6228, 1.0000],\n",
            "        [1.6228, 1.0000],\n",
            "        [1.5949, 1.0000],\n",
            "        [1.5772, 1.0000],\n",
            "        [1.5747, 1.0000],\n",
            "        [1.0000, 1.5995],\n",
            "        [1.0000, 1.6364],\n",
            "        [1.0000, 1.6369],\n",
            "        [1.0000, 1.5280],\n",
            "        [1.0000, 1.5387],\n",
            "        [1.0000, 1.5401],\n",
            "        [1.0000, 1.5506],\n",
            "        [1.0000, 1.6279],\n",
            "        [1.0000, 1.6187],\n",
            "        [1.0000, 1.5520],\n",
            "        [1.5999, 1.0000],\n",
            "        [1.6174, 1.0000],\n",
            "        [1.6006, 1.0000],\n",
            "        [1.6242, 1.0000],\n",
            "        [1.6199, 1.0000],\n",
            "        [1.0000, 1.6292],\n",
            "        [1.0000, 1.6328],\n",
            "        [1.0000, 1.6420],\n",
            "        [1.0000, 1.6395],\n",
            "        [1.0000, 1.6225],\n",
            "        [1.0000, 1.6415],\n",
            "        [1.0000, 1.6291],\n",
            "        [1.0000, 1.6403],\n",
            "        [1.5825, 1.0000],\n",
            "        [1.5170, 1.0000],\n",
            "        [1.5836, 1.0000],\n",
            "        [1.6232, 1.0000],\n",
            "        [1.5823, 1.0000],\n",
            "        [1.5639, 1.0000],\n",
            "        [1.5542, 1.0000],\n",
            "        [1.5684, 1.0000],\n",
            "        [1.5358, 1.0000],\n",
            "        [1.5621, 1.0000],\n",
            "        [1.0000, 1.5796],\n",
            "        [1.0000, 1.5600],\n",
            "        [1.0000, 1.5542],\n",
            "        [1.0000, 1.6312],\n",
            "        [1.0000, 1.6051],\n",
            "        [1.0000, 1.6386],\n",
            "        [1.0000, 1.5935],\n",
            "        [1.0000, 1.6184],\n",
            "        [1.0000, 1.6311],\n",
            "        [1.5616, 1.0000],\n",
            "        [1.5403, 1.0000],\n",
            "        [1.6152, 1.0000],\n",
            "        [1.5692, 1.0000],\n",
            "        [1.5447, 1.0000],\n",
            "        [1.5904, 1.0000],\n",
            "        [1.5987, 1.0000],\n",
            "        [1.6049, 1.0000],\n",
            "        [1.5992, 1.0000],\n",
            "        [1.6183, 1.0000],\n",
            "        [1.0000, 1.5193],\n",
            "        [1.0000, 1.5477],\n",
            "        [1.0000, 1.5439],\n",
            "        [1.0000, 1.4597],\n",
            "        [1.0000, 1.4815],\n",
            "        [1.0000, 1.4963],\n",
            "        [1.0000, 1.5189],\n",
            "        [1.0000, 1.6094],\n",
            "        [1.0000, 1.6374],\n",
            "        [1.0000, 1.6037],\n",
            "        [1.6093, 1.0000],\n",
            "        [1.6167, 1.0000],\n",
            "        [1.5692, 1.0000],\n",
            "        [1.6274, 1.0000],\n",
            "        [1.5397, 1.0000],\n",
            "        [1.5280, 1.0000],\n",
            "        [1.5787, 1.0000],\n",
            "        [1.6280, 1.0000],\n",
            "        [1.5995, 1.0000],\n",
            "        [1.6059, 1.0000],\n",
            "        [1.0000, 1.6490],\n",
            "        [1.0000, 1.5419],\n",
            "        [1.0000, 1.6357],\n",
            "        [1.0000, 1.5937],\n",
            "        [1.0000, 1.6199],\n",
            "        [1.0000, 1.6235],\n",
            "        [1.0000, 1.5866],\n",
            "        [1.0000, 1.5679],\n",
            "        [1.0000, 1.6591],\n",
            "        [1.5233, 1.0000],\n",
            "        [1.6050, 1.0000],\n",
            "        [1.5749, 1.0000],\n",
            "        [1.4998, 1.0000],\n",
            "        [1.2869, 1.0000],\n",
            "        [1.6049, 1.0000],\n",
            "        [1.6297, 1.0000],\n",
            "        [1.5823, 1.0000],\n",
            "        [1.2869, 1.0000],\n",
            "        [1.5970, 1.0000],\n",
            "        [1.5431, 1.0000],\n",
            "        [1.5842, 1.0000],\n",
            "        [1.5487, 1.0000],\n",
            "        [1.5779, 1.0000],\n",
            "        [1.5999, 1.0000],\n",
            "        [1.6234, 1.0000],\n",
            "        [1.0000, 1.6111],\n",
            "        [1.0000, 1.5888],\n",
            "        [1.0000, 1.6437],\n",
            "        [1.0000, 1.6483],\n",
            "        [1.0000, 1.6430],\n",
            "        [1.0000, 1.6416],\n",
            "        [1.0000, 1.6233],\n",
            "        [1.0000, 1.5970],\n",
            "        [1.0000, 1.6328],\n",
            "        [1.5103, 1.0000],\n",
            "        [1.5551, 1.0000],\n",
            "        [1.6041, 1.0000],\n",
            "        [1.5609, 1.0000],\n",
            "        [1.5943, 1.0000],\n",
            "        [1.5226, 1.0000],\n",
            "        [1.5674, 1.0000],\n",
            "        [1.6118, 1.0000],\n",
            "        [1.5629, 1.0000],\n",
            "        [1.5946, 1.0000],\n",
            "        [1.0000, 1.6454],\n",
            "        [1.0000, 1.5466],\n",
            "        [1.0000, 1.6769],\n",
            "        [1.0000, 1.5379],\n",
            "        [1.0000, 1.5643],\n",
            "        [1.0000, 1.6033],\n",
            "        [1.0000, 1.6818],\n",
            "        [1.0000, 1.6405],\n",
            "        [1.0000, 1.5818],\n",
            "        [1.6048, 1.0000],\n",
            "        [1.5878, 1.0000],\n",
            "        [1.5364, 1.0000],\n",
            "        [1.6239, 1.0000],\n",
            "        [1.6119, 1.0000],\n",
            "        [1.5886, 1.0000],\n",
            "        [1.5996, 1.0000],\n",
            "        [1.5929, 1.0000],\n",
            "        [1.6088, 1.0000],\n",
            "        [1.0000, 1.6020],\n",
            "        [1.0000, 1.5549],\n",
            "        [1.0000, 1.5527],\n",
            "        [1.0000, 1.5088],\n",
            "        [1.0000, 1.7188],\n",
            "        [1.0000, 1.6348],\n",
            "        [1.0000, 1.6253],\n",
            "        [1.0000, 1.6076],\n",
            "        [1.0000, 1.6152],\n",
            "        [1.0000, 1.5334],\n",
            "        [1.5583, 1.0000],\n",
            "        [1.5495, 1.0000],\n",
            "        [1.5929, 1.0000],\n",
            "        [1.6204, 1.0000],\n",
            "        [1.5713, 1.0000],\n",
            "        [1.5255, 1.0000],\n",
            "        [1.6165, 1.0000],\n",
            "        [1.6139, 1.0000],\n",
            "        [1.6120, 1.0000],\n",
            "        [1.0000, 1.5895],\n",
            "        [1.0000, 1.6271],\n",
            "        [1.0000, 1.6274],\n",
            "        [1.0000, 1.6573],\n",
            "        [1.0000, 1.5974],\n",
            "        [1.0000, 1.6456],\n",
            "        [1.0000, 1.6180],\n",
            "        [1.0000, 1.6400],\n",
            "        [1.6306, 1.0000],\n",
            "        [1.4877, 1.0000],\n",
            "        [1.5007, 1.0000],\n",
            "        [1.5878, 1.0000],\n",
            "        [1.6244, 1.0000],\n",
            "        [1.6247, 1.0000],\n",
            "        [1.6124, 1.0000],\n",
            "        [1.4875, 1.0000],\n",
            "        [1.5200, 1.0000],\n",
            "        [1.5758, 1.0000],\n",
            "        [1.0000, 1.6409],\n",
            "        [1.0000, 1.4404],\n",
            "        [1.0000, 1.5398],\n",
            "        [1.0000, 1.0843],\n",
            "        [1.0000, 1.4857],\n",
            "        [1.0000, 1.6391],\n",
            "        [1.0000, 0.9821],\n",
            "        [1.0000, 1.5400],\n",
            "        [1.5246, 1.0000],\n",
            "        [1.4681, 1.0000],\n",
            "        [1.5296, 1.0000],\n",
            "        [1.6243, 1.0000],\n",
            "        [1.5026, 1.0000],\n",
            "        [1.6367, 1.0000],\n",
            "        [1.5720, 1.0000],\n",
            "        [1.4864, 1.0000],\n",
            "        [1.5280, 1.0000],\n",
            "        [1.5441, 1.0000]])\n",
            "class_layer.bias\n",
            "tensor([1.0604, 0.9396])\n"
          ]
        }
      ],
      "source": [
        "print_model_features(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y90VH3xzsTEd",
        "outputId": "dcdc3910-5cb7-461e-bc34-95657799ed14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "521"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT0y7FUq2FNC"
      },
      "source": [
        "####TODO testing here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awqOomGo1orp"
      },
      "outputs": [],
      "source": [
        "# for regression only. for classification is different\n",
        "#feature_activations, case_activations, predicted_number\n",
        "model.eval()\n",
        "case_activations, output, predicted_class = model(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S94m-mPsZ-tY"
      },
      "outputs": [],
      "source": [
        "top_case_indices = torch.topk(case_activations, 5, dim=1)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5h3V73UaI3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a547d9-d951-4a59-dc1f-e022cd251837"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[64, 67, 66, 65, 68],\n",
              "        [ 5, 66, 68, 67, 64],\n",
              "        [ 1, 66, 68, 67, 63],\n",
              "        [64, 67, 66, 65, 68],\n",
              "        [10,  9, 66, 67, 64],\n",
              "        [30, 65, 67, 66, 69],\n",
              "        [64, 67, 66, 65, 68],\n",
              "        [34, 40, 66, 67, 65],\n",
              "        [64, 67, 66, 65, 68],\n",
              "        [57, 65, 67, 66, 68],\n",
              "        [64, 67, 66, 65, 68]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "top_case_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFkiqBp91san",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6f701c-3da9-40ba-b629-60b9f29a5a84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7o4dGeF1uEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e223482e-8bfb-4fe5-8b47-a1ac40a1a3cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIMYkbGc-Gv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e24c86-9588-48ad-b3d1-1d305a3745d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17,  1,  1,  1,  2,  7,  8,  8, 10, 11, 11, 12, 13, 13, 14, 14, 16,\n",
              "       17, 14, 19, 20, 21, 22, 22, 23, 23, 24, 25, 27, 28, 29, 30, 32, 32,\n",
              "       33,  4, 37, 38, 38,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ],
      "source": [
        "knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FClgau3yY32o"
      },
      "outputs": [],
      "source": [
        "# X_test[0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJc1HOGxrdEJ"
      },
      "outputs": [],
      "source": [
        "# case_activations, output, predicted_class = model(X_test[120].unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jwdhc0IalI-"
      },
      "outputs": [],
      "source": [
        "# case_activations[0].nonzero()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv7b51l1ankc"
      },
      "outputs": [],
      "source": [
        "# case_activations[0].nonzero()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYqw4PIa0OO2"
      },
      "outputs": [],
      "source": [
        "# predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD0QdQ7a6Ov5"
      },
      "outputs": [],
      "source": [
        "# y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoHcFKWfrh35"
      },
      "outputs": [],
      "source": [
        "# knn_predict = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A07T05e8rzT"
      },
      "outputs": [],
      "source": [
        "# prompt: show where there is the difference between predicted_class and knn_predict\n",
        "\n",
        "# (predicted_class != torch.tensor(knn_predict)).nonzero(as_tuple=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q6RszGcsKRg"
      },
      "outputs": [],
      "source": [
        "# knn.predict(X_test[120].unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UklELrXlZNvm"
      },
      "source": [
        "###Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DAHjmSILFnV",
        "outputId": "3c536002-9eca-4a8f-e125-d5566c15c12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.45454545454545453, 0.5454545454545454, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.5454545454545454, 0.6363636363636364, 0.36363636363636365, 0.8181818181818182]\n"
          ]
        }
      ],
      "source": [
        "print(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GThviV3f3w6h",
        "outputId": "ee49fe7f-6cbe-474d-a213-3a7ec20ac49f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.45454545454545453, 0.6363636363636364, 0.2727272727272727, 0.36363636363636365, 0.45454545454545453, 0.2727272727272727, 0.2727272727272727, 0.36363636363636365, 0.18181818181818182, 0.36363636363636365]\n"
          ]
        }
      ],
      "source": [
        "print(knn_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10AORGQInY-G",
        "outputId": "1b73996c-0372-4fd1-bc04-c8293fde76cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8727272727272727\n",
            "0.3181818181818182\n",
            "nan\n"
          ]
        }
      ],
      "source": [
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "print(np.mean(klmnn_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "nllY5LQDDKse",
        "outputId": "cdb338fd-b5c4-4505-f717-7fc5127d34b1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got numpy.float64",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-6f1f73685118>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k_average_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.float64"
          ]
        }
      ],
      "source": [
        "# For regression\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVHtGRsrUu7H"
      },
      "outputs": [],
      "source": [
        "# prompt: get the number of unique values in the list of tensor y_train\n",
        "\n",
        "print(torch.unique(y_train).shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ub7LFE5UdDP"
      },
      "outputs": [],
      "source": [
        "torch.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qeV7IHLDpRR"
      },
      "source": [
        "##Results, classification, zebra\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzEwyfuOGElJ"
      },
      "source": [
        "we are using 110 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDn2UR_hDs2_"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9090909090909092\n",
        "0.36363636363636365\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99OeX0wUD5EG"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.8727272727272727\n",
        "0.3181818181818182\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxMcxPBaE4T1"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.8727272727272727\n",
        "0.34545454545454546\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xBV-JBRFnsH"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9181818181818182\n",
        "0.3090909090909091\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSOsMVN0q5oO"
      },
      "source": [
        "##Results, classification, double zebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4NG5pN4rJKs"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.85\n",
        "0.36818181818181817\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM0I_QD7s00D"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = False\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9727272727272729\n",
        "0.39545454545454545\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FkVw9gg2qi6"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "case weighting can compensate!! Even\n",
        "\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9681818181818181\n",
        "0.36818181818181817\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv1COulNQdAY"
      },
      "source": [
        "##Results, classification, fetch_olivetti_faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggj0rN4XQhBt"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "0s\n",
        "123\n",
        "# For classificationprint(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.8625\n",
        "0.85\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1C9EQRsHflK"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classificationprint(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9349999999999998\n",
        "0.85\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7RU-RRcDC6F"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9875\n",
        "0.8875\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCYyxPiJ3sl7"
      },
      "source": [
        "##Results, classification, digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJnfWHP1j57Y"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "During this run, we noticed that the training stopped pretty fast after patience. If patience is 40, we often stop before 50. This means that this data set is actually easy.\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9833022967101179\n",
        "0.9872004965859714\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkXc8OnvMI2m"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "0.9872036002482931\n",
        "0.9872098075729362\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ts8a5_o_gP"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "0.988\n",
        "0.985\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_srOl9RDC4E_"
      },
      "source": [
        "##Results, classification, iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d87QjdBtP7w"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9266666666666667\n",
        "0.9666666666666668\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivz0QCjhzpqM"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9666666666666668\n",
        "0.96\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ02ethFJ0B0"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.96\n",
        "0.9666666666666668\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqSg9--skYJy"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = False\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9533333333333334\n",
        "0.96\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp1OFAQOziaS"
      },
      "source": [
        "##Results, classification, wine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-vxACVlzk1u"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classificationprint(np.mean(accuracies))print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.961111111111111\n",
        "0.6803921568627451\n",
        "```\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.8663398692810457\n",
        "0.6980392156862746\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI9xXzSz0yyL"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "new\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9944444444444445\n",
        "0.6839869281045752\n",
        "```\n",
        "\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.8052287581699347\n",
        "0.684640522875817\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRgzE4jl1E_v"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "new\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9666666666666666\n",
        "0.7026143790849673\n",
        "```\n",
        "\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.6781045751633987\n",
        "0.6915032679738562\n",
        "```\n",
        "This result is to be expected. As we don't share weighting, topk may not work because different cases can be trained during training. They do not work collectively as well during testing, as different set of cases could be retrieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASjRV9422nzQ"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 300\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "new\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "0.9833333333333334\n",
        "0.7189542483660131\n",
        "```\n",
        "\n",
        "\n",
        "old\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.6970588235294117\n",
        "0.696078431372549\n",
        "```\n",
        "It seems that, weight sharing is really good for this data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ7e2l7lX6Es"
      },
      "source": [
        "##Results, classification, breast cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Dz9gwyzLiR"
      },
      "source": [
        "I reran things below. Something wasn't right with the kNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YrhxmJ3lxVr"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9455513784461151\n",
        "0.4514411027568922\n",
        "```\n",
        "????\n",
        "Old result below, it is so different from our new one?\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9104323308270675\n",
        "0.9297932330827068\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmQz3W1fqkOL"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9420112781954886\n",
        "0.47092731829573936\n",
        "```\n",
        "This 0.47 is probably something going wrong... We can ignore this one and use other knn accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFEv9cn6rnhS"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9227130325814535\n",
        "0.9332706766917293\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfLJaow_rq36"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 136\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "123\n",
        "# For classification\n",
        "print(np.mean(accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "output\n",
        "0.9595551378446114\n",
        "0.9332080200501254\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox8LsmRb5DnI"
      },
      "source": [
        "##Results, regression, Diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiyPBzOA15nV"
      },
      "source": [
        "DOING, continue rerun my experiments from here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnS97fLj5Kj7"
      },
      "source": [
        "loss is MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGQbG9nj5FqQ"
      },
      "source": [
        "```\n",
        "For the vanilla neural network\n",
        "\n",
        "Average loss on the test set: 5937.172802734375\n",
        "```\n",
        "\n",
        "This is a bad neural net as it only outputs a single number as output\n",
        "\n",
        "I am not sure why. I also tried training with batch but no good result`\n",
        "\n",
        "This neural network should be powerful enough as it has about 900 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNvHmRF_D1ul"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(6001.3232, grad_fn=<MeanBackward0>)\n",
        "3944.142\n",
        "3665.2056\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvLNHcb9GKqJ"
      },
      "source": [
        "Here we see that our model performs worse than baseline knn. I think it's because top k is enabled.\n",
        "\n",
        "Let's turn off top k and see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwKD02lLXNcr"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "```\n",
        "12345\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(3819.3308, grad_fn=<MeanBackward0>)\n",
        "4752.0566\n",
        "3813.1926\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2BEzJkmhlZc"
      },
      "source": [
        "Turning off ca_weight_sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0_FoDeMhtLt"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "12345\n",
        "# For regression\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(3560.3840, grad_fn=<MeanBackward0>)\n",
        "5768.119\n",
        "3647.5903\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_6SF2ysoNuF"
      },
      "source": [
        "```\n",
        "1234\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(3385.0386, grad_fn=<MeanBackward0>)\n",
        "5738.933\n",
        "3572.9805\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4rIyA5RWxac"
      },
      "source": [
        "##Results, regression, California\n",
        "\n",
        "All the leaves are brown ~\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUSW7u59VumL"
      },
      "source": [
        "California Housing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_-P0DljcmNQ"
      },
      "source": [
        "```\n",
        "For the vanilla neural network\n",
        "\n",
        "Loss on the test set: 1.3838589191436768\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD86mIVQW4tJ"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = True\n",
        "top_k = 5\n",
        "\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "print(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "tensor(0.8763, grad_fn=<MeanBackward0>)\n",
        "0.7351894\n",
        "1.4575317\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMXym3iMigp"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = True\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "\n",
        "```\n",
        "12345\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "account_circle\n",
        "tensor(0.8593, grad_fn=<MeanBackward0>)\n",
        "0.8837114\n",
        "1.4449631\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXdfL9-aN15y"
      },
      "source": [
        "```\n",
        "training_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.MSELoss()\n",
        "batch_size = 100\n",
        "\n",
        "fa_weight_sharing_within_segment = True\n",
        "fa_weight_sharing_between_segment = True\n",
        "ca_weight_sharing = False\n",
        "top_case_enabled = False\n",
        "top_k = 5\n",
        "class_weight_sharing = True\n",
        "nn_cdh_enabled = False\n",
        "\n",
        "patience = 40\n",
        "```\n",
        "```\n",
        "# For regressionprint(torch.mean( torch.stack(accuracies, dim=0)))\n",
        "print(np.mean(top_k_average_accuracies))\n",
        "print(np.mean(knn_accuracies))\n",
        "\n",
        "output\n",
        "tensor(0.6832, grad_fn=<MeanBackward0>)\n",
        "1.179338\n",
        "1.448509\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak6-DEZeZxcb"
      },
      "source": [
        "#Interesting notes  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGpEt98hZzzJ"
      },
      "source": [
        "There are lots and lots of design decisions to make for your neural network. Here I list a few:\n",
        "\n",
        "*   For activation functions in each layer, do you use relu (only positive), or leakyRelu (to avoid dead neuron)\n",
        "*   For certain weights, do you want them to be positive only?\n",
        "*   For case->class activation, do you do a ``torch.topk`` to only allow a few top cases to activate or all cases to activate?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzavJN1pNh-y"
      },
      "source": [
        "#Tensor operation broadcasting experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkFyuBTFNamy"
      },
      "source": [
        "The above code relies on the broadcasting feature of pytorch tensor operations:\n",
        "\n",
        "PyTorch broadcasts the smaller tensor (tensor2) along the missing dimensions so that its shape matches that of the larger tensor (tensor1). The element-wise multiplication is then performed as usual.\n",
        "\n",
        "Keep in mind that broadcasting has some rules, and the dimensions of the tensors need to be compatible. The trailing dimensions of the tensors must either be the same or one of them should be 1. If a dimension is missing in one of the tensors, PyTorch considers it to be of size 1 for the purpose of broadcasting. If broadcasting cannot be done, PyTorch will raise a RuntimeError."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_u4VvM4MPAf"
      },
      "outputs": [],
      "source": [
        "tensor_1 = torch.randn(1)\n",
        "tensor_2 = torch.randn(10,5)\n",
        "tensor_3 = torch.randn(10,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wWjRKRlnPA7"
      },
      "outputs": [],
      "source": [
        "# result = torch.matmul(tensor_2, tensor_3)\n",
        "result = tensor_2 * tensor_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-nboj_-nYzk"
      },
      "outputs": [],
      "source": [
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM9C_MKBMaLh"
      },
      "outputs": [],
      "source": [
        "print(tensor_1)\n",
        "print(tensor_2)\n",
        "print(tensor_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5TOI7qIMrsh"
      },
      "outputs": [],
      "source": [
        "tensor_2* tensor_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU6XNBTHNHh4"
      },
      "outputs": [],
      "source": [
        "tensor_1 * tensor_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og0Vd1GdMCcG"
      },
      "outputs": [],
      "source": [
        "tensor1 = torch.tensor([5.3712e-01, 1.6094e-01, -8.2879e-02, 7.0379e-01, -1.9436e-01,\n",
        "                        -8.6414e-02, -1.1221e+00, 1.7594e+00, -6.0423e-01, 1.9192e-02])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSuD7F5BNsat"
      },
      "outputs": [],
      "source": [
        "torch.matmul(tensor1, tensor1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcbrjZMhMJKF"
      },
      "outputs": [],
      "source": [
        "tensor2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJgs8II-Lm05"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the 1D tensor\n",
        "tensor1 = torch.tensor([[5.3712e-01, 1.6094e-01, -8.2879e-02, 7.0379e-01, -1.9436e-01,\n",
        "                        -8.6414e-02, -1.1221e+00, 1.7594e+00, -6.0423e-01, 1.9192e-02]])\n",
        "\n",
        "# Define the column vector (2D tensor)\n",
        "tensor2 = torch.tensor([[0.1390],\n",
        "                        [0.0275],\n",
        "                        [-0.5462],\n",
        "                        [-1.9431],\n",
        "                        [-0.0524],\n",
        "                        [0.9949],\n",
        "                        [-2.7876],\n",
        "                        [0.3611],\n",
        "                        [0.0308],\n",
        "                        [0.0524]])\n",
        "\n",
        "# Element-wise multiplication\n",
        "result = torch.matmul(tensor1 , tensor2)\n",
        "\n",
        "\n",
        "\n",
        "# Alternatively, you can use the * operator\n",
        "# result = tensor1 * tensor2.squeeze()\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXEY0twgLP9K"
      },
      "outputs": [],
      "source": [
        "torch.matmul(tensor_2, tensor_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yjIqbApsCLk"
      },
      "outputs": [],
      "source": [
        "torch.matmul(tensor_3, tensor_2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Z6BE0VHTogJo",
        "hdMkoZr_KgZX",
        "f2NnlWpVWqn6"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}